<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ | H.W.</title><meta name=keywords content="PySpark"><meta name=description content="Hi PySparkï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™"><meta name=author content="èƒ¡å­å”å”"><link rel=canonical href=https://unclehuzi.github.io/posts/da-pyspark-beginning/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://unclehuzi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://unclehuzi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://unclehuzi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://unclehuzi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://unclehuzi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JZ3C49ZSKM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JZ3C49ZSKM",{anonymize_ip:!1})}</script><meta property="og:title" content="PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’"><meta property="og:description" content="Hi PySparkï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™"><meta property="og:type" content="article"><meta property="og:url" content="https://unclehuzi.github.io/posts/da-pyspark-beginning/"><meta property="og:image" content="https://unclehuzi.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-10T20:28:01+08:00"><meta property="article:modified_time" content="2021-11-10T20:28:01+08:00"><meta property="og:site_name" content="èƒ¡å­å”å”çš„å°ç«™"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://unclehuzi.github.io/"><meta name=twitter:title content="PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’"><meta name=twitter:description content="Hi PySparkï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://unclehuzi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’","item":"https://unclehuzi.github.io/posts/da-pyspark-beginning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’","name":"PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’","description":"Hi PySparkï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™\n","keywords":["PySpark"],"articleBody":"Hi PySparkï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™\nPySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.\né¦–å…ˆï¼Œæˆ‘æ˜¯è¿™ä¹ˆæ¥çœ‹PySparkçš„ï¼šæœ‰ä¸€æ³¢äººä¼šPythonä½†ä¸ä¼šJavaï¼Œé‚£å°±æä¸ªæ¥å£è®©ä¼šPythonçš„å°ä¼™ä¼´äº«å—Sparkåˆ†å¸ƒå¼ç¯å¢ƒå¸¦æ¥çš„å¿«æ„Ÿï¼Œæ›´å¥½çš„åˆ†æå¤§æ•°æ®ã€‚\né‚£ä¹ˆå¯¹äºâ€œé¢å‘é—®é¢˜ç¼–ç¨‹â€çš„ä»ä¸šäººå‘˜æ¥è¯´PySparkçš„ä½œç”¨å°±å¾ˆæ˜æ˜¾äº†ã€‚å½“è§‰å¾—ç°æœ‰çš„åˆ†æå·¥å…·å¾ˆæ…¢æ—¶å¯ä»¥è€ƒè™‘ä¸‹PySparkï¼Œå½“ç„¶è¿™é‡Œæ˜¯åŸºäºSparkç¯å¢ƒã€‚æ¢å¥è¯è¯´ï¼Œâ€œå¿«â€æ˜¯åˆ†å¸ƒå¼ç¯å¢ƒå¸¦æ¥çš„å¿«æ„Ÿä¹‹ä¸€ã€‚\nå¼•å…¥ PySpark åï¼Œåˆ†æå·¥ä½œå¤§è‡´æµç¨‹å°±å˜æˆäº†è¿™æ · ğŸ‘‡\nå…¶å®ï¼Œæ•´ä½“è¿˜æ˜¯â€œç®±å­æ¨¡å‹â€ ğŸ“¦ ï¼Œâ€œå–‚â€æ•°æ® =\u003e å¤„ç†ã€è®¡ç®—æ¨¡å— =\u003e ç»“æœ\næ‰€ä»¥ï¼Œåº”ç”¨å±‚è§’åº¦æ¥çœ‹ PySpark ä¹Ÿå°±ç®€å•äº†ï¼š\nå¦‚ä½•è¯»å–æ•°æ®ï¼Ÿ å¦‚ä½•å¤„ç†ã€è®¡ç®—å¾—åˆ°è‡ªå·±æƒ³è¦çš„ç»“æœï¼Œå³â€œé¢å‘é—®é¢˜ç¼–ç¨‹â€ å¦‚ä½•å¤„ç†ç»“æœï¼Ÿè¦ä¿å­˜åˆ°å“ªå„¿ï¼Ÿ å¦‚ä½•è¯»å–æ•°æ® è¿™ä¸ªå¾€å¾€å–å†³äºæ•°æ®åœ¨å“ªå„¿ï¼Œè­¬å¦‚æœ‰äº›æ•°æ®æ˜¯ä»¥csvæ ¼å¼ä¿å­˜ï¼Œæœ‰äº›æ˜¯åœ¨æ•°æ®åº“â€¦\næ€»ä¹‹éƒ½æ˜¯ä¸ºäº† Loading data onto Spark RDDsï¼Œäº«å—åˆ†å¸ƒå¼çš„å¿«æ„Ÿ\nå®é™…æ“ä½œå¯ä»¥åŸºäºå…·ä½“æƒ…å†µåœ¨ç½‘ä¸Šæ£€ç´¢ç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚ pyspark read hive table\n# A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html from os.path import abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath('spark-warehouse') spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL Hive integration example\") \\ .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() # spark is an existing SparkSession spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\") spark.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\") # The results of SQL queries are themselves DataFrames and support all normal functions. sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key \u003c 10 ORDER BY key\") # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # \"\"\" An interactive shell. This file is designed to be launched as a PYTHONSTARTUP script. \"\"\" import atexit import os import platform import warnings import py4j from pyspark import SparkConf from pyspark.context import SparkContext from pyspark.sql import SparkSession, SQLContext if os.environ.get(\"SPARK_EXECUTOR_URI\"): SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"]) SparkContext._ensure_initialized() try: # Try to access HiveConf, it will raise exception if Hive is not added conf = SparkConf() if conf.get('spark.sql.catalogImplementation', 'hive').lower() == 'hive': SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf() spark = SparkSession.builder\\ .enableHiveSupport()\\ .getOrCreate() else: spark = SparkSession.builder.getOrCreate() except py4j.protocol.Py4JError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() except TypeError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() sc = spark.sparkContext sql = spark.sql atexit.register(lambda: sc.stop()) # for compatibility sqlContext = spark._wrapped sqlCtx = sqlContext print(\"\"\"Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version %s /_/ \"\"\" % sc.version) print(\"Using Python version %s (%s, %s)\" % ( platform.python_version(), platform.python_build()[0], platform.python_build()[1])) print(\"SparkSession available as 'spark'.\") # The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP, # which allows us to execute the user's PYTHONSTARTUP file: _pythonstartup = os.environ.get('OLD_PYTHONSTARTUP') if _pythonstartup and os.path.isfile(_pythonstartup): with open(_pythonstartup) as f: code = compile(f.read(), _pythonstartup, 'exec') exec(code) æ•°æ®å¤„ç†ã€è®¡ç®— è¯»å–æ•°æ®å¾—åˆ° Spark DataFrame åï¼Œå¯ä»¥ç›´æ¥å¯¹æ­¤è¿›è¡Œæ“ä½œï¼Œé™¤äº†å¸¸è§çš„ä¸šåŠ¡åˆ†æè¿˜æœ‰æœºå™¨å­¦ä¹ æ¨¡å—ï¼ˆMLlibï¼‰\nraw_data = sc.textFile(\"./kddcup.data.gz\") ## Comma-Separated Value csv = raw_data.map(lambda x: x.split(\",\")) metrics = csv.map(lambda x: [x[0], x[4], x[5]]) from pyspark.mllib.stat import Statistics Statistics.corr(metrics, method=\"spearman\") Statistics.corr(metrics, method=\"pearson\") å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒSpark DataFrame to pandas DataFrame\nå¯ä»¥ç”¨ toPandas() æ–¹æ³•ï¼ŒåŒæ—¶å‚æ•°æ–¹é¢è®¾ç½® spark.sql.execution.arrow.enabled=true èƒ½æé«˜æ•ˆç‡\n# A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas import numpy as np import pandas as pd # Enable Arrow-based columnar data transfers spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Generate a pandas DataFrame pdf = pd.DataFrame(np.random.rand(100, 3)) # Create a Spark DataFrame from a pandas DataFrame using Arrow df = spark.createDataFrame(pdf) # Convert the Spark DataFrame back to a pandas DataFrame using Arrow result_pdf = df.select(\"*\").toPandas() è¿™éƒ¨åˆ†å†Markä¸€ä¸ªå…³äº collect() çš„å°ç‚¹ï¼Œæ€»ä¹‹æ•°æ®é‡æ¯”è¾ƒå¤§çš„æ—¶å€™å°±ä¸è¦ç”¨è¿™ä¸ªæ–¹æ³•ã€‚\nThe collect() function returns a list that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as all the data is loaded in a driverâ€™s memory, in which case we lose the benefits of distributing the data around a cluster of Spark instances.\nå¦‚ä½•å¤„ç†ç»“æœ å¤„ç†ã€è®¡ç®—åçš„ç»“æœå¾€å¾€ä¼šå†ä¸€æ¬¡çš„è½åº“ï¼Œè¿™ä¸ªæ—¶å€™åŒ â€œæ•°æ®è¯»å–â€ çš„éƒ¨åˆ†ï¼ŒğŸ‰‘ï¸ æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œæ£€ç´¢ã€‚\nä»¥è½åˆ° hive è¡¨ä¸ºä¾‹ï¼Œæˆªæ­¢åˆ°ç›®å‰æ•´ç†çš„ï¼Œå¤§è‡´æœ‰ä¸¤ç§æ–¹æ³•ã€‚\né¦–å…ˆç¡®ä¿æ•°æ®ä¸º Spark DataFrame çŠ¶æ€ï¼ˆå¯ä»¥é€šè¿‡ spark.createDataFrame(df) çš„æ–¹æ³•å°† pandas DataFrame è½¬ä¸º Spark DataFrameï¼‰\nspark_df.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"dbName.tableName\") # æ³¨æ„æ˜¯ overwrite æˆ–è€…\nspark_df.createOrReplaceTempView(\"myTempTableName\") spark.sql(\"drop table if exists dbName.tableName\") spark.sql(\"create table dbName.tableName as select * from myTempTableName\") æ€»ç»“ æœ€è¿‘å·¥ä½œä¸­é‡åˆ°äº† PySpark çš„ä½¿ç”¨ï¼Œåœ¨æ­¤ä»åº”ç”¨å±‚å°ç™½è§†è§’é€šè¿‡ ğŸ“¦ â€œç®±å­æ¨¡å‹â€ï¼ˆInput =\u003e Box =\u003e Outputï¼‰ ç®€å•è®°å½•å¤§è‡´çš„ä½¿ç”¨æµç¨‹ï¼Œæ–¹ä¾¿äºæ–°æ‰‹ï½\nReference 360æ•°ç§‘æ·±åœ³æ•°æ®ç»„ Rudy Lai and BartÅ‚omiej Potaczek.ã€ŠHands On Big Data Analytics With PySparkã€‹ https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas http://spark.apache.org/docs/latest/api/python/index.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py ","wordCount":"760","inLanguage":"en","datePublished":"2021-11-10T20:28:01+08:00","dateModified":"2021-11-10T20:28:01+08:00","author":{"@type":"Person","name":"èƒ¡å­å”å”"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://unclehuzi.github.io/posts/da-pyspark-beginning/"},"publisher":{"@type":"Organization","name":"H.W.","logo":{"@type":"ImageObject","url":"https://unclehuzi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://unclehuzi.github.io/ accesskey=h title="H.W. (Alt + H)">H.W.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://unclehuzi.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://unclehuzi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://unclehuzi.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://unclehuzi.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://unclehuzi.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://unclehuzi.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://unclehuzi.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’</h1><div class=post-meta><span title='2021-11-10 20:28:01 +0800 +0800'>2021-11-10</span>&nbsp;Â·&nbsp;èƒ¡å­å”å”</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#å¦‚ä½•è¯»å–æ•°æ®>å¦‚ä½•è¯»å–æ•°æ®</a></li><li><a href=#æ•°æ®å¤„ç†è®¡ç®—>æ•°æ®å¤„ç†ã€è®¡ç®—</a></li><li><a href=#å¦‚ä½•å¤„ç†ç»“æœ>å¦‚ä½•å¤„ç†ç»“æœ</a></li><li><a href=#æ€»ç»“>æ€»ç»“</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><p>Hi <code>PySpark</code>ï¼Œåˆæ¬¡è§é¢ï¼Œåˆ«æ¥æ— æ™</p><blockquote><p>PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.</p></blockquote><p>é¦–å…ˆï¼Œæˆ‘æ˜¯è¿™ä¹ˆæ¥çœ‹<code>PySpark</code>çš„ï¼šæœ‰ä¸€æ³¢äººä¼š<code>Python</code>ä½†ä¸ä¼š<code>Java</code>ï¼Œé‚£å°±æä¸ªæ¥å£è®©ä¼š<code>Python</code>çš„å°ä¼™ä¼´äº«å—<code>Spark</code>åˆ†å¸ƒå¼ç¯å¢ƒå¸¦æ¥çš„å¿«æ„Ÿï¼Œæ›´å¥½çš„åˆ†æå¤§æ•°æ®ã€‚</p><p>é‚£ä¹ˆå¯¹äºâ€œé¢å‘é—®é¢˜ç¼–ç¨‹â€çš„ä»ä¸šäººå‘˜æ¥è¯´<code>PySpark</code>çš„ä½œç”¨å°±å¾ˆæ˜æ˜¾äº†ã€‚å½“è§‰å¾—ç°æœ‰çš„åˆ†æå·¥å…·å¾ˆæ…¢æ—¶å¯ä»¥è€ƒè™‘ä¸‹<code>PySpark</code>ï¼Œå½“ç„¶è¿™é‡Œæ˜¯åŸºäº<code>Spark</code>ç¯å¢ƒã€‚æ¢å¥è¯è¯´ï¼Œâ€œå¿«â€æ˜¯åˆ†å¸ƒå¼ç¯å¢ƒå¸¦æ¥çš„å¿«æ„Ÿä¹‹ä¸€ã€‚</p><p>å¼•å…¥ <code>PySpark</code> åï¼Œåˆ†æå·¥ä½œå¤§è‡´æµç¨‹å°±å˜æˆäº†è¿™æ · ğŸ‘‡</p><p>å…¶å®ï¼Œæ•´ä½“è¿˜æ˜¯â€œç®±å­æ¨¡å‹â€ ğŸ“¦ ï¼Œâ€œå–‚â€æ•°æ® => å¤„ç†ã€è®¡ç®—æ¨¡å— => ç»“æœ</p><p>æ‰€ä»¥ï¼Œåº”ç”¨å±‚è§’åº¦æ¥çœ‹ <code>PySpark</code> ä¹Ÿå°±ç®€å•äº†ï¼š</p><ol><li>å¦‚ä½•è¯»å–æ•°æ®ï¼Ÿ</li><li>å¦‚ä½•å¤„ç†ã€è®¡ç®—å¾—åˆ°è‡ªå·±æƒ³è¦çš„ç»“æœï¼Œå³â€œé¢å‘é—®é¢˜ç¼–ç¨‹â€</li><li>å¦‚ä½•å¤„ç†ç»“æœï¼Ÿè¦ä¿å­˜åˆ°å“ªå„¿ï¼Ÿ</li></ol><h2 id=å¦‚ä½•è¯»å–æ•°æ®>å¦‚ä½•è¯»å–æ•°æ®<a hidden class=anchor aria-hidden=true href=#å¦‚ä½•è¯»å–æ•°æ®>#</a></h2><p>è¿™ä¸ªå¾€å¾€å–å†³äºæ•°æ®åœ¨å“ªå„¿ï¼Œè­¬å¦‚æœ‰äº›æ•°æ®æ˜¯ä»¥csvæ ¼å¼ä¿å­˜ï¼Œæœ‰äº›æ˜¯åœ¨æ•°æ®åº“&mldr;</p><p>æ€»ä¹‹éƒ½æ˜¯ä¸ºäº† <em><strong>Loading data onto Spark RDDs</strong></em>ï¼Œäº«å—åˆ†å¸ƒå¼çš„å¿«æ„Ÿ</p><p>å®é™…æ“ä½œå¯ä»¥åŸºäºå…·ä½“æƒ…å†µåœ¨ç½‘ä¸Šæ£€ç´¢ç›¸åº”çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚ <code>pyspark read hive table</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>os.path</span> <span class=kn>import</span> <span class=n>abspath</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>Row</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># warehouse_location points to the default location for managed databases and tables</span>
</span></span><span class=line><span class=cl><span class=n>warehouse_location</span> <span class=o>=</span> <span class=n>abspath</span><span class=p>(</span><span class=s1>&#39;spark-warehouse&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Python Spark SQL Hive integration example&#34;</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.sql.warehouse.dir&#34;</span><span class=p>,</span> <span class=n>warehouse_location</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># spark is an existing SparkSession</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The results of SQL queries are themselves DataFrames and support all normal functions.</span>
</span></span><span class=line><span class=cl><span class=n>sqlDF</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&#34;</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Licensed to the Apache Software Foundation (ASF) under one or more</span>
</span></span><span class=line><span class=cl><span class=c1># contributor license agreements.  See the NOTICE file distributed with</span>
</span></span><span class=line><span class=cl><span class=c1># this work for additional information regarding copyright ownership.</span>
</span></span><span class=line><span class=cl><span class=c1># The ASF licenses this file to You under the Apache License, Version 2.0</span>
</span></span><span class=line><span class=cl><span class=c1># (the &#34;License&#34;); you may not use this file except in compliance with</span>
</span></span><span class=line><span class=cl><span class=c1># the License.  You may obtain a copy of the License at</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1>#    http://www.apache.org/licenses/LICENSE-2.0</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Unless required by applicable law or agreed to in writing, software</span>
</span></span><span class=line><span class=cl><span class=c1># distributed under the License is distributed on an &#34;AS IS&#34; BASIS,</span>
</span></span><span class=line><span class=cl><span class=c1># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span></span><span class=line><span class=cl><span class=c1># See the License for the specific language governing permissions and</span>
</span></span><span class=line><span class=cl><span class=c1># limitations under the License.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>An interactive shell.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>This file is designed to be launched as a PYTHONSTARTUP script.
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>atexit</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>platform</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>warnings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>py4j</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark</span> <span class=kn>import</span> <span class=n>SparkConf</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.context</span> <span class=kn>import</span> <span class=n>SparkContext</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span><span class=p>,</span> <span class=n>SQLContext</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;SPARK_EXECUTOR_URI&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>SparkContext</span><span class=o>.</span><span class=n>setSystemProperty</span><span class=p>(</span><span class=s2>&#34;spark.executor.uri&#34;</span><span class=p>,</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SPARK_EXECUTOR_URI&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>SparkContext</span><span class=o>.</span><span class=n>_ensure_initialized</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Try to access HiveConf, it will raise exception if Hive is not added</span>
</span></span><span class=line><span class=cl>    <span class=n>conf</span> <span class=o>=</span> <span class=n>SparkConf</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;hive&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>SparkContext</span><span class=o>.</span><span class=n>_jvm</span><span class=o>.</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>hadoop</span><span class=o>.</span><span class=n>hive</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>HiveConf</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span>\
</span></span><span class=line><span class=cl>            <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span>\
</span></span><span class=line><span class=cl>            <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=n>py4j</span><span class=o>.</span><span class=n>protocol</span><span class=o>.</span><span class=n>Py4JError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span><span class=s2>&#34;Fall back to non-hive support because failing to access HiveConf, &#34;</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;please make sure you build spark with hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>TypeError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span><span class=s2>&#34;Fall back to non-hive support because failing to access HiveConf, &#34;</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;please make sure you build spark with hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span>
</span></span><span class=line><span class=cl><span class=n>sql</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span>
</span></span><span class=line><span class=cl><span class=n>atexit</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=k>lambda</span><span class=p>:</span> <span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># for compatibility</span>
</span></span><span class=line><span class=cl><span class=n>sqlContext</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>_wrapped</span>
</span></span><span class=line><span class=cl><span class=n>sqlCtx</span> <span class=o>=</span> <span class=n>sqlContext</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;&#34;&#34;Welcome to
</span></span></span><span class=line><span class=cl><span class=s2>      ____              __
</span></span></span><span class=line><span class=cl><span class=s2>     / __/__  ___ _____/ /__
</span></span></span><span class=line><span class=cl><span class=s2>    _\ \/ _ \/ _ `/ __/  &#39;_/
</span></span></span><span class=line><span class=cl><span class=s2>   /__ / .__/\_,_/_/ /_/\_\   version </span><span class=si>%s</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>      /_/
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span> <span class=o>%</span> <span class=n>sc</span><span class=o>.</span><span class=n>version</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using Python version </span><span class=si>%s</span><span class=s2> (</span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>)&#34;</span> <span class=o>%</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_version</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_build</span><span class=p>()[</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_build</span><span class=p>()[</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;SparkSession available as &#39;spark&#39;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP,</span>
</span></span><span class=line><span class=cl><span class=c1># which allows us to execute the user&#39;s PYTHONSTARTUP file:</span>
</span></span><span class=line><span class=cl><span class=n>_pythonstartup</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;OLD_PYTHONSTARTUP&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>_pythonstartup</span> <span class=ow>and</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>isfile</span><span class=p>(</span><span class=n>_pythonstartup</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>_pythonstartup</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>code</span> <span class=o>=</span> <span class=nb>compile</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>(),</span> <span class=n>_pythonstartup</span><span class=p>,</span> <span class=s1>&#39;exec&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>exec</span><span class=p>(</span><span class=n>code</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=æ•°æ®å¤„ç†è®¡ç®—>æ•°æ®å¤„ç†ã€è®¡ç®—<a hidden class=anchor aria-hidden=true href=#æ•°æ®å¤„ç†è®¡ç®—>#</a></h2><p>è¯»å–æ•°æ®å¾—åˆ° <code>Spark DataFrame</code> åï¼Œå¯ä»¥ç›´æ¥å¯¹æ­¤è¿›è¡Œæ“ä½œï¼Œé™¤äº†å¸¸è§çš„ä¸šåŠ¡åˆ†æè¿˜æœ‰æœºå™¨å­¦ä¹ æ¨¡å—ï¼ˆ<code>MLlib</code>ï¼‰</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>raw_data</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>&#34;./kddcup.data.gz&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>## Comma-Separated Value</span>
</span></span><span class=line><span class=cl><span class=n>csv</span> <span class=o>=</span> <span class=n>raw_data</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;,&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>metrics</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>[</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>4</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>5</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.mllib.stat</span> <span class=kn>import</span> <span class=n>Statistics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Statistics</span><span class=o>.</span><span class=n>corr</span><span class=p>(</span><span class=n>metrics</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&#34;spearman&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Statistics</span><span class=o>.</span><span class=n>corr</span><span class=p>(</span><span class=n>metrics</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&#34;pearson&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>å€¼å¾—ä¸€æçš„æ˜¯ï¼Œ<strong>Spark DataFrame to pandas DataFrame</strong></p><p>å¯ä»¥ç”¨ <code>toPandas()</code> æ–¹æ³•ï¼ŒåŒæ—¶å‚æ•°æ–¹é¢è®¾ç½® <code>spark.sql.execution.arrow.enabled=true</code> èƒ½æé«˜æ•ˆç‡</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Enable Arrow-based columnar data transfers</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>&#34;spark.sql.execution.arrow.enabled&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate a pandas DataFrame</span>
</span></span><span class=line><span class=cl><span class=n>pdf</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a Spark DataFrame from a pandas DataFrame using Arrow</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>pdf</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert the Spark DataFrame back to a pandas DataFrame using Arrow</span>
</span></span><span class=line><span class=cl><span class=n>result_pdf</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;*&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>toPandas</span><span class=p>()</span>
</span></span></code></pre></div><p>è¿™éƒ¨åˆ†å†Markä¸€ä¸ªå…³äº <code>collect()</code> çš„å°ç‚¹ï¼Œæ€»ä¹‹æ•°æ®é‡æ¯”è¾ƒå¤§çš„æ—¶å€™å°±ä¸è¦ç”¨è¿™ä¸ªæ–¹æ³•ã€‚</p><blockquote><p>The <code>collect()</code> function <strong>returns a list</strong> that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as <em>all the data is loaded in a driver&rsquo;s memory</em>, in which case we lose the benefits of distributing the data around a cluster of Spark instances.</p></blockquote><h2 id=å¦‚ä½•å¤„ç†ç»“æœ>å¦‚ä½•å¤„ç†ç»“æœ<a hidden class=anchor aria-hidden=true href=#å¦‚ä½•å¤„ç†ç»“æœ>#</a></h2><p>å¤„ç†ã€è®¡ç®—åçš„ç»“æœå¾€å¾€ä¼šå†ä¸€æ¬¡çš„è½åº“ï¼Œè¿™ä¸ªæ—¶å€™åŒ â€œæ•°æ®è¯»å–â€ çš„éƒ¨åˆ†ï¼ŒğŸ‰‘ï¸ æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œæ£€ç´¢ã€‚</p><p><strong>ä»¥è½åˆ° hive è¡¨ä¸ºä¾‹</strong>ï¼Œæˆªæ­¢åˆ°ç›®å‰æ•´ç†çš„ï¼Œå¤§è‡´æœ‰ä¸¤ç§æ–¹æ³•ã€‚</p><p>é¦–å…ˆç¡®ä¿æ•°æ®ä¸º Spark DataFrame çŠ¶æ€ï¼ˆå¯ä»¥é€šè¿‡ <code>spark.createDataFrame(df)</code> çš„æ–¹æ³•å°† pandas DataFrame è½¬ä¸º Spark DataFrameï¼‰</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark_df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>&#34;overwrite&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;hive&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>&#34;dbName.tableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># æ³¨æ„æ˜¯ overwrite </span>
</span></span></code></pre></div><p>æˆ–è€…</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark_df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;myTempTableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;drop table if exists dbName.tableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;create table dbName.tableName as select * from myTempTableName&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=æ€»ç»“>æ€»ç»“<a hidden class=anchor aria-hidden=true href=#æ€»ç»“>#</a></h2><p>æœ€è¿‘å·¥ä½œä¸­é‡åˆ°äº† <code>PySpark</code> çš„ä½¿ç”¨ï¼Œåœ¨æ­¤ä»åº”ç”¨å±‚å°ç™½è§†è§’é€šè¿‡ ğŸ“¦ â€œç®±å­æ¨¡å‹â€ï¼ˆInput => Box => Outputï¼‰ ç®€å•è®°å½•å¤§è‡´çš„ä½¿ç”¨æµç¨‹ï¼Œæ–¹ä¾¿äºæ–°æ‰‹ï½</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ol><li>360æ•°ç§‘æ·±åœ³æ•°æ®ç»„</li><li>Rudy Lai and BartÅ‚omiej Potaczek.ã€ŠHands On Big Data Analytics With PySparkã€‹</li><li><a href=https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas>https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas</a></li><li><a href=http://spark.apache.org/docs/latest/api/python/index.html>http://spark.apache.org/docs/latest/api/python/index.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html>https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</a></li><li><a href=https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive>https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive</a></li><li><a href=https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py>https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://unclehuzi.github.io/tags/pyspark/>PySpark</a></li></ul><nav class=paginav><a class=prev href=https://unclehuzi.github.io/posts/life-movie-x-men/><span class=title>Â« Prev</span><br><span>Xæˆ˜è­¦ç³»åˆ—è§‚å½±é¡ºåº</span></a>
<a class=next href=https://unclehuzi.github.io/posts/causal-resources/><span class=title>Next Â»</span><br><span>å› æœæ¨æ–­è¡¥ç»™ç«™</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on x" href="https://x.com/intent/tweet/?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&amp;hashtags=PySpark"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&amp;title=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;summary=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;source=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on reddit" href="https://reddit.com/submit?url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&title=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on whatsapp" href="https://api.whatsapp.com/send?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92%20-%20https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on telegram" href="https://telegram.me/share/url?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySparkä¹‹åº”ç”¨å±‚å°ç™½è§†è§’ on ycombinator" href="https://news.ycombinator.com/submitlink?t=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&u=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://unclehuzi.github.io/>H.W.</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>