<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PySpark之应用层小白视角 | H.W.</title><meta name=keywords content="PySpark"><meta name=description content="Hi PySpark，初次见面，别来无恙"><meta name=author content="胡子叔叔"><link rel=canonical href=https://unclehuzi.github.io/posts/da-pyspark-beginning/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://unclehuzi.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://unclehuzi.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://unclehuzi.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://unclehuzi.github.io/apple-touch-icon.png><link rel=mask-icon href=https://unclehuzi.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JZ3C49ZSKM"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JZ3C49ZSKM",{anonymize_ip:!1})}</script><meta property="og:title" content="PySpark之应用层小白视角"><meta property="og:description" content="Hi PySpark，初次见面，别来无恙"><meta property="og:type" content="article"><meta property="og:url" content="https://unclehuzi.github.io/posts/da-pyspark-beginning/"><meta property="og:image" content="https://unclehuzi.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-10T20:28:01+08:00"><meta property="article:modified_time" content="2021-11-10T20:28:01+08:00"><meta property="og:site_name" content="胡子叔叔的小站"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://unclehuzi.github.io/"><meta name=twitter:title content="PySpark之应用层小白视角"><meta name=twitter:description content="Hi PySpark，初次见面，别来无恙"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://unclehuzi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"PySpark之应用层小白视角","item":"https://unclehuzi.github.io/posts/da-pyspark-beginning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PySpark之应用层小白视角","name":"PySpark之应用层小白视角","description":"Hi PySpark，初次见面，别来无恙\n","keywords":["PySpark"],"articleBody":"Hi PySpark，初次见面，别来无恙\nPySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.\n首先，我是这么来看PySpark的：有一波人会Python但不会Java，那就搞个接口让会Python的小伙伴享受Spark分布式环境带来的快感，更好的分析大数据。\n那么对于“面向问题编程”的从业人员来说PySpark的作用就很明显了。当觉得现有的分析工具很慢时可以考虑下PySpark，当然这里是基于Spark环境。换句话说，“快”是分布式环境带来的快感之一。\n引入 PySpark 后，分析工作大致流程就变成了这样 👇\n其实，整体还是“箱子模型” 📦 ，“喂”数据 =\u003e 处理、计算模块 =\u003e 结果\n所以，应用层角度来看 PySpark 也就简单了：\n如何读取数据？ 如何处理、计算得到自己想要的结果，即“面向问题编程” 如何处理结果？要保存到哪儿？ 如何读取数据 这个往往取决于数据在哪儿，譬如有些数据是以csv格式保存，有些是在数据库…\n总之都是为了 Loading data onto Spark RDDs，享受分布式的快感\n实际操作可以基于具体情况在网上检索相应的解决方案，如 pyspark read hive table\n# A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html from os.path import abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath('spark-warehouse') spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL Hive integration example\") \\ .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() # spark is an existing SparkSession spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\") spark.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\") # The results of SQL queries are themselves DataFrames and support all normal functions. sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key \u003c 10 ORDER BY key\") # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # \"\"\" An interactive shell. This file is designed to be launched as a PYTHONSTARTUP script. \"\"\" import atexit import os import platform import warnings import py4j from pyspark import SparkConf from pyspark.context import SparkContext from pyspark.sql import SparkSession, SQLContext if os.environ.get(\"SPARK_EXECUTOR_URI\"): SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"]) SparkContext._ensure_initialized() try: # Try to access HiveConf, it will raise exception if Hive is not added conf = SparkConf() if conf.get('spark.sql.catalogImplementation', 'hive').lower() == 'hive': SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf() spark = SparkSession.builder\\ .enableHiveSupport()\\ .getOrCreate() else: spark = SparkSession.builder.getOrCreate() except py4j.protocol.Py4JError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() except TypeError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() sc = spark.sparkContext sql = spark.sql atexit.register(lambda: sc.stop()) # for compatibility sqlContext = spark._wrapped sqlCtx = sqlContext print(\"\"\"Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version %s /_/ \"\"\" % sc.version) print(\"Using Python version %s (%s, %s)\" % ( platform.python_version(), platform.python_build()[0], platform.python_build()[1])) print(\"SparkSession available as 'spark'.\") # The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP, # which allows us to execute the user's PYTHONSTARTUP file: _pythonstartup = os.environ.get('OLD_PYTHONSTARTUP') if _pythonstartup and os.path.isfile(_pythonstartup): with open(_pythonstartup) as f: code = compile(f.read(), _pythonstartup, 'exec') exec(code) 数据处理、计算 读取数据得到 Spark DataFrame 后，可以直接对此进行操作，除了常见的业务分析还有机器学习模块（MLlib）\nraw_data = sc.textFile(\"./kddcup.data.gz\") ## Comma-Separated Value csv = raw_data.map(lambda x: x.split(\",\")) metrics = csv.map(lambda x: [x[0], x[4], x[5]]) from pyspark.mllib.stat import Statistics Statistics.corr(metrics, method=\"spearman\") Statistics.corr(metrics, method=\"pearson\") 值得一提的是，Spark DataFrame to pandas DataFrame\n可以用 toPandas() 方法，同时参数方面设置 spark.sql.execution.arrow.enabled=true 能提高效率\n# A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas import numpy as np import pandas as pd # Enable Arrow-based columnar data transfers spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Generate a pandas DataFrame pdf = pd.DataFrame(np.random.rand(100, 3)) # Create a Spark DataFrame from a pandas DataFrame using Arrow df = spark.createDataFrame(pdf) # Convert the Spark DataFrame back to a pandas DataFrame using Arrow result_pdf = df.select(\"*\").toPandas() 这部分再Mark一个关于 collect() 的小点，总之数据量比较大的时候就不要用这个方法。\nThe collect() function returns a list that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as all the data is loaded in a driver’s memory, in which case we lose the benefits of distributing the data around a cluster of Spark instances.\n如何处理结果 处理、计算后的结果往往会再一次的落库，这个时候同 “数据读取” 的部分，🉑️ 根据具体情况进行检索。\n以落到 hive 表为例，截止到目前整理的，大致有两种方法。\n首先确保数据为 Spark DataFrame 状态（可以通过 spark.createDataFrame(df) 的方法将 pandas DataFrame 转为 Spark DataFrame）\nspark_df.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"dbName.tableName\") # 注意是 overwrite 或者\nspark_df.createOrReplaceTempView(\"myTempTableName\") spark.sql(\"drop table if exists dbName.tableName\") spark.sql(\"create table dbName.tableName as select * from myTempTableName\") 总结 最近工作中遇到了 PySpark 的使用，在此从应用层小白视角通过 📦 “箱子模型”（Input =\u003e Box =\u003e Output） 简单记录大致的使用流程，方便于新手～\nReference 360数科深圳数据组 Rudy Lai and Bartłomiej Potaczek.《Hands On Big Data Analytics With PySpark》 https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas http://spark.apache.org/docs/latest/api/python/index.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py ","wordCount":"760","inLanguage":"en","datePublished":"2021-11-10T20:28:01+08:00","dateModified":"2021-11-10T20:28:01+08:00","author":{"@type":"Person","name":"胡子叔叔"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://unclehuzi.github.io/posts/da-pyspark-beginning/"},"publisher":{"@type":"Organization","name":"H.W.","logo":{"@type":"ImageObject","url":"https://unclehuzi.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://unclehuzi.github.io/ accesskey=h title="H.W. (Alt + H)">H.W.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://unclehuzi.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://unclehuzi.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://unclehuzi.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://unclehuzi.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://unclehuzi.github.io/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://unclehuzi.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://unclehuzi.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PySpark之应用层小白视角</h1><div class=post-meta><span title='2021-11-10 20:28:01 +0800 +0800'>2021-11-10</span>&nbsp;·&nbsp;胡子叔叔</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#如何读取数据>如何读取数据</a></li><li><a href=#数据处理计算>数据处理、计算</a></li><li><a href=#如何处理结果>如何处理结果</a></li><li><a href=#总结>总结</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><p>Hi <code>PySpark</code>，初次见面，别来无恙</p><blockquote><p>PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.</p></blockquote><p>首先，我是这么来看<code>PySpark</code>的：有一波人会<code>Python</code>但不会<code>Java</code>，那就搞个接口让会<code>Python</code>的小伙伴享受<code>Spark</code>分布式环境带来的快感，更好的分析大数据。</p><p>那么对于“面向问题编程”的从业人员来说<code>PySpark</code>的作用就很明显了。当觉得现有的分析工具很慢时可以考虑下<code>PySpark</code>，当然这里是基于<code>Spark</code>环境。换句话说，“快”是分布式环境带来的快感之一。</p><p>引入 <code>PySpark</code> 后，分析工作大致流程就变成了这样 👇</p><p>其实，整体还是“箱子模型” 📦 ，“喂”数据 => 处理、计算模块 => 结果</p><p>所以，应用层角度来看 <code>PySpark</code> 也就简单了：</p><ol><li>如何读取数据？</li><li>如何处理、计算得到自己想要的结果，即“面向问题编程”</li><li>如何处理结果？要保存到哪儿？</li></ol><h2 id=如何读取数据>如何读取数据<a hidden class=anchor aria-hidden=true href=#如何读取数据>#</a></h2><p>这个往往取决于数据在哪儿，譬如有些数据是以csv格式保存，有些是在数据库&mldr;</p><p>总之都是为了 <em><strong>Loading data onto Spark RDDs</strong></em>，享受分布式的快感</p><p>实际操作可以基于具体情况在网上检索相应的解决方案，如 <code>pyspark read hive table</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>os.path</span> <span class=kn>import</span> <span class=n>abspath</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>Row</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># warehouse_location points to the default location for managed databases and tables</span>
</span></span><span class=line><span class=cl><span class=n>warehouse_location</span> <span class=o>=</span> <span class=n>abspath</span><span class=p>(</span><span class=s1>&#39;spark-warehouse&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Python Spark SQL Hive integration example&#34;</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.sql.warehouse.dir&#34;</span><span class=p>,</span> <span class=n>warehouse_location</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># spark is an existing SparkSession</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The results of SQL queries are themselves DataFrames and support all normal functions.</span>
</span></span><span class=line><span class=cl><span class=n>sqlDF</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&#34;</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Licensed to the Apache Software Foundation (ASF) under one or more</span>
</span></span><span class=line><span class=cl><span class=c1># contributor license agreements.  See the NOTICE file distributed with</span>
</span></span><span class=line><span class=cl><span class=c1># this work for additional information regarding copyright ownership.</span>
</span></span><span class=line><span class=cl><span class=c1># The ASF licenses this file to You under the Apache License, Version 2.0</span>
</span></span><span class=line><span class=cl><span class=c1># (the &#34;License&#34;); you may not use this file except in compliance with</span>
</span></span><span class=line><span class=cl><span class=c1># the License.  You may obtain a copy of the License at</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1>#    http://www.apache.org/licenses/LICENSE-2.0</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Unless required by applicable law or agreed to in writing, software</span>
</span></span><span class=line><span class=cl><span class=c1># distributed under the License is distributed on an &#34;AS IS&#34; BASIS,</span>
</span></span><span class=line><span class=cl><span class=c1># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span></span><span class=line><span class=cl><span class=c1># See the License for the specific language governing permissions and</span>
</span></span><span class=line><span class=cl><span class=c1># limitations under the License.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>An interactive shell.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>This file is designed to be launched as a PYTHONSTARTUP script.
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>atexit</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>platform</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>warnings</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>py4j</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark</span> <span class=kn>import</span> <span class=n>SparkConf</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.context</span> <span class=kn>import</span> <span class=n>SparkContext</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span><span class=p>,</span> <span class=n>SQLContext</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;SPARK_EXECUTOR_URI&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>SparkContext</span><span class=o>.</span><span class=n>setSystemProperty</span><span class=p>(</span><span class=s2>&#34;spark.executor.uri&#34;</span><span class=p>,</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;SPARK_EXECUTOR_URI&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>SparkContext</span><span class=o>.</span><span class=n>_ensure_initialized</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Try to access HiveConf, it will raise exception if Hive is not added</span>
</span></span><span class=line><span class=cl>    <span class=n>conf</span> <span class=o>=</span> <span class=n>SparkConf</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;hive&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>SparkContext</span><span class=o>.</span><span class=n>_jvm</span><span class=o>.</span><span class=n>org</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>hadoop</span><span class=o>.</span><span class=n>hive</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>HiveConf</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span>\
</span></span><span class=line><span class=cl>            <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span>\
</span></span><span class=line><span class=cl>            <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=n>py4j</span><span class=o>.</span><span class=n>protocol</span><span class=o>.</span><span class=n>Py4JError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span><span class=s2>&#34;Fall back to non-hive support because failing to access HiveConf, &#34;</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;please make sure you build spark with hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>TypeError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>conf</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;spark.sql.catalogImplementation&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()</span> <span class=o>==</span> <span class=s1>&#39;hive&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span><span class=s2>&#34;Fall back to non-hive support because failing to access HiveConf, &#34;</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;please make sure you build spark with hive&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span><span class=o>.</span><span class=n>builder</span><span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sc</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sparkContext</span>
</span></span><span class=line><span class=cl><span class=n>sql</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span>
</span></span><span class=line><span class=cl><span class=n>atexit</span><span class=o>.</span><span class=n>register</span><span class=p>(</span><span class=k>lambda</span><span class=p>:</span> <span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># for compatibility</span>
</span></span><span class=line><span class=cl><span class=n>sqlContext</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>_wrapped</span>
</span></span><span class=line><span class=cl><span class=n>sqlCtx</span> <span class=o>=</span> <span class=n>sqlContext</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;&#34;&#34;Welcome to
</span></span></span><span class=line><span class=cl><span class=s2>      ____              __
</span></span></span><span class=line><span class=cl><span class=s2>     / __/__  ___ _____/ /__
</span></span></span><span class=line><span class=cl><span class=s2>    _\ \/ _ \/ _ `/ __/  &#39;_/
</span></span></span><span class=line><span class=cl><span class=s2>   /__ / .__/\_,_/_/ /_/\_\   version </span><span class=si>%s</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>      /_/
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span> <span class=o>%</span> <span class=n>sc</span><span class=o>.</span><span class=n>version</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using Python version </span><span class=si>%s</span><span class=s2> (</span><span class=si>%s</span><span class=s2>, </span><span class=si>%s</span><span class=s2>)&#34;</span> <span class=o>%</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_version</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_build</span><span class=p>()[</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>platform</span><span class=o>.</span><span class=n>python_build</span><span class=p>()[</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;SparkSession available as &#39;spark&#39;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP,</span>
</span></span><span class=line><span class=cl><span class=c1># which allows us to execute the user&#39;s PYTHONSTARTUP file:</span>
</span></span><span class=line><span class=cl><span class=n>_pythonstartup</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;OLD_PYTHONSTARTUP&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>_pythonstartup</span> <span class=ow>and</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>isfile</span><span class=p>(</span><span class=n>_pythonstartup</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>_pythonstartup</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>code</span> <span class=o>=</span> <span class=nb>compile</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>(),</span> <span class=n>_pythonstartup</span><span class=p>,</span> <span class=s1>&#39;exec&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>exec</span><span class=p>(</span><span class=n>code</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=数据处理计算>数据处理、计算<a hidden class=anchor aria-hidden=true href=#数据处理计算>#</a></h2><p>读取数据得到 <code>Spark DataFrame</code> 后，可以直接对此进行操作，除了常见的业务分析还有机器学习模块（<code>MLlib</code>）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>raw_data</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=p>(</span><span class=s2>&#34;./kddcup.data.gz&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1>## Comma-Separated Value</span>
</span></span><span class=line><span class=cl><span class=n>csv</span> <span class=o>=</span> <span class=n>raw_data</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s2>&#34;,&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>metrics</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>[</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>4</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>5</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.mllib.stat</span> <span class=kn>import</span> <span class=n>Statistics</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>Statistics</span><span class=o>.</span><span class=n>corr</span><span class=p>(</span><span class=n>metrics</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&#34;spearman&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Statistics</span><span class=o>.</span><span class=n>corr</span><span class=p>(</span><span class=n>metrics</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&#34;pearson&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>值得一提的是，<strong>Spark DataFrame to pandas DataFrame</strong></p><p>可以用 <code>toPandas()</code> 方法，同时参数方面设置 <code>spark.sql.execution.arrow.enabled=true</code> 能提高效率</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Enable Arrow-based columnar data transfers</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>conf</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=s2>&#34;spark.sql.execution.arrow.enabled&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Generate a pandas DataFrame</span>
</span></span><span class=line><span class=cl><span class=n>pdf</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>rand</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a Spark DataFrame from a pandas DataFrame using Arrow</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>pdf</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert the Spark DataFrame back to a pandas DataFrame using Arrow</span>
</span></span><span class=line><span class=cl><span class=n>result_pdf</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;*&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>toPandas</span><span class=p>()</span>
</span></span></code></pre></div><p>这部分再Mark一个关于 <code>collect()</code> 的小点，总之数据量比较大的时候就不要用这个方法。</p><blockquote><p>The <code>collect()</code> function <strong>returns a list</strong> that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as <em>all the data is loaded in a driver&rsquo;s memory</em>, in which case we lose the benefits of distributing the data around a cluster of Spark instances.</p></blockquote><h2 id=如何处理结果>如何处理结果<a hidden class=anchor aria-hidden=true href=#如何处理结果>#</a></h2><p>处理、计算后的结果往往会再一次的落库，这个时候同 “数据读取” 的部分，🉑️ 根据具体情况进行检索。</p><p><strong>以落到 hive 表为例</strong>，截止到目前整理的，大致有两种方法。</p><p>首先确保数据为 Spark DataFrame 状态（可以通过 <code>spark.createDataFrame(df)</code> 的方法将 pandas DataFrame 转为 Spark DataFrame）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark_df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>&#34;overwrite&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;hive&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=s2>&#34;dbName.tableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 注意是 overwrite </span>
</span></span></code></pre></div><p>或者</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark_df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;myTempTableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;drop table if exists dbName.tableName&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;create table dbName.tableName as select * from myTempTableName&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>最近工作中遇到了 <code>PySpark</code> 的使用，在此从应用层小白视角通过 📦 “箱子模型”（Input => Box => Output） 简单记录大致的使用流程，方便于新手～</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ol><li>360数科深圳数据组</li><li>Rudy Lai and Bartłomiej Potaczek.《Hands On Big Data Analytics With PySpark》</li><li><a href=https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas>https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas</a></li><li><a href=http://spark.apache.org/docs/latest/api/python/index.html>http://spark.apache.org/docs/latest/api/python/index.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html>https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</a></li><li><a href=https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive>https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive</a></li><li><a href=https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py>https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://unclehuzi.github.io/tags/pyspark/>PySpark</a></li></ul><nav class=paginav><a class=prev href=https://unclehuzi.github.io/posts/life-movie-x-men/><span class=title>« Prev</span><br><span>X战警系列观影顺序</span></a>
<a class=next href=https://unclehuzi.github.io/posts/causal-resources/><span class=title>Next »</span><br><span>因果推断补给站</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on x" href="https://x.com/intent/tweet/?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&amp;hashtags=PySpark"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&amp;title=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;summary=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;source=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f&title=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on whatsapp" href="https://api.whatsapp.com/send?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92%20-%20https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on telegram" href="https://telegram.me/share/url?text=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&amp;url=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share PySpark之应用层小白视角 on ycombinator" href="https://news.ycombinator.com/submitlink?t=PySpark%e4%b9%8b%e5%ba%94%e7%94%a8%e5%b1%82%e5%b0%8f%e7%99%bd%e8%a7%86%e8%a7%92&u=https%3a%2f%2funclehuzi.github.io%2fposts%2fda-pyspark-beginning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://unclehuzi.github.io/>H.W.</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>