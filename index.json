[{"categories":["读书笔记"],"content":"其实，你是为了大发雷霆而生气😠 以一个简单且常见的例子展开说明 小明同学去餐馆用膳，结果紧张的服务员不小心把番茄酱洒在了小明新买的衣服。小明很生气，对服务员大发雷霆。 这波操作似乎还是符合剧情的，即 生气 $\\Rightarrow$ 大发雷霆 但阿德勒心理学的态度是，因为小明要大发雷霆所以才生气，是“目的论”。 （「插播」 阿德勒、荣格、弗洛伊德 仨人所谓是心理学“三巨头”） 我们结合 📦箱子模型来看：外界刺激（“衣服被弄脏”）$\\Rightarrow$ 小明这个整体 $\\Rightarrow$ 行为（大发雷霆）。最后这个行为是小明自己选择的。其实最后选择什么是小明的自由，但就是需要对选择负责 o.o 我是被开篇这个观点吸引了，觉得很有意思。但也有不同的观点：认为是生硬的套上这一层目的。（不排除是我说的不富有感染力 😂） 借助这个例子是要想表达 我们应该立足于目的论而不是弗洛伊德的原因论；不可以从过去中找原因；要否定精神创伤；人不是受过去原因支配的存在，人是为了达成某种目的而采取行动的。 过去已然无法改变，我们需要专注于当下，做好此时此刻该做的事儿。过去和未来根本不存在，所以才要谈现在。起决定作用的既不是昨天也不是明天，而是“此时此刻”。 话又说回来，有时候在当下受挫或者受到些打击之类的很难专注于当下。但我们需要的是接受普通而又平庸的自己o.o “自我接纳”，而不是自我肯定，诚实的对待自己。正所谓，知之为知之，不知为不知，是知也。 我一直尝试用一句话来抽象概括全书😂 似乎是想营造一种“我理解了”的氛围。我还是诚实的对待自己吧😂 凭着自己的理解，还是有这么一张图👇 作为一个独立自由个体的我生活在这个世界上总会遇着你和他，即人际关系。这也带来了人生三大课题：👬交友、💻工作和❤️爱。而面对这三大课题，我们总会有烦恼、不开心的时候。为此，我们需要做的是“课题”分离，通过 责任方（“某种选择所带来的结果最终要由谁来承担？”） 确定这是谁的课题。此外，通过“自我接纳”、“他者信赖”、“他者贡献”建立起“共同体感觉”，以更好的解决人际关系的问题。 人的目标是追求幸福，开心快乐最为重要。正所谓重点不是说什么，而是做什么。经过独立、自由的个体选择后所产生的行动，肯定是有“爽点”的。或许是当下、或许是以后、或许…阿德勒心理学提倡、坚守的这些也是为了能增加自身的幸福感。 书中提到， 甚至也有人说要想真正理解阿德勒心理学直至改变生活方式，需要“相当于自身岁数一半的时间”。也就是说，如果40岁开始学的话，需要20年也就是到60岁才能学会。20岁开始学的话，加上10年，得到30岁才能学会。 修生养性的路子还长着呢～但在路上就是好的 🌞 ","date":"2022-03-01","objectID":"/2022/03/notes-the_courage_to_be_disliked/:0:0","tags":["心理学"],"title":"《被讨厌的勇气》","uri":"/2022/03/notes-the_courage_to_be_disliked/"},{"categories":["FEM"],"content":"台大经济学原理 课程笔记之“Efficiency of Markets” 步入新的篇章：市场与福利。福利经济学 welfare economics，研究资源配置如何影响经济福利。 ","date":"2022-03-01","objectID":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/:0:0","tags":["经济学","课程笔记"],"title":"Ch7-市场效率","uri":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/"},{"categories":["FEM"],"content":"消费者剩余 支付意愿（willingness to pay）：买者愿意支付的最高价格。衡量买者对物品的评价 消费者剩余（consumer surplus）：买者愿意为一种物品支付的量 减去 其为此实际支付的量。衡量买者从参与市场中得到的利益。 （类比营销里的 满意度=实际-预期） 需求曲线以下和价格以上的面积衡量一个市场上的消费者剩余（结合积分、极限来理解） 价格降低消费者剩余增加。之前爽到的更爽，之前没爽到的现在爽到了。“现在”是指价格降低后的时点 ","date":"2022-03-01","objectID":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/:1:0","tags":["经济学","课程笔记"],"title":"Ch7-市场效率","uri":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/"},{"categories":["FEM"],"content":"生产者剩余 生产者剩余（producer surplus）：卖者得到的量减去其生产成本。衡量卖者从市场中得到的利益 买者愿意为一种物品支付的量 减去 其为此实际支付的量。衡量买者从参与市场中得到的利益。 价格之下和供给曲线以上的面积衡量一个市场上的生产者剩余 价格上升生产者剩余增加。同理，之前爽到的更爽，之前没爽到的现在爽到了。“现在”是指价格上升后的时点 $$ \\begin{equation} \\begin{split} 总剩余 \u0026= 消费者剩余+生产者剩余\\ \u0026=(买者的评价-买者支付的量)+(卖者得到的量-卖者成本)\\ \u0026=买者的评价-卖者的成本 \\end{split} \\end{equation} $$ ","date":"2022-03-01","objectID":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/:2:0","tags":["经济学","课程笔记"],"title":"Ch7-市场效率","uri":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/"},{"categories":["FEM"],"content":"结论：市场效率和市场失灵 书中指出，市场失灵是指一些不受管制的市场不能有效的配置资源。而“市场势力”和“外部性”是典型的 🌰例子。“市场势力”是指影响价格的能力：在一些市场，某个单个（或一小群）卖者或买者可以控制价格。“外部性“是指买者和卖者的决策会影响那些不参与市场的人，譬如农药的使用不仅影响生产商和农民，还影响呼吸、饮用被农药污染的空气、水的其他人。 但张维迎教授认为 传统经济学所谓的“完全竞争”，实际上是没有竞争；所谓的“垄断”，实际上是真实市场中的竞争手段；外部性本质上是个产权界定问题；信息不对称是以分工为基础的市场经济的基本特征，市场本身是解决信息不对称的有效机制。因此，所谓的市场失灵，实际上是传统市场理论的失灵，不是市场本身的失灵；所谓的垄断、外部性和信息不对称的存在，都不构成政府干预市场的正当理由。——《经济学原理》 ","date":"2022-03-01","objectID":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/:3:0","tags":["经济学","课程笔记"],"title":"Ch7-市场效率","uri":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/"},{"categories":["FEM"],"content":"Reference ","date":"2022-03-01","objectID":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/:4:0","tags":["经济学","课程笔记"],"title":"Ch7-市场效率","uri":"/2022/03/notes-principles-of-economics-ch7-efficiency_of_markets/"},{"categories":["FEM"],"content":"台大经济学原理 课程笔记之“Supply, Demand and Government Policies” 经济受两种规则体系支配：供需规律和政府制定的法规。本章主要是举例分析政府介入后的供需变化 ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:0:0","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"价格控制 ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:1:0","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"价格上限（Price Ceiling） 定义 政府規定價格不能超過一定金額 non-binding 若把價格定在均衡之上，則此約束無效，但若有一個供給面的衝擊發生(s 移往 s’)時， binding 則有可能變成有效，造成短缺。 例： 1970s 的石油危機衝擊造成 non-binding（線段 bc）意外轉變為短缺(線段 ab) binding 發生效力時會造成供不應求 例一：房租上限，政府往往為了保障弱勢團體有房子可住，常規定一價 格上限，但卻造成房子短缺（如線段 ab），反而害了想幫助的人。 ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:1:1","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"价格下限（Price floor） 定义 政府規定價格不能低於一定金額 non-binding: 若把價格定於均衡價格之下，則此限制無效 binding 發生效力時會造成供過於求（线段 bc） 譬如，最低工资法。劳动力即为供给方。 (1) 工會的組成 雖然有可能造成大規模的分配，但更可能的情況是工會內的既得利益者把持權力，只爭取已經有工作者的權益，沒有工作的人仍然沒有工作 (2) 老闆寧缺不補 政府只能規定最低工資，卻無法規定僱主的僱用意願，此時失業者難以獲得工作，政府政策反而只保護到既有勞工而非潛在勞工 (3) 企業主找到替代品 例如機器人的使用，有可能造成勞動需求的下降，嚴重者可能更多人失業（造成交點在 b 點左邊），如麥當勞在日前在歐洲 7000 個地方買進 touch screen cashier 便有可能造成這個效果 关于最低工资法，最有意思的当属2021年的诺贝尔经济学奖获得者之一：戴维·卡德（David Card）。David Card 曾經在 AER (American Economic Journal, one of top 4 American Economic journals) 發過一篇文章，說明最低工資不會讓失業增加，發表之時立即引起軒然大波，不過曾獲麥克阿瑟天才獎的芝大經濟系莫菲（ Kevin Murphy）教授卻發現，「 當次的基本工資上升。 整體來說，使得新州的十五到二十四歲年輕人、非裔美人以及女性的就業率大幅下降，而這些正是構成邊際勞工的最重要族群！ 」 我想林明仁教授在讲授这门课程的时候怎么也不会想到他能拿诺奖吧😂 ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:1:2","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"税收 除了价格管制，政府还会通过 税收 影響市場 无论是向生产者征税还是向需求者征税都会得到相同的结果：消费者负担紅色線段，生產者負擔綠色線段，政府獲得黑色線段的稅收。看似单方面征税，实则供给双方均贡献了税收 譬如冰激凌🍦 的例子，原本市场达到了 $P_0$ 的均衡 此时政府对生产者征税 $P_x$ 1⃣️ 生产曲线上移，达到新的均衡价格 $P_1$ ，也是消费者支付的价格 2⃣️ 此时生产者实际得到的是 $P_1-P_x$ 3⃣️ 相比于 $P_0$ 状态，消费者多支付了 $P_1-P_0$ 即红色线段；生产者少了 $P_0-(P_1-P_x)$ 即绿色线段 此时政府向消费者征税 $P_x$ 1⃣️ 生产曲线下移，达到新的均衡价格 $P_1$ ，也是生产者所得到的收益 2⃣️ 因为对消费者征税，所以消费者支付的价格为 $P_1+P_x$ 3⃣️ 相比于 $P_0$ 状态，消费者多支付了 $P_1+P_x-P_0$ 即红色线段；生产者少了 $P_0-P_1$ 即绿色线段 當政府想改善公平性的時候，可以藉由課稅達成這個目標，因為其會進行租稅轉嫁，依據 bargaining power 來決定，而 bargainging power 又可由彈性看出（彈性是圖中供給需求線的斜率改變）。即负担比例更多取决于弹性如何。 相比需求，供给更富有弹性 此时税收主要由消费者承担 相比供给，需求更富有弹性 此时税收主要由生产者承担 1990年美国对游艇征税 相比游艇制造商，富人需求更富有弹性，替代品多。 此时税收主要由生产者即游艇制造商承担 ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:2:0","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"Reference ","date":"2022-02-27","objectID":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/:3:0","tags":["经济学","课程笔记"],"title":"Ch6-供给、需求与政府政策","uri":"/2022/02/notes-principles-of-economics-ch6-supply_demand_and_government_policies/"},{"categories":["FEM"],"content":"台大经济学原理 课程笔记之“Elasticity and its Application” 彈性（Elasticity）是用來衡量某一被解釋變數（Y）對其解釋變數（X）變動是否敏感的指標。 A measurement of responsiveness of variable Y to one of its determinants, X. 中文里提到「 有彈性」往往指容易受外力影響，圓融；而「 沒彈性」 🈯️不知變通，正直。 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:0:0","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"需求弹性 $$ 需求的价格弹性=\\frac{需求变化百分比}{价格变化百分比} $$ 备注：被解释变数（Y）为分子，解释变数（X）为分母 ⚠️ 同时也需要注意，参考点不同，弹性不同，如下图所示 而且不同点之间的弹性也不同 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:1:0","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"需求曲线 结合弹性来看，对应有5类 Name_cn name_en |Elasticity| 完全无弹性 Perfectly Inelastic $=0$ 完全弹性 Perfectly Elasticity $\\infty$ 无弹性需求 Inelastic Demand $\u003c1$ 单位弹性 Unit Elastic $=1$ 弹性需求 Elastic Demand $\u003e1$ ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:1:1","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"决定因素 替代品的多寡與強弱： 替代品多、強→彈性大 Ex: 便利商店裡的飲料 時間長短： the longer the time period → The larger the elasticity ，事緩則圓 Ex: gas↑ 一開始大家沒辦法只好忍耐；時間一久，改搭公車， 叫政府蓋捷運，或換省油車→汽油需求量下降幅度變大 Ex: 野蠻女友或宅男男友， 短期雞肋效果 vs.長期汰換效果 市場的定義： 定義愈窄， 彈性愈大 Ex: 松阪牛肉 vs.牛肉 vs. 食物， Lexus 460 vs. car →替代品多寡 必需品 vs. 奢侈品 必需品佔所得比例小，彈性小，奢侈品則反之。 Ex: visiting doctor vs. buying a sailboat 支出佔所得的比例越高，彈性越大。 Ex: 房價 vs.菜價 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:1:2","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"Case Analysis — 总收入 通过需求弹性的角度来分析 总收入 Total Revenue（TR） （这里又以坐标轴为准 $P=f(Q)$ ， $TR$ 是关于 $Q$ 的函数） $$ TR=P\\times Q=P(Q)\\times Q $$ 需求曲线上的点与坐标轴围成的矩形面积即为 TR ，其中，中点处 TR 最大 无弹性需求，价格⬆️ TR⬆️ 弹性需求，价格⬆️ TR⬇️ ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:1:3","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"其他的需求弹性 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:1:4","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"供给弹性 $$ 供给的价格弹性=\\frac{供给变化百分比}{价格变化百分比} $$ ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:2:0","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"供给曲线 Name_cn name_en |Elasticity| 完全无弹性 Perfectly Inelastic $=0$ 完全弹性 Perfectly Elasticity $\\infty$ 无弹性供给 Inelastic Supply $\u003c1$ 单位弹性 Unit Elastic $=1$ 弹性供给 Elastic Supply $\u003e1$ ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:2:1","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"决定因素 生产投入是否具备多种用途 比亚迪生产口罩 时间长短，时间拉长较有弹性 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:2:2","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"Case Analysis 旨在 结合弹性分析供、需 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:3:0","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"谷贱伤农 谷物产量增加，供给曲线 👉右移，价格下降，但人们对谷物的需求无弹性，所以Total Revenue 减少 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:3:1","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"OPEC无法维持高油价 备注：参考资料横坐标标注有误 OPEC 联合减少石油供给，早期且短期内替代品较少，无弹性需求 $\\Rightarrow$​ 短期内价格 $\\uparrow$​​ （左图） 随着时间的推移，相应替代品增加，转为弹性需求 $\\Rightarrow$​​ 价格回落（右图） ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:3:2","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"打击毒品 👊“扫毒” 的点在于很难甚至不可能做到把供应转为 0 作者认为，要对人们进行教育，减少人们的毒品需求。 ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:3:3","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"Reference ","date":"2022-02-23","objectID":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/:4:0","tags":["经济学","课程笔记"],"title":"Ch5-弹性及其应用","uri":"/2022/02/notes-principles-of-economics-ch5-elasticity_and_its_application/"},{"categories":["FEM"],"content":"台大经济学原理 课程笔记之“Supply and Demand” 四种市场结构 完全竞争 垄断 寡占 Oligopoly 独占性竞争 Monopolistic Competition ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:0:0","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"Demand 需求量 (Quantity of Demand) : The amount of a good that buyers are willing and able to purchase. 在某种价格下，买家愿意且能购买的量。 需求法则 (Law of demand) : Other things being equal, the quantity demanded of a good falls when the price of the good rises. 其他情况不变的情况下，通常来说价格与需求量为负向关系。但也有意外 (比例非常少) 👇 Conspicuous goods (炫耀性商品) Veblen good 价格越高可能需求量越高 The Theory of the Leisure Class: An Economic Study of Institutions (1899), by Thorstein Veblen (《有闲阶级论》) 也是研究奢侈品的理论基础。 Giffen’s good (季芬財) 愛爾蘭經濟學家發現，由於當時經濟不佳、馬鈴薯是最便宜的食物，因此在馬鈴薯歉收導致價格上升之際，反而會有人搶購，畢竟就算買更貴的馬鈴薯也比其它食物便宜。 「MARK」关于需求曲线中横纵坐标的问题 如下图所示，横轴（X）是 Quantity 、纵轴（Y）是 Price，习惯性理解是X影响Y，即$Y=f(X)$ 但需求曲线描述的是，给定价格P，需求量Q几何，即 $Q=f(P)$​。林老师给的解释是鼻祖 Alfred Marshall 搞错了 P.S. 市場中不太可能每個人決定不買的價格相同，在此情況下加總，會產生許多拗折 (kinked)，最後就會變成一個有弧度的曲線。 为啥是凸的呢？私认为，高价的时候价格稍微降低一些，买的人也不会增加多少；极端至免费，不要白不要了，需求量增加很多。 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:1:0","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"需求量的改变 v.s. 需求曲线的移动 Change in demand quantity 🆚 Shift in demand curve 前者是沿着需求曲线移动；后者是需求曲线发生移动 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:1:1","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"影响需求曲线移动的因素 所得 Incomes normal good（income 增加，需求增加） 🆚 inferior good（income 增加， 需求减少） 但也取决于定义方式 相关物品价格 Prices of related goods 替代品 想要得到某種體驗收穫時，可以用A也可以用B，則A與B就是消費上的替代。 When a fall in the price of one good reduces the demand for another good, the two goods are called substitutes. A商品的需求量增加、B商品的需求量就會減少。 互补品 消費A商品時必須要和B商品一起消費。A商品的需求量增加、B商品的需求量也就會增加。 如，買鞋子不能只買左腳； 買爆米花配電影 偏好 Tastes 这个比较难以衡量。 买家人数 Number of buyers 市場需求是個人需求的平行加總，買家數量越多就會增加市場需求量。 对未来的预期 Expectations 如果可以做跨期替代時，像是知道之後購買會比較便宜就會減少當下的需求量，或是知道颱風來價格會上漲於是就先大量購買。 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:1:2","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"Supply 供给量：在给定的价格，生产者愿意且能生产的数量。 其他情况不变的情况下，通常来说价格与需求量为正向关系。 很多情况类比 #需求 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:2:0","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"影响供给曲线移动的因素 相关物品价格：替代、互补（生产者角度） 替代：整车厂生产轿车、SUV 互补举例：猪脑、猪排骨🥩、五花肉 生产要素价格（input price） 生产技术 预期：囤积居奇 卖方人数 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:2:1","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"Equilibrium 供给均衡 当价格到达某个水平，供给量=需求量 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:3:0","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"价格 \u003e 均衡价格： 剩余 Surplus 超额供给 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:3:1","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"价格 \u003c 均衡价格：短缺 Shortage 超额需求 「MARK」 均衡分析不一定是实际的交易点。在一个范围内波动，但相差不大 供给 和 需求 是独立的 买卖不成，供需在 供需价格为负的情况 譬如视频中提到 某老师（supply）为了让学生（demand）听课赠送咖啡 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:3:2","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"分析均衡变化 Decide whether the event shifts the supply or demand curve (or perhaps both). 确定供给、需求曲线是否移动 Decide in which direction the curve shifts. 确定移动方向 Use the supply-and-demand diagram to see how the shift changes the equilibrium price and quality. 看图说话，新的平衡点情况 改变需求 天气热 =》 冰激凌需求增加 改变供给 冰激凌的原材料——糖的价格增加 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:3:3","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"供给九宫格 ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:3:4","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["FEM"],"content":"Reference ","date":"2022-02-15","objectID":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/:4:0","tags":["经济学","课程笔记"],"title":"Ch4-供给和需求","uri":"/2022/02/notes-principles-of-economics-ch4-supply_and_demand/"},{"categories":["读书笔记"],"content":"对于“心理咨询”，现在我觉得是被污名化了，人们也有些误解 过去总觉得是心理有问题了才搞些“心理咨询”什么的，就像是只有生病了才去找医生👨‍⚕️ 基于《中国文化的深层结构》我更加认为“心理学”、“心理咨询”等等皆为舶来品（⚠️未考证）。或许是历史原因，中国自古以来对于“身心”健康更加重视的“身”，而对于自我“心”方面的关心较少；甚至认为“攻心为上”的捷径为“身”。所以，我大胆揣测心理学方面的研究较少。 对于《蛤蟆去看心理医生》这本书也会结合自身的心理咨询经历，其中也有些产生共鸣的地方。譬如书中有提到“老师用讲解模式，咨询师用倾听模式”，我在校咨询的时候早期还真的不习惯“倾听模式”，毕竟从小到大接触的都是灌输式的“讲解模式”，随着咨询次数的增加，后续慢慢的很享受这种交流方式。 之前和某同学聊天的时候得知，该同学在校期间心情不好或有啥想不通的时候就会预约学校的心理咨询 在此我不得不说华师的心理咨询很是认真、负责且专业了。正所谓没有对比就没有伤害 《蛤蟆去看心理医生》是以蛤蟆陷入抑郁为背景，以此进行心理咨询。（在此我还是想说并不是抑郁或心理疾病了才想着找心理咨询，尤其是在学校的时候。社会上的心理咨询成本确实较高） 书中反复提到蛤蟆现在的状态更多是和儿童时期的经历有关（有时候我不得不想以后怎么教育自己的孩子😂） 诚然过去已无法改变，我们现在、当下能做的是改变对过去的看法以此修身养性、强大自己，将自我从过去中解放出来。 我认为没有人能‘让’我们产生什么感受，除非他们用蛮力胁迫你。说到底，是我们‘选择’了自己的感受。我们‘选择’了愤怒，我们‘选择’了悲伤。” 这和“箱子模型”不谋而合了。外界的刺激 =\u003e 我 =\u003e 反应，“我”便是那个箱子。 书中通过“我是怎么看自己的”以及 “我是怎么看别人的”得到二维矩阵，即 我认为： 我好，你好 正所谓“大家好，才是真的好” 我好，你不好 “挑剔型家长” 我不好，你好 我不好，你不好 这些理念只是用来理解行为的方法，尤其是理解我们自己的行为。 记得在校咨询时，最后有个经验是在理性时（即“成人状态”）针对过去苦恼或自己讨厌的事情多想想解决方案，以备不时之需，虽然存在一定的“转化”（即未必能100%执行），但比过去进步了、心态好了就是进步呐。我想这也是自我成长的一个过程吧 ","date":"2022-02-08","objectID":"/2022/02/notes-counselling-for-toads/:0:0","tags":["心理学"],"title":"其实可以试试心理咨询","uri":"/2022/02/notes-counselling-for-toads/"},{"categories":["生活"],"content":"试图记录📝一些感悟，有些东西记着记着就成「原则」了吧 「MARK」如何解决“剧场效应”（以教育为例）的问题？（2022-02-05） “环境”越动荡的时候越要沉住气、保持学习，以备反弹的时候有机会顺势而为（2022-01-29） 当犹豫不决、拿不准的想法时，不要急于实施，多和朋友们（尤其是过来人）推心置腹的好好聊聊，多听听不同的声音。（2022-01-01） ","date":"2022-01-29","objectID":"/2022/01/life-thoughts/:0:0","tags":["鸡汤"],"title":"感悟？=\u003e原则？","uri":"/2022/01/life-thoughts/"},{"categories":["读书笔记"],"content":"读后感？可还行？ 从腾讯视角初窥中国互联网。之所以这么说是因为各大巨头的发展与壮大离不开当时的环境以及“企业家精神”，而本书也是在中国互联网的背景下记录下腾讯的一些里程碑事件。 《腾讯传》这本书于2016年出版，作者吴晓波将腾讯的发展分为三大时间段： 创业：1998—2004 出击：2005—2009 巨头：2010—2016 撸完之后让我产生了一些想法吧，难以说中国互联网的发展史，更多是其他书籍甚至学科之间的碰撞与融合。 ","date":"2022-01-22","objectID":"/2022/01/notes-bio-of-tencent/:0:0","tags":["传记"],"title":"《腾讯传》","uri":"/2022/01/notes-bio-of-tencent/"},{"categories":["读书笔记"],"content":"领先者的跟随策略 在校期间，浪迹图书馆时，翻看了一本博弈论方面的科普类书籍，大致记得有个理论的应用是说，领先者可通过模仿跟随者的策略维持其领先地位。 书中有句话是这么说的， 紧盯市场动态，以最快的方式复制成功者模式，利用QQ用户优势进行后发超越 而且书中也提到，业界对腾讯的评价大多是“抄袭”。其实这也是一种竞争策略啊😂 看到这我又想到了现在（2022年1月）的字节跳动，截止目前，字节依托抖音的用户优势陆陆续续差不多把目前各大互联网公司干的事儿都实现了：电商、本地生活、汽车媒体、长视频、房产中介、支付、借贷等等等 其实这种，从1到N快速孵化的模式，中台的优势是体现的淋漓尽致了。 书中的那句话就变成，【字节】 紧盯市场动态，以最快的方式复制成功者模式，利用抖音用户优势进行后发超越 是不是可以这么总结，互联网时代需要掌握流量，“流量”通过（新工具）解决满足中国特色的需求而获得 “领先者的跟随策略”可以说是通用性的“经验”，但是放在个案中是不是又是另外一回事儿呐，因为撸完后也有了第二个困惑 ","date":"2022-01-22","objectID":"/2022/01/notes-bio-of-tencent/:1:0","tags":["传记"],"title":"《腾讯传》","uri":"/2022/01/notes-bio-of-tencent/"},{"categories":["读书笔记"],"content":"为什么是新浪微博？ 对啊，有QQ用户的基础及导流，为什么不是腾讯微博呐？ 关于微博之间的战争书中笔墨并没“3Q大战”（360 v.s. QQ）多。作者似乎更多是归因于时间上的问题 腾讯微博上线于2010年5月，比新浪微博迟了整整8个月，这对于一个战略性产品来说，几乎是难以追赶的时间距离。——《腾讯传》 或许是我对微博无感，也或许是因为我懒，依然想不通为什么是新浪微博 🤔 至少在内因方面难以得到一手信息 o.o 对市场快速作出反应并“跑马圈地”的重要性确实无需多言，但腾讯的“模仿”大法为什么在“微博”这块儿失效了呢？🤔 几乎所有的观察家都意识到，在白热化的微博一战中，腾讯对新浪的取胜概率十分渺茫，“能够战胜微博的，一定不是另外一个微博”，如果没有新的战略级产品诞生——正如迈克尔·波特所提示的，“挑战者必须找到不同于领先者的新竞争方式以取得成功”，腾讯在移动互联网时代的未来无疑是黯淡的。——《腾讯传》 但我觉得这段话更像是引出下一章《微信：移动互联时代的“站台票”》的内容。 ","date":"2022-01-22","objectID":"/2022/01/notes-bio-of-tencent/:2:0","tags":["传记"],"title":"《腾讯传》","uri":"/2022/01/notes-bio-of-tencent/"},{"categories":["读书笔记"],"content":"下一个终端？ 2011年1月21日推出“微信”，腾讯张小龙谦虚的说通过微信拿到了移动互联时代的“站台票”。 基于我目前的认知，我一直按照终端的变革来切分互联网发展的时间点，即 PC =\u003e 手机。近十年来手机带来的便利性、普及性可谓是有目共睹。各大互联网公司依托于手机终端开展各自的业务，借助于手机终端打破对于人、产品和信息的时空限制。 那么，这就引发出了下一代终端是啥？未来依托什么呐？ 留意观察身边生活、活跃在数字时代的亲朋好友，放眼望去，大家鼻梁上都架着一副眼镜。可见，眼镜有天然的优势，那么眼镜有没有可能成为下一代终端呐？🤔 根据“智猪博弈”，我们可以等待大厂的结果~毕竟谷歌、苹果、华为等已经有推出各自的眼镜产品了。 本文可谓是以“大”见“小” / 管中窥豹 了，从《腾讯传》长文中 zhái 出某些点简单展开 o.o 更多是了解性质的范畴 ","date":"2022-01-22","objectID":"/2022/01/notes-bio-of-tencent/:3:0","tags":["传记"],"title":"《腾讯传》","uri":"/2022/01/notes-bio-of-tencent/"},{"categories":["生活"],"content":"巴菲特：不纠结过去的事，纠结也没用。人生只能向前看。 🔗 原文链接 1998年巴菲特在佛罗里达大学商学院做了一场演讲，段永平曾力荐这场演讲，并坦言自己看了不下10次。这场演讲也被人们称为：“这是巴菲特最经典的演讲，没有之一。” ","date":"2022-01-15","objectID":"/2022/01/life-forward/:0:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"一、成功不只取决于智商和勤奋 我先简单说几句，把大部分时间留下来回答大家的问题。我想聊聊大家关心的话题。 请各位提问的时候一定要刁钻。你们问的问题越难，才越好玩。什么都可以问，就是不能问上个月我交了多少税，这个问题我无可奉告。 各位同学，你们毕业之后未来会怎样？ 我简单说说我的想法。各位在这所大学能学到大量关于投资的知识，你们将拥有成功所需的知识。 既然各位能坐在这里，你们也拥有成功所需的智商，你们还有成功所需的拼劲。你们大多数人都会成功地实现自己的理想。 但是最后你到底能否成功，不只取决于你的头脑和勤奋。 我简单讲一下这个道理。 奥马哈有个叫彼得•基威特的人，他说他招人的时候看三点：品行、头脑和勤奋。 他说一个人要是头脑聪明、勤奋努力，但品行不好，肯定是个祸害。品行不端的人，最好又懒又蠢。 我知道各位都头脑聪明、勤奋努力，所以我今天只讲品行。为了更好地思考这个问题，我们不妨一起做个游戏。 各位都MBA二年级的学生，应该很了解自己周围的同学了。假设现在你可以选一个同学，买入他今后一生之内10%的收入。 你不能选富二代，只能选靠自己奋斗的人。请各位仔细想一下，你会选班里的哪位同学，买入他今后一生之内10%的收入。 你会给所有同学做个智商测试，选智商最高的吗？未必。你会选考试成绩最高的吗？未必。 你会选最有拼劲的吗？不一定。因为大家都很聪明，也都很努力，我觉得你会主要考虑定性方面的因素。 好好想想，你会把赌注压在谁的身上？也许你会选你最有认同感的那个人，那个拥有领导能力，能把别人组织起来的人。 这样的人应该是慷慨大方的、诚实正直的，他们自己做了贡献，却说是别人的功劳。我觉得让你做出决定的应该是这样的品质。 找到了你最钦佩的这位同学之后，想一想他身上有哪些优秀品质，拿一张纸，把这些品质写在纸的左边。 下面我要加大难度了。为了拥有这位同学今后一生10%的收入，你还要同时做空另一位同学今后一生10%的收入，这个更好玩。 想想你会做空谁？你不会选智商最低的。你会想到那些招人烦的人，他们可能学习成绩优秀，但你就是不想和他们打交道，不但你烦他们，别人也烦他们。 为什么有人会招人烦？原因很多，这样的人可能自私自利、贪得无厌、投机取巧或者弄虚作假。类似这样的品质，你想想还有什么，请把它们写在刚才那张纸的右边。 看看左右两边分别列出来的品质，你发现了吗？这些品质不是把橄榄球扔出60米，不是10秒钟跑完100米，不是相貌在全班最出众。 左边的这些品质，你真想拥有的话，你可以有。这些是关于行为、脾气和性格的品质，是能培养出来的。在座的各位，只要你想要获得这些品质，没一个是你得不到的。 再看一下右边的那些品质，那些令人生厌的品质，没一个是你非有不可的，你身上要是有，想改的话，可以改掉。 大多数行为都是习惯成自然。我已经老了，但你们还年轻，想摆脱恶习，你们年轻人做起来更容易。 常言道，习惯的枷锁，开始的时候轻的难以察觉，到后来却重的无法摆脱。这话特别在理。 我在生活中见过一些人，他们有的和我年纪差不多，有的比我年轻十几二十几岁，但是他们染上了一些坏习性，把自己毁了，改也改不掉，走到哪都招人烦。他们原来不是这样的，但是习惯成自然，积累到一定程度，根本改不了了。 你们还年轻，想养成什么习惯、想形成什么品格，都可以，就看你自己怎么想了。 本•格雷厄姆，还有他之前的本•富兰克林，他们都这么做过。 本•格雷厄姆十几岁的时候就观察自己周围那些令人敬佩的人，他对自己说：“我也想成为一个被别人敬佩的人，我要向他们学习。”格雷厄姆发现学习他敬佩的人，像他们一样为人处世，是完全做得到的。 他同样观察周围遭人厌恶的人，摆脱他们身上的缺点。我建议大家把这些品质写下来，好好想想，把好品质养成习惯，最后你想买谁 10% 的收入，就会变成他。 你已经确定拥有自己100%的收入，再有别人的10%，这多好。你选择了谁，你都可以学得像他一样。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:1:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"二、救赎长期资本管理公司的启示 这件事非常耐人寻味。长期资本管理公司的由来，相信在座的大多人都知道，实在太令人感慨了。 约翰•梅里韦瑟、艾瑞克•罗森菲尔德、拉里•希利布兰德、格雷格•霍金斯、维克多•哈格哈尼，还有两位诺贝尔奖桂冠得主罗伯特•默顿和迈伦•舒尔兹。 把他们这16个人加起来，他们的智商该多高，随便从哪家公司挑16个人出来，包括微软，都没法和他们比。 第一，他们的智商高得不得了。 第二，他们这 16 个人都是投资领域的老手。他们不是倒卖服装发的家，然后来搞证券的。他们这16个人加起来，有三四百年的经验了，一直都在投资这行摸爬滚打。 第三，他们大多数人都几乎把自己的整个身家财产都投入到了长期资本管理公司，他们把自己的钱也投进去了。他们自己投了几亿的钱，而且智商高超，经验老道，结果却破产了。真是让人感慨。 要让我写一本书的话，书名我都想好了，就叫《聪明人怎么做蠢事》，我的合伙人说我的自传可以叫这个名字。 但是，我们从长期资本这件事能得到很多启发：长期资本的人都是好人。我尊重他们。当我在所罗门焦头烂额的时候，他们帮过我。他们根本不是坏人。 但是他们为了赚更多的钱，为了赚自己不需要的钱，把自己手里的钱，把自己需要的钱都搭进去了。这不是傻是什么？绝对是傻，不管智商多高，都是傻。 为了得到对自己不重要的东西，甘愿拿对自己重要的东西去冒险，哪能这么干？ 我不管成功的概率是100比1，还是1000比1，我都不做这样的事。 假设你递给我一把枪，里面有1000个弹仓、100万个弹仓，其中只有一个弹仓里有一颗子弹，你说：“把枪对准你的太阳穴，扣一下扳机，你要多少钱？” 我不干。你给我多少钱，我都不干。 要是我赢了，我不需要那些钱；要是我输了，结果不用说了。这样的事，我一点都不想做，但是在金融领域，人们经常做这样的事，都不经过大脑。 有一本很好的书，不是书好，是书名好。这是一本烂书，但是书名起得很好，是沃尔特•古特曼写的，书名是《一生只需富一次》。这个道理难道不是很简单吗？ 假设年初你有1亿美元，如果不上杠杆，能赚10%，上杠杆的成功率是99%，能赚20%，年末时你有1.1亿美元，还是1.2亿美元，有区别吗？没一点区别。 要是年末你死了，写讣告的人可能有个笔误，虽然你有1.2亿，但他写成了1.1亿。 多赚的钱有什么用？一点用没有。对你、对你的家人，对别人，都没用。 要是亏钱了的话，特别是给别人管钱，亏的不但是钱，而且颜面扫地、无地自容，把朋友的钱都亏了，没脸见人。 我真理解不了，怎么有人会像这16个人一样，智商很高、人品也好，却做这样的事，一定是疯了。他们吃到了苦果，因为他们太依赖外物了。 我临时掌管所罗门的时候，他们和我说，六西格玛的事件、七西格玛的事件伤不着他们。他们错了。只看过去的情况，无法确定未来金融事件发生的概率。 他们太依赖数学了，以为知道了一只股票的贝塔系数，就知道了这只股票的风险。要我说，贝塔系数和股票的风险根本是八竿子打不着。 会计算西格玛，不代表你就知道破产的风险。我是这么想的，不知道现在他们是不是也这么想了。 说真的，我都不愿意以长期资本为例。我们都有一定的概率会摊上类似的事，我们都有盲点，或许是因为我们了解了太多的细枝末节，把最关键的地方忽略了。 亨利•考夫曼说过一句话：“破产的有两种人，一种是什么都不知道的，一种是什么都知道的。”说起来，真是令人扼腕叹息。 同学们，引以为戒。我们基本上没借过钱，当然我们的保险公司里有浮存金。但是我压根没借过钱。 我只有1万块钱的时候都不借钱，不借钱不一样吗？我钱少的时候做投资也很开心。 我根本不在乎我到底是有1万、10万，还是100万。除非遇上了急事，比如生了大病急需用钱。 当年我钱很少，但我也没盼着以后钱多了要过不一样的生活。从衣食住行来看，你我之间有什么差别吗？ 我们穿一样的衣服，我们都能喝天赐的可口可乐，我们都能吃上麦当劳，还有更美味的DQ冰淇淋，我们都住在冬暖夏凉的房子里，我们都在大屏幕上看橄榄球赛。你在大电视上看，我也在大电视上看。我们的生活完全一样，没多大差别。 要是你生了大病，会得到良好的治疗。如果我得了大病，也会得到良好的治疗。我们唯一不一样的地方是我们出行的方式不同。 我有一架小飞机，可以飞来飞去，我特别喜欢这架飞机，这是要花钱的。除了我们出行的方式不同，你说有什么是我能做，但你做不了的吗？ 我有一份我热爱的工作，但我一直都在做我喜欢的工作。当年我觉得赚1000 美元是一大笔钱的时候，我就喜欢我的工作。 同学们，做你们喜欢的工作。要是你总做那些自己不喜欢的工作，只是为了让简历上的工作经历更漂亮，那你真是糊涂了。 有一次，我去做一个演讲，来接我的是一个 28 岁的哈佛大学的学生。我听他讲完了他的工作经历，觉得他很了不起。 我问他：“以后你有什么打算？”他说：“等我MBA毕业后，可能先进一家咨询公司，这样能给简历增加一些分量。” 我说：“你才28岁，已经有这么漂亮的工作经历了，你的简历比一般人的漂亮10倍。你还接着做自己不喜欢的工作，不觉得有点像年轻的时候把性生活省下来，留到岁数大的时候再用吗？ 或早或晚，你们都应该开始做自己真心想做的事。 我觉得我说的话，大家都听明白了。各位毕业之后，挑一个自己真心喜欢的工作，别为了让自己的简历更漂亮而工作，要做自己真心喜欢的。 时间久了，你的喜好可能会变，但在做自己喜欢的事的时候，早晨你会从床上跳起来。 我刚从哥伦比亚大学商学院毕业，就迫不及待地希望立刻为格雷厄姆工作。我说我不要工资，格雷厄姆说我要的薪水太高了，我一直骚扰他。 回到奥马哈后，我做了三年股票经纪人，一直给格雷厄姆写信，告诉他我发现的投资机会。 最后，我终于得到了机会，在他手下工作了一两年。那是一段宝贵的经历。总之，我做的工作始终都是我喜欢的。 你财富自由之后想做什么工作，现在就该做什么工作，这样的工作才是理想的工作。做这样的工作，你会很开心，能学到东西，能充满激情。每天会从床上跳起来，一天不工作都不行。 或许以后你喜欢的东西会变，但是现在做你喜欢的工作，你会收获很多。我根本不在乎工资是多少。不知怎么，扯得有点远了。 总之，如果你现在有1块钱，以为将来有2块钱的时候，自己能比现在过得更幸福，你可能想错了。你应该找到自己真心喜欢做的事情，投入地去做。 别以为赚10倍或20倍能解决生活中的所有问题，这样的想法很容易把你带到沟里去。 在不该借钱的时候借钱，或者急功近利、投机取巧，做自己不该做的事，将来都没地方买后悔药。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:2:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"三、喜欢什么样的公司？ 我喜欢我能看懂的生意。先从能不能看懂开始，我用这一条筛选，90%的公司都被过滤掉了。 我不懂的东西很多，好在我懂的东西足够用了。世界如此之大，几乎所有公司都是公众持股的。所有的美国公司，随便挑。 首先，有些东西明知道自己不懂的，不能做。 有些东西是你能看懂的。可口可乐，是我们都能看懂的，谁都能看懂。可口可乐这个产品从1886年起基本没变过。可口可乐的生意很简单，但是不容易。 我不喜欢很容易的生意，生意很容易，会招来竞争对手。我喜欢有护城河的生意。我希望拥有一座价值连城的城堡，守护城堡的公爵德才兼备。 有的生意，我看不出来十年后会怎样，我不买。一只股票，假设从明天起纽约股票交易所关门五年，我就不愿意持有了，这样的股票，我不买。 我买一家农场，五年里没人给我的农场报价，只要农场的生意好，我就开心。我买一个房子，五年里没人给我的房子报价，只要房子的回报率达到了我的预期，我就开心。 人们买完股票后，第二天一早就盯着股价，看股价决定自己的投资做得好不好。糊涂到家了。买股票就是买公司，这是格雷厄姆教给我的最基本的道理。 买的不是股票，是公司的一部分所有权。 只要公司生意好，而且你买的价格不是高得离谱，你的收益也差不了。投资股票就这么简单。 要买你能看懂的公司，就像买农场，你肯定买自己觉得合适的。没什么复杂的。这个思想不是我发明的，都是格雷厄姆提出来的。 我特别走运。19岁的时候，我有幸读到了《聪明的投资者》。我六七岁的时候就对股票感兴趣，11岁时第一次买股票。我一直都在自己摸索，看走势图、看成交量，做各种技术分析的计算，什么路子都试过。 后来，我读到了《聪明的投资者》，书里说，买股票，买的不是代码，不是上蹿下跳的报价，买股票就是买公司。我转变到这种思维方式以后，一切都理顺了。道理很简单。所以说，我们买我们能看懂的公司。 在座的各位，没有看不懂可口可乐公司的，但是某些新兴的互联网公司呢，我敢说，在座的各位，没一个能看懂的。 今年在伯克希尔的股东大会上，我说要是我在商学院教课，期末考试时，我会出这样的题目，告诉学生一家互联网公司的信息，让他们给这家公司估值。哪个学生给出了估值，我就给他不及格。 无论什么时候，都要知道自己在做什么，这样才能做好投资。 必须把生意看懂了，有的生意是我们能看懂的，但不是所有生意我们都能看懂。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:3:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"四、在商业中所犯的错误 对于我和我的合伙人查理•芒格来说，我们犯过的最大的错误不是做错了什么，而是该做的没做。 在这些错误中，我们对生意很了解，本来应该行动，但不知道怎么了，我们就在那犹豫来犹豫去，什么都没做。 有些东西我们不明白就算了，但有些东西是我们能看明白的，本来可以赚几十亿、几百亿的，却眼睁睁看着机会溜走了。 我本来可以买微软赚几十亿，但这不算数，因为我一直搞不懂微软。但是医药股，我本来是可以赚到几十亿的，这些钱是我该赚到的，我却没赚到。 当克林顿当局提出医疗改革方案后，所有的医药股都崩盘了。我们本来可以买入医药股大赚特赚的，因为我能看懂医药股，我却没做这笔投资。 至于各位能看到的错误，几年前我买入美国航空优先股是个错误。当时我手里闲钱很多。手里一有闲钱，我就容易犯错。 查理让我去酒吧喝酒去，别在办公室里待着。但我还是留在办公室，兜里有钱，就做了傻事。每次都这样。当时我买了美国航空的优先股。没人逼我，是我自己要买的。 现在我有一个800热线电话，每次我一想买航空股，就打这个电话。电话那边的人会安抚我。 我说：“我是沃伦，又犯了想买航空公司的老毛病。” 他们说：“继续讲，别停下，别挂电话，别冲动。”最后那股劲就过去了。 我买了美国航空以后，差点把所有钱都亏进去，真是差一点全亏了。我活该亏钱。 我买入美国航空，是因为它是一只很合适的证券，但它的生意不好。对所罗门的投资也一样。我根本不想买它的生意，只是觉得它的证券便宜。这也算是一种错误。 本来不太喜欢公司的生意，却因为喜欢证券的条款而买了。这样的错误我过去犯过，将来可能还会犯。最大的错误还是该做的没做。 我想告诉大家，人们总说通过错误学习，我觉得最好是尽量从别人的错误里学习。不过，在伯克希尔，我们的处事原则是，过去的事就让它过去。 我有个合伙人，查理•芒格，我们一起合作40年了，我们从来没红过脸。我们对很多东西看法不一样，但是我们不争不吵。 我们从来不想已经过去的事。我们觉得未来有那么多值得期待的，何必对过去耿耿于怀。 不纠结过去的事，纠结也没用。人生只能向前看。 你们从错误里或许能学到东西，但最重要的是只投资自己能看懂的生意。如果你像很多人一样，跳出了自己的能力圈，听别人的消息买了自己毫不了解的股票，犯了这样的错，你需要反省，要记得只投资自己能看懂的。 你做投资决策的时候，就应该对着镜子，自言自语：“我要用每股55美元的价格买入100股通用汽车，理由是……” 自己要买什么，得对自己负责。一定要有个理由，说不出来理由，别买。 是因为别人在和你闲聊时告诉你这只股票能涨吗？这个理由不行。是因为成交量异动或者走势图发出了信号吗？ 这样的理由不行。你的理由，一定是你为什么要买这个生意。我们恪守这个原则，这是本•格雷厄姆教我的。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:4:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"五、对分散投资的看法 如果不是职业投资者，不追求通过管理资金实现超额收益率的目标，我觉得应该高度分散。 我认为98%到99%的投资者应该高度分散，但不能频繁交易，他们的投资应该和成本极低的指数型基金差不多。 只要持有美国的一部分就可以了，这样投资，是相信持有美国的一部分会得到很好的回报，我对这样的做法毫无异议。对于普通投资者来说，这么投资是正路。 如果想积极参与投资活动，研究公司并主动做投资决策，那就不一样了。 既然你走上研究公司这条路，既然你决定投入时间和精力把投资做好，我觉得分散投资是大错特错的。 那天我在SunTrust的时候，说到过这个问题。要是你真能看懂生意，你拥有的生意不应该超过六个。 要是你能找到六个好生意，就已经足够分散了，用不着再分散了，而且你能赚很多钱。 我敢保证，你不把钱投到你最看好的那个生意，而是再去做第七个生意，肯定会掉到沟里。靠第七个最好的主意发家的人很少，靠最好的生意发家的人很多。 所以，我说任何人，在资金量一般的情况下，要是对自己要投资的生意确实了解，六个就很多了，换了是我的话，我可能就选三个我最看好的。 我本人不搞分散。我认识的投资比较成功的人，都不搞分散，沃尔特•施洛斯是个例外，沃尔特的投资非常分散，他什么东西都买一点。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:5:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["生活"],"content":"六、如果能重新活一次，为了让生活更幸福，会怎么做？ 要是我重新活一次的话，我只想做一件事，选能活到120岁的基因。 我其实是非常幸运的。我经常举一个例子，觉得可能会对各位有启发，所以花两分钟时间讲讲。 假设现在是你出生前24小时，一个神仙出现了，他说：“孩子，我看你前途无量，我现在手里有个难题，我得设计你出生后生活的世界，我觉得太难了，你来设计吧。 你有24小时的时间，社会规则、经济规则、政府规则，这些都给你设计，你还有你的子孙后代都在这些规则的约束下生活。” 你问了：“我什么都能设计？” 神仙说：“对，什么都能设计。” 你说：“没什么附加条件？” 神仙说：“有一个附加条件。你不知道自己出生后是黑人还是白人，是富有还是贫穷，是男人还是女人，是身体健壮还是体弱多病，是聪明过人还是头脑迟钝。你知道的就一点，你要从一个装着58亿个球的桶里选一个球。” 我把这个叫娘胎彩票。你要从这58亿个球里选一个，这是你一生之中最重大的决定，它会决定你是出生在美国还是阿富汗，智商是130还是70。选出来之后，很多东西都注定了。你会设计一个怎样的世界？ 我觉得用这种思维方式可以很好地看待社会问题。 因为你不知道自己会选到哪个球，所以在设计世界的时候，你会希望这个世界能提供大量产品和服务，你希望所有人都能过上好日子。你会希望这个世界的产品越来越丰富，将来你的子孙后代能越过越好。 在希望世界能提供大量产品和服务的同时，还要考虑到有的人手气太差，拿到的球不好，天生不适合这个世界的体系，你希望他们不会被这个世界抛弃。 我天生非常适合我们现在的这个世界。我一生下来就具备了分配资金的天赋。这其实没什么了不起的。 如果我们都被困在荒岛上，永远回不来，我们所有人里，谁最会种地，谁最有本事。我再怎么说我多擅长分配资金，你们也不会理我。 我赶上了好时候。盖茨说，要是我生在几百万年前，早成了动物的盘中餐。他说：“你跑不快，也不会爬树，什么都不行，刚生下来就得被吃了。你生在今天是走运。” 既然我运气这么好，我就要把自己的天分发挥出来，一辈子都做自己喜欢的事，与自己喜欢的人交往，只和自己喜欢的人共事。 要是有个人让我倒胃口，但是和他走到一起，我能赚1亿美元，我会断然拒绝，要不和为了钱结婚有什么两样？ 无论什么时候，都不能为了钱结婚，要是已经很有钱了，更不能这样了，你们说是不是？ 我不为了钱结婚。我还是会一如既往地生活，只是不想再买美国航空了（2021年巴菲特股东大会又强调了这件事）！ 谢谢。 ","date":"2022-01-15","objectID":"/2022/01/life-forward/:6:0","tags":["鸡汤"],"title":"【转载】不纠结过去的事，人生只能向前看","uri":"/2022/01/life-forward/"},{"categories":["数据分析"],"content":"最近接触了曲线拟合（curve fitting），在此简单整理一波Python的实现方式 依稀记得高中数学课本有提到这个，$x$​​、$y$​​​​ 二维坐标。大致是两种方式：一种是看着像啥样或基于先验知识给出常见函数的关系式，通过数据拟合得到相应的系数；第二种是直接从数据出发，采用“基函数”拟合，和泰勒展开、级数有关系，函数越复杂拟合的越完美，但泛化能力就有待考究了，即复杂度越高越容易出现过拟合 「插播」这两种又让我想起了统计里的“参数估计”和“非参数估计” 备注：以下分类名称仅从个人理解出发 ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:0:0","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"自定义函数拟合 “自定义函数拟合”即我们可以自行编写定义各种函数（如幂函数、指数函数等）关系，基于此对现有数据进行拟合。往往需要一些领域内的知识🤔 具体实现可参考 scipy.optimize.curve_fit ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:1:0","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"官方示例 import matplotlib.pyplot as plt import numpy as np from scipy.optimize import curve_fit # 自定义函数 def func(x, a, b, c): return a * np.exp(-b * x) + c # 构造数据 xdata = np.linspace(0, 4, 50) y = func(xdata, 2.5, 1.3, 0.5) rng = np.random.default_rng() y_noise = 0.2 * rng.normal(size=xdata.size) ydata = y + y_noise # 拟合 popt, pcov = curve_fit(func, xdata, ydata) ## 设置参数取值范围 popt1, pcov1 = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5])) # 可视化 plt.plot(xdata, ydata, 'b-', label='data') plt.plot(xdata, func(xdata, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt)) plt.plot(xdata, func(xdata, *popt1), 'g--', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt1)) plt.legend() plt.show() curve_fit() 的参数方面： p0 系数初始值 bounds 各系数的取值范围 method 最优化算法，‘lm’, ‘trf’, ‘dogbox’ ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:1:1","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"MARK-log 此外还要 MARK 的一点是关于 $log$​ 的问题，Python中 numpy 和 math 都可以计算对数（$log$） 首先 math.log 和 numpy.log 都是以自然常数 $e$​​​ 为底的自然对数，针对底数不同各自都有以2、10为底的函数，分别为log2(), log10() 。其中，math.log2(x), math.log10(x)计算的准确性高于 math.log(x,2), math.log(x,10) 但是 math.log() 支持自选底数，比如计算 $log_{12}{10}$​​ ：math.log(10,12) 其实结合“换底公式”， 这个就等价于 math.log(10)/math.log(12) $$ log_ab=\\frac{log_ca}{log_cb} $$ 但但但但是，numpy.log 是支持数组、列表等形式的，享受 broadcasting带来的快感；而 math.log 只支持单个数字的计算，若传入数组等 array_like 则会报错：TypeError: only size-1 arrays can be converted to Python scalars 若一定要用 math.log 也是可以的： [math.log(i) for i in [1,2,3,4,5]] ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:1:2","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"基函数拟合 关于这部分，目前记录的 “基函数” 更多是多项式（polynomials）形式，numpy提供了 sub-package： numpy.polynomial （NumPy 1.4 版本，2009-12-28，引入的） Name Provides Polynomial Power series Chebyshev Chebyshev series Legendre Legendre series Laguerre Laguerre series Hermite Hermite series HermiteE HermiteE series Power series（Polynomial）就是我们常见的 $y=1+2x+3x^2$​ 这种 import numpy as np import matplotlib.pyplot as plt from numpy import polynomial as P x = np.array([10,20,30,40,50,60,70,80]) y = np.array([174,236,305,334,349,351,342,323]) # 3 表示想要拟合的最高次项是多少。 p = P.polynomial.Polynomial.fit(x,y,deg=3) # 表达式 # print(p) # 343.18750000000006 + 59.98042929292918·x¹ - 96.68750000000027·x² + 15.80744949494958·x³ yvals = p(x) #拟合y值 plt.plot(x, y, 's',label='original values') plt.plot(x, yvals, 'r',label='Power series') plt.legend() plt.show() ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:2:0","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"MARK 在官方文档会看到 Polynomial 这个类前面套了好几层numpy.polynomial.polynomial.Polynomial 实则关系是 numpy.polynomial 下面有个 polynomial.py 文件，文件里有个类是 Polynomial。同理，剩下五种多项式也是如此，chebyshev.py 有个 Chebyshev类 在实际使用的时候可以严格遵守这种关系 from numpy import polynomial as P ## 使用 Power series 拟合 p = P.polynomial.Polynomial.fit(x,y,deg=3) ## 使用 Chebyshev series 拟合 c = P.chebyshev.Chebyshev.fit(x,y,deg=2) ## 其他几种多项式类似 plt.plot(x, y, 's',label='original values') plt.plot(x, p(x), 'r',label='Power series') plt.plot(x, c(x), 'g--',label='Chebyshev series') plt.legend() plt.show() ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:2:1","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["数据分析"],"content":"Reference https://numpy.org/doc/stable/reference/generated/numpy.log.html https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html https://docs.python.org/3.7/library/math.html https://numpy.org/doc/stable/reference/routines.polynomials.html https://numpy.org/doc/stable/reference/routines.polynomials.package.html#module-numpy.polynomial https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.polyfit.html#numpy.polynomial.polynomial.polyfit https://numpy.org/doc/stable/reference/routines.polynomials.html https://www.jianshu.com/p/44baeed131df ","date":"2022-01-13","objectID":"/2022/01/da-curve-fitting-python/:3:0","tags":["曲线拟合","Python"],"title":"Python曲线拟合","uri":"/2022/01/da-curve-fitting-python/"},{"categories":["FEM"],"content":"经徐惟能教授许可，转载其知乎的回答 原文链接 恰逢2020年毕业季，学院邀请了徐教授作讲座，当时也咨询过徐教授关于量化的一些事儿，老师也推荐了相关金融、经济方面的书籍，在此也做个同步，防走丢 = = = = = = 分割线 = = = = = = 在金融学领域教学科研多年，在这里我系统化地只推荐高质量的优秀教材。金融学和其他学科一样，是需要沉下心来认真领悟其中逻辑的一门学科。市面上充斥着大量所谓可以赚快钱的金融学快餐文化，本人并不推荐。只知其一、不知其二，试图多快好省地实现财务自由对于很多人来讲是不切实际的——韭菜就是这么来的。 我按照金融学学习的总体架构分，推荐以下一些书籍： ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:0:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["FEM"],"content":"1. 经济学常识 第一大类：经济学常识。经济学是金融学的基础和基石。金融学的很多逻辑和原理和经济学是相通的。一般高校在专业课程安排上会在大一大二安排《微观经济学》和《宏观经济学》两门课程，道理也是如此。这里建议学习曼昆的经济学原理，分微观经济学和宏观经济学两部分。先学习微观经济学、再学习宏观经济学：这两本书也是我十多年前初入金融经济学大门的启蒙书，高度推荐。 ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:1:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["FEM"],"content":"2. 财务会计入门 第二大类：财务会计入门。财务会计的基础知识是理解公司财务所必备的先修内容。会计学为企业财务决策提供了数据支持和依据。一般高校会在大一大二安排《财务会计》课程。当然在这里，取决于读者的需求——对于大部分不太想花太多时间深究会计分录及做账的读者来说，可能了解企业的三张报表（资产负债、利润表、现金流量表）及其互相之间的联系可能就够了，对于这样的读者，请跳到第三部分。否则，如果想对财务会计有一定深入了解的读者，请参考以下这个问题中的答案。我的专业背景不是会计学，因此不想在这里随意推荐，误导各位。 ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:2:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["FEM"],"content":"3. 金融学原理及公司金融 第三大类：金融学原理及公司金融。有些答主可能会推荐先专门读一本有关于金融市场学方面的教材，再进入公司金融的学习。但我认为先后次序并无所谓。一般高校的金融学专业会在大二大三安排一门类似《金融学原理》的课程，这门课程的教材一般运用的就是公司金融的教材。在上一段，我建议想粗略了解三大报表的读者跳到这一段，因为我推荐的这本教材囊括了最基本的、公司金融所必要的财务会计知识，又涉及资本市场运作的一些扫盲，同时也涵盖了公司金融的全部内容： 这本教材是《公司金融》的经典之作，也是我从教8年来一直在课上使用的教材。前几章铺垫了金融学的基本框架、货币时间价值、基本的财务分析基础，然后循序渐进地引出公司金融这个金融学大分支的各个方面。如果读者能够认真读完，必将对企业的价值创造、财务管理、财务分析有一个系统化的全新认识，又对金融市场有一个初步的认知。 我在知乎上开过一个有关于一个公司金融入门的 Live，差不多相当于我在学校授课《公司金融》的第一堂课，希望给刚刚入门金融学，准备学习《公司金融》的同学一些指导和启发： ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:3:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["FEM"],"content":"4. 投资学 第四大类：投资学。读完公司金融以后，读者可能会有跃跃欲试的冲动，那么这个时侯学习《投资学》将是水到渠成的事情。投资学是资产定价的一部分，也是金融学领域另一个重大分支，它涉及各种金融资产的特点性质、投资组合的要领、金融资产的交易等等内容。在这里我推荐博迪的《投资学》： 好，如果读者能够坚持读到这里，读完上述几本书籍，其实已经对于金融学有了一个很完整的概览了。 ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:4:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["FEM"],"content":"5. 进阶 如果读者还有兴趣，或者想更进阶一些，我再推荐几本延伸的方向： 想更多了解金融衍生品的：以下这本教材是个经典著作（原版+中文译版） 想更多了解行为金融学的：诺奖获得者、耶鲁大学教授席勒的著作： 想往数理金融方向拓展、入门计量经济学的： 能把这些教材读懂吃透，对于金融学来讲已经不仅仅是入门了。 = = = = = = 分割线 = = = = = = 毕业后也有看老师推荐的经济学、财务报表方面的书籍、资料，但遗憾的是一直没坚持，无论是曼昆的经济学原理还是财务报表分析皆中断 希望2022年能都好好补上～ ","date":"2022-01-08","objectID":"/2022/01/fem-resources/:5:0","tags":["金融","经济学"],"title":"系统学习金融经济方面知识的书籍推荐","uri":"/2022/01/fem-resources/"},{"categories":["生活"],"content":"限制 pday 分区时，开头要写 2022 了 ","date":"2022-01-01","objectID":"/2022/01/life-new-year-2022/:0:0","tags":["碎碎念","总结与展望"],"title":"回顾2021，展望2022","uri":"/2022/01/life-new-year-2022/"},{"categories":["生活"],"content":"向后看 2021 2021年是毕业后完整的第一个自然年。这一年在很多事我都没处理好，也很糟糕。 若展开细说，故事真的太长、太长，或许也是我不愿再回忆及梳理。但理性的时候想想还是得到之前的结论：这些都是人生中的素材。 正如杨绛先生所说， 如果事与愿违 请相信一定另有安排。 工作、情感等方面遇到磕磕绊绊很多也是自己造成的，万事皆有因，万般皆是果。 而我们要做的便是从素材中总结出经验教训，避免再犯 叨叨了这么多，也不符合向后看的主题 hhhhh （非记叙文） 就让我们放下往事，管它过去有多美。 ","date":"2022-01-01","objectID":"/2022/01/life-new-year-2022/:1:0","tags":["碎碎念","总结与展望"],"title":"回顾2021，展望2022","uri":"/2022/01/life-new-year-2022/"},{"categories":["生活"],"content":"向前看 2022 既然放下，则应向前看。 所有的失去，都将以另一种方式归来。 因果循环，都是命数～ 济姐与我说， 世界不是围着你转的，不是所有的事情都要跟着你的进程来的 是的，我记得有人总结过类似人生阶段的东西，其中一个便是 以自我为中心，总认为世界是围着自己转的。 说白了就还是心智还不够成熟，总要经历些什么让自己成长，而所付出的代价便是因人而异了。 若想降低相应的代价，我认为“向前看”便是 当你有犹豫不决、拿不准的想法时，不要急于实施，多和朋友们（尤其是过来人）推心置腹的好好聊聊，多听听不同的声音。以此降低犯错成本。 过去，我总会寄希望于“新年新气象”，即真的单纯的认为什么都不用做，事情放那儿就能解决。其实只是忘却了罢了；其实只是要去做另一件事罢了，可能还是影响下一件事。然而之前困扰我的事情始终没有得到很好的解决。历史总会重演，再见面时还是会不知所措。 所以，向前看，不仅光看着外部环境中 2021 的 1 变成了 2；更多还是要自己主动的去解决问题，正所谓幸福掌握在自己手中～ 等待并期待新气象的同时，也要解决问题～ ","date":"2022-01-01","objectID":"/2022/01/life-new-year-2022/:2:0","tags":["碎碎念","总结与展望"],"title":"回顾2021，展望2022","uri":"/2022/01/life-new-year-2022/"},{"categories":["数据分析"],"content":"当下处理大规模数据集比较流行的两大产品：Hive和Spark。本文从历史等维度对两者进行比较 🔗原文链接 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:0:0","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Hive Hive 是一个运行在 Hadoop 分布式文件系统上的开源分布式数据仓库。Hive为查询和分析大规模（以表格形式存储的）结构化数据集而生。人们可以通过Hive内置的SQL引擎——HiveQL操作数据。将HiveSQL解析为Map-Reduce的操作。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:1:0","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"简短的历史 Hive最初是由 Facebook 开发的，当时Facebook的开发人员发现他们的数据在几天内从 GB级别 呈指数增长到 TB级别。当时，Facebook 使用 Python 将他们的数据加载到关系型数据库。性能和可扩展性很快成为棘手的问题，因为关系型数据库只能纵向扩展。他们需要一个可以横向扩展并处理大量数据的数据库。那时Hadoop已经很流行了；不久之后，构建在 Hadoop 之上的 Hive 便出现了。Hive和关系型数据库很类似，但又不完全是。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:1:1","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"为什么选择Hive 选择 Hive 的核心原因是因为它是一个运行在 Hadoop 上的 SQL 接口，降低了MapReduce框架的复杂性。Hive 帮助企业在 HDFS 上执行大规模数据分析，使其成为一个可横向扩展的数据库。它的 SQL 接口——HiveQL，使具有关系型数据库背景的开发人员可以更轻松地构建和开发性能更快、可扩展的数据仓库。（简单来说是上手快，方便原有DBA完成数据建模） ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:1:2","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Hive的特性和功能 Hive 具有企业级特性和功能，可帮助组织构建高效、高端的数据仓库解决方案。具备以下特性： Hive 使用 Hadoop 作为其存储引擎，并且仅在 HDFS 上运行 它专为数据仓库操作而构建，但不适用于 OLTP HiveQL 是 SQL 引擎，可为数据仓库层级的操作构建复杂的 SQL 查询。Hive 可以与其他分布式数据库（如 HBase）和 NoSQL 数据库（如 Cassandra）集成。 Hive的架构 有一个 Hive 接口，并使用 HDFS 跨多个服务器存储数据以进行分布式数据处理。 Hive在数仓中的应用 Hive 是专门为操作数据仓库而构建的数据库，尤其是那些处理TB、PB级数据的数据库。正如上文所说，Hive支持横向扩展，并结合 Hadoop 的功能，使其成为一个快速、高效及高维扩展的数据库。它可以在数千个节点上运行，充分体现硬件的性能。这使得 Hive 成为具有高性能和可扩展性的高性价比产品。 Hive 的集成功能 因为 HiveQL 符合 ANSI SQL标准，所以 HBase 和 Cassandra 等数据库也可以集成Hive。这些数据库对 SQL 的支持有限，但可以帮助应用程序对更大的数据集进行分析。Spark, Kafka和Flume等工具也可以集成Hive。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:1:3","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Hive的不足 Hive 是一个以表格形式存储数据的数据仓库，所以他只能处理使用 SQL 查询读取和写入的结构化数据。并不适用于存储非结构化数据，也不适用于OLTP。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:1:4","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Spark Spark 是一个分布式大数据框架，可处理并分析 RDD 格式的大规模数据集。简而言之，Spark 不是数据库，而是一个框架，可以将Hive和HBase等数据库中的数据加载至Spark的RDD，以享受分布式带来的快感。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:2:0","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Spark Streaming Spark Streaming是Spark 的一个扩展，它可以从 Web 源流式传输实时数据以进行各种分析。尽管还有其他工具（例如 Kafka 和 Flume）可以执行此操作，但 Spark 在复杂的数据分析方面更胜一筹。Spark 有自己特有的 SQL 引擎，并且在与 Kafka 和 Flume 集成时也能很好的兼容。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:2:1","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"简短的历史 Spark 是 MapReduce 的替代方案。Spark通过“in-memory”的形式完成大数据分析，不依赖磁盘空间及网络带宽。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:2:2","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"为什么选择Spark Spark 的核心优势在于它能够执行复杂的“in-memory”分析和 PB 级的流数据，效能优于MapReduce。Spark 可以从运行在 Hadoop 上的任何数据存储中提取数据，并在内存中并行执行复杂的分析。此功能减少了磁盘 I/O 和网络争用，使其速度提高了十倍甚至一百倍。此外，Spark 中的数据分析框架可以使用 Java、Scala、Python、R 甚至 SQL 构建。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:2:3","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Hive的特性和功能 Spark的架构 四大主要部分： Spark SQL Spark Streaming MLlib (machine learning) GraphX (graph 高效分析 Spark 从 Hadoop 中提取数据并在内存中执行分析。数据以并行方式分块进入内存，然后将生成的数据集推送到目的地。数据集也可以驻留在内存中，直到它们被消耗。 Spark Streaming Spark Streaming 是 Spark 的扩展，它可以实时流式传输来自高频使用的 Web 源的大量数据。由于其执行高级分析的能力，与其他数据流工具（如 Kafka 和 Flume）相比，Spark 便脱颖而出。 多个API Spark 支持不同的编程语言，如 Java、Python 和 Scala，这些语言在大数据和数据分析领域非常流行。人们可以使用这些语言中的任何一种编写数据分析框架。 处理大规模数据集 Spark也是为了分析、处理大数据的，就像上文提到的MapReduce。也是为了更加高效的大数据分析工作 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:2:4","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"Hive v.s. Spark Hive 和 Spark 是在大数据领域为不同目的而构建的不同产品。 Hive 是分布式数据库，Spark 是数据分析框架。 Hive Spark 类似关系型数据库，存储表格形式的数据 数据分析框架，不是数据库 通过HiveQL 取数 支持 SQL、Python、Java、R和scala 基于Hadoop 本身不存储数据，可以将数据加载至Spark RDDs 简单来说，Hive是数据仓库，而Spark是个让取数更快的框架。 ","date":"2021-12-27","objectID":"/2021/12/da-hive-vs-spark/:3:0","tags":["Hive","Spark"],"title":"Hive和Spark的区别","uri":"/2021/12/da-hive-vs-spark/"},{"categories":["数据分析"],"content":"最近用 pandas.qcut 分箱遇到了个很奇怪的bug，先Mark一波 在此之前也顺便记录下 pandas.qcut, pandas.Series.value_counts 和 pandas.Series.value_counts 的用法 ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:0:0","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"相关函数 ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:1:0","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"pandas.qcut 基于样本数据排序或分位数分箱 pandas.qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise') Parameter type comment x 一维 ndarray 或 Series q int or list-like of float 如4，或 [0,0.2,0.8,1] labels array or False, default None 对分箱后的区间重命名，要求和bins等数量 retbins bool, optional 是否返回边界值 precision int, optional 用于调整精度，一般默认 duplicates ‘raise’ or ‘drop’ ‘raise’: 若边界值重复则报错 ‘drop’: 删除重复的 import pandas as pd pd.qcut(range(5), 4) #[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]] #Categories (4, interval[float64, right]): [(-0.001, 1.0] \u003c (1.0, 2.0] ... pd.qcut(range(5), 3, labels=[\"good\", \"medium\", \"bad\"]) #[good, good, medium, bad, bad] #Categories (3, object): [good \u003c medium \u003c bad] ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:1:1","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"pandas.Series.value_counts Return a Series containing counts of unique values. 返回的结果默认是根据频数降序排序，且排除 NaN Series.value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True) Parameter type comment normalize bool, default False If Ture 则返回频率 sort bool, default True 按照频数排序 ascending bool, default False If True，从小到大排序 bins int, optional 仅适用于数值型数据，分箱计数 dropna bool, default True If False， 包含 NaN \u003e\u003e\u003e pd.Series(range(1,101)).value_counts(bins=4) (75.25, 100.0] 25 (50.5, 75.25] 25 (25.75, 50.5] 25 (0.9, 25.75] 25 dtype: int64 ############################################### \u003e\u003e\u003e pd.Series(range(100)).value_counts(normalize=True,bins=4) (74.25, 99.0] 0.25 (49.5, 74.25] 0.25 (24.75, 49.5] 0.25 (-0.1, 24.75] 0.25 dtype: float64 ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:1:2","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"pandas.Series.unique Return unique values of Series object. Series.unique() \u003e\u003e\u003e pd.Series([3, 1, 2, 3, 4, np.nan]).unique() array([ 3., 1., 2., 4., nan]) ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:1:3","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"那个奇怪的bug Python: 3.6.4|Anaconda, Inc. pandas: 1.1.5 背景是对数据进行等频分箱 pdf['qcut'] = pd.qcut(pdf['prob'],20,duplicates='drop') 之后通过value_counts计数发现有个区间频数为0，且为17个区间 此时，再通过 unique 去重，就只有16个区间，上图中的 (22.0,22.2] 那个区间就没了 根本原因还是数据分布的问题，因为本来是要分20个bin，存在较多边界值重复的情况 ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:2:0","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"解决方案 源码方面的原因暂时不得而知 分箱后得到的数据类型是 Categorical将其转换为 String后再进行后续的分组统计计算 pdf['qcut'] = pdf['qcut'].astype(str) ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:3:0","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["数据分析"],"content":"Reference https://pandas.pydata.org/docs/reference/api/pandas.qcut.html https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html ","date":"2021-12-16","objectID":"/2021/12/da-pandas-qcut/:4:0","tags":["Python","pandas"],"title":"pandas.qcut及一个很奇怪的bug","uri":"/2021/12/da-pandas-qcut/"},{"categories":["生活"],"content":"最近在撸 X战警系列的电影，从故事的时间线维度出发，整理现有影片的观影顺序 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:0:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: First Class(2011) 万磁王（Erik）小时候在奥斯维辛集中营因愤怒等情绪展现出了控制铁制品的超能力，被纳粹Klaus Schmidt盯上了，一顿蹂躏，甚至还当着Erik面枪杀他的母亲 时间到了1962年，也是故事的大背景——古巴导弹危机。之前的纳粹Klaus Schmidt，因为具备吸收能量的超能力，老的没那么快，之后叫 Shaw（Sebastian Shaw） Erik也各种展开复仇。 同时这部X战警也介绍了后续变种人中两大理念的领导者，Professor X（Charles） 和 Erik（“万磁王”）早期的爱恨情仇。以及“魔形女” Raven，Hank 四人之间的你侬我侬。 期间也闪现了Professor X（Charles） 和 Erik（“万磁王”）两人拉 James Howlett（即之后的“金刚狼”）入伙的镜头 Mark 这时候（1962年）越南战争还没结束。 第一战 即是变种人和普通人类之间的第一战，也是Professor X（Charles） 和 Erik（“万磁王”）之间的第一战 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:1:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men Origins: Wolverine (2009) / 金刚狼1 金刚狼（James Howlett）小时候一个不小心干掉了生父（Thomas Logan），之后和他的大兄弟（Victor Creed）狂奔，共同经历了美国内战，第一、二次世界大战，越南战争。 时间点 越南战争结束已是 1975 年，这期间已经过了121年，变种人的故事多多少少也会夹杂着同步发生 之后他俩加入了 Stryker 的变种人团队 X 时间来到了 6 年后，那么至少是1981年之后了 感觉后面的矛盾基本都由 Stryker 挑起，金刚狼那钢铁般的爪子也是他给整出来的 这部最后还有些体现时间点的东西： Professor X（Charles） 已经坐上轮椅且头发没了 眼睛喷火的大哥（Scott）还小 Deadpool 没完全GG ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:2:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: Apocalypse | 天启（2016） Scott 的课堂上，老师讲授着，变种人大规模出现在公众视野是在1973年的巴黎和平协约上 balabala 这集主要是讲如何干掉 传说中第一个变种人：En Sabah Nur 在此期间出现的人物，包括但不限于 风暴女（Ororo Moonroe，还没好好跟着 X-Professor） Caliban，收集情报 夜行者 Kurt Wagner Jean Grey 这里最牛逼的当属 Jean（琴），最后干掉那个伪神（En Sabah Nur）时并没有太多笔墨，感觉完全就是两个世界的人，直接降 n 维打击，了结了所谓的Boss。可谓是 Jean 才是大大大Boss啊。 这里有两个点🉑️ 先 「Mark」 Jean 第一次与 金刚狼相遇，并帮他找到了名字 Logan。名字嘛，和“我是谁” 这种哲学性十足的问题简直不是个数量级的 X-Professor 的头发没了。挺难的：“第一站”之后难以直立行走，现在又头秃了，不是以前随意撩妹的帅教授了 最后关于时间点上来看还是有些瑕疵的，比如在“X-Men Origins: Wolverine” 的时候，Scott还是个孩子，那时至少是1981年之后，但这里已经是1983年了 不过本文并不是聚焦于此～ ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:3:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men (2000) 电影中似乎未提及具体时间，故事发生在1944年之后，not-too-distant future，但看着人物状态估计离X-Professor头秃的时候也有20年了，即2000年左右 Professor-X 和 万磁王（Erik）都老了，且之前X教授收的几个小弟（琴、暴风女、Scott）都具备一定的单兵作战能力 关于时间线方面，此时金刚狼还没找到当年实验基地的（An abandoned military installation in Canada）的记忆，即 X-Men Origins: Wolverine (2009) 讲述的故事。 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:4:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men 2 (2003) 承接 X-Men(2000) 之后的故事，似乎也没有啥时间点 这里出现了 Stryker，那个给予金刚狼xxx合金爪子的Stryker Stryker想干掉变种人，而 万磁王（Erik） 想干掉人类 😂 不过最后也是互相妥协了吧，也说明白了是有人想挑起战争，搞得两败俱伤。应当求同存异、互利共赢、共创美好未来～ 关于 Stryker 儿子——Jason 的事儿一直也没整明白，和 Professor X 之间的交集也不得而知，不知道是不是哪里漏了 🤷‍♂️ ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:5:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: The Last Stand(2006) 承接 X-Men2 之后的故事，依然没有啥时间点 强大且产生威胁时就要被干掉o.o 人们总是对自己不了解的东西小心翼翼 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:6:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"The Wolverine(2013) 1945年8月9日，美国投掷原子弹轰炸日本长崎。金刚狼不仅扛过来了，还救了军官 Ichirō Yashida（未执行尊贵的切腹） 后面的故事就是这个幸存的军官 Ichirō Yashida 想永生而引发的各种故事。 剧中提到 Do not let the lights fool you. These are dark days in Japan. 不知道说的是不是20世纪90年代经济泡沫之后的日本 P.S. 片尾有彩蛋：看样子似乎 Professor-X 和 万磁王（Erik）统一战线了，“一致对外” ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:7:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: Days of Future Past ｜ 逆转未来（2014） RT，逆转未来呐么，回到过去改写历史，玩起了穿越，回到了1973年 剧中时间点是2023年，（本文写作时间点是2021年12月25日），改写历史的任务交给了拥有自愈能力的金刚狼 片尾也出现了琴、Scott～回到了最初的美好 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:8:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"Reference https://rddiy.com/chuangyisheji/shijue/ygqje.html ↩︎ ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:9:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["数据分析"],"content":"Hi PySpark，初次见面，别来无恙 PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. 首先，我是这么来看PySpark的：有一波人会Python但不会Java，那就搞个接口让会Python的小伙伴享受Spark分布式环境带来的快感，更好的分析大数据。 那么对于“面向问题编程”的从业人员来说PySpark的作用就很明显了。当觉得现有的分析工具很慢时可以考虑下PySpark，当然这里是基于Spark环境。换句话说，“快”是分布式环境带来的快感之一。 引入 PySpark 后，分析工作大致流程就变成了这样 👇 其实，整体还是“箱子模型” 📦 ，“喂”数据 =\u003e 处理、计算模块 =\u003e 结果 所以，应用层角度来看 PySpark 也就简单了： 如何读取数据？ 如何处理、计算得到自己想要的结果，即“面向问题编程” 如何处理结果？要保存到哪儿？ ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:0:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"如何读取数据 这个往往取决于数据在哪儿，譬如有些数据是以csv格式保存，有些是在数据库… 总之都是为了 Loading data onto Spark RDDs，享受分布式的快感 实际操作可以基于具体情况在网上检索相应的解决方案，如 pyspark read hive table # A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html from os.path import abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath('spark-warehouse') spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL Hive integration example\") \\ .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() # spark is an existing SparkSession spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\") spark.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\") # The results of SQL queries are themselves DataFrames and support all normal functions. sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key \u003c 10 ORDER BY key\") # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # \"\"\" An interactive shell. This file is designed to be launched as a PYTHONSTARTUP script. \"\"\" import atexit import os import platform import warnings import py4j from pyspark import SparkConf from pyspark.context import SparkContext from pyspark.sql import SparkSession, SQLContext if os.environ.get(\"SPARK_EXECUTOR_URI\"): SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"]) SparkContext._ensure_initialized() try: # Try to access HiveConf, it will raise exception if Hive is not added conf = SparkConf() if conf.get('spark.sql.catalogImplementation', 'hive').lower() == 'hive': SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf() spark = SparkSession.builder\\ .enableHiveSupport()\\ .getOrCreate() else: spark = SparkSession.builder.getOrCreate() except py4j.protocol.Py4JError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() except TypeError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() sc = spark.sparkContext sql = spark.sql atexit.register(lambda: sc.stop()) # for compatibility sqlContext = spark._wrapped sqlCtx = sqlContext print(\"\"\"Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version %s/_/ \"\"\" % sc.version) print(\"Using Python version %s(%s, %s)\" % ( platform.python_version(), platform.python_build()[0], platform.python_build()[1])) print(\"SparkSession available as 'spark'.\") # The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP, # which allows us to execute the user's PYTHONSTARTUP file: _pythonstartup = os.environ.get('OLD_PYTHONSTARTUP') if _pythonstartup and os.path.isfile(_pythonstartup): with open(_pythonstartup) as f: code = compile(f.read(), _pythonstartup, 'exec') exec(code) ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:1:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"数据处理、计算 读取数据得到 Spark DataFrame 后，可以直接对此进行操作，除了常见的业务分析还有机器学习模块（MLlib） raw_data = sc.textFile(\"./kddcup.data.gz\") ## Comma-Separated Value csv = raw_data.map(lambda x: x.split(\",\")) metrics = csv.map(lambda x: [x[0], x[4], x[5]]) from pyspark.mllib.stat import Statistics Statistics.corr(metrics, method=\"spearman\") Statistics.corr(metrics, method=\"pearson\") 值得一提的是，Spark DataFrame to pandas DataFrame 可以用 toPandas() 方法，同时参数方面设置 spark.sql.execution.arrow.enabled=true 能提高效率 # A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas import numpy as np import pandas as pd # Enable Arrow-based columnar data transfers spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Generate a pandas DataFrame pdf = pd.DataFrame(np.random.rand(100, 3)) # Create a Spark DataFrame from a pandas DataFrame using Arrow df = spark.createDataFrame(pdf) # Convert the Spark DataFrame back to a pandas DataFrame using Arrow result_pdf = df.select(\"*\").toPandas() 这部分再Mark一个关于 collect() 的小点，总之数据量比较大的时候就不要用这个方法。 The collect() function returns a list that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as all the data is loaded in a driver’s memory, in which case we lose the benefits of distributing the data around a cluster of Spark instances. ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:2:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"如何处理结果 处理、计算后的结果往往会再一次的落库，这个时候同 “数据读取” 的部分，🉑️ 根据具体情况进行检索。 以落到 hive 表为例，截止到目前整理的，大致有两种方法。 首先确保数据为 Spark DataFrame 状态（可以通过 spark.createDataFrame(df) 的方法将 pandas DataFrame 转为 Spark DataFrame） spark_df.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"dbName.tableName\") # 注意是 overwrite 或者 spark_df.createOrReplaceTempView(\"myTempTableName\") spark.sql(\"drop table if exists dbName.tableName\") spark.sql(\"create table dbName.tableName as select * from myTempTableName\") ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:3:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"总结 最近工作中遇到了 PySpark 的使用，在此从应用层小白视角通过 📦 “箱子模型”（Input =\u003e Box =\u003e Output） 简单记录大致的使用流程，方便于新手～ ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:4:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"Reference 360数科深圳数据组 Rudy Lai and Bartłomiej Potaczek.《Hands On Big Data Analytics With PySpark》 https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas http://spark.apache.org/docs/latest/api/python/index.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:5:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"汇总一些【因果推断】方面的学习资料 最近看了下《原因与结果的经济学》并且结合硕士期间关于相关性与因果的思考，感觉这个 因果推断（causal inference）似乎挺有意思的。 基于数据分析的特性，我把目前因果推断方面的研究分为四大类 -- 聚焦某领域 未聚焦 理论 基于某领域的理论研究（如心理学因果关系的研究方法） 纯理论研究（这种常见于“扛把子”引领一个方向） 应用 基于某领域的应用研究（如业界的一些策略评估） 类似咨询 常见（或高产）的应该是“基于某领域的理论研究” 和 “基于某领域的应用研究”，至于第四种（“未聚焦领域的应用型研究”）因为商业性的问题应该较难找到公开案例。 后续也打算站在小白/门外汉的角度深入了解下，记录下遇到的学习资料。（持续更新…） ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:0:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"网文 ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"博客 Emre Kiciman @Microsoft Amit Sharma @Microsoft 二位似乎一起搞了个网站：Getting Started with Causal Inference，里面有正在写的书：Causal Reasoning: Fundamentals and Machine Learning Applications 以及一些课程之类的 Yishi Li @Tencent 统计之都 Brady Neal，有篇从需求出发选择读物的文章 ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:1","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"知乎 因果推断会是下一个AI热潮吗？ ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:2","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"Github amit-sharma / causal-inference-tutorial Paper_CausalInference_abtest ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:3","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"书籍 《原因与结果的经济学》 《别拿相关当因果！因果关系简易入门》（Why: A Guide to Finding and Using Causes） JUDEA PEARL. 《为什么》（THE BOOK OF WHY: THE NEW SCIENCE OF CAUSE AND EFFECT） JUDEA PEARL. Causality: Models, Reasoning, and Inference JUDEA PEARL. Causal Inference in Statistics: A Primer Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction Brady Neal. Introduction to Causal Inference Hernán MA, Robins JM. Causal Inference: What If，以及书中对应的Python-code ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:2:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"工具包 Uber-Causal ML: A Python package that provides a suite of uplift modeling and causal inference methods using machine learning. Microsoft-EconML: A Python package for estimating heterogeneous treatment effects from observational data via machine learning. Microsoft-DoWhy: A Python library that aims to spark causal thinking and analysis. CausalDiscoveryToolbox: A package for causal inference in graphs and in the pairwise settings ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:3:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["FEM","读书笔记"],"content":"为什么要讲究因果？因为只有因果才能决定未来 可能在相关性盛行的大数据时代，因果关系也成为了一种稀缺 但科学研究一直提倡的是因果🤔 可以从数据中的规律出发，找寻因果关系（相关性 $\\Rightarrow$ 因果） 与此同时，经济学中还有个流派——从公理出发，经过一顿操作（逻辑推演），得到相应的结论。 或许这是“研究范式”的不同吧。按照邓小平爷爷的“猫论”，研究成果能造福人类就好。 本文是《原因与结果的经济学》的读书笔记，全书主要分为两大部分：1是提出因果推理并强调“反事实”是其必经之路；2是例举一些构建“反事实”的方式方法 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:0:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"因果推理 两个变量的关系是否真的是因果关系？解答这个问题所需的思维方法便是“因果推理”。 而判断因果关系有三个要点： 是否“纯属巧合”； 是否存在“第三变量” 是否存在“逆向因果关系” 备注 两个变量之间为因果关系时才能画如上图中的实线指向箭头，原因指向结果 而推翻以上三点的方法便是 对现实和“反事实”进行对比 反事实是指对过去未曾发生的事实所做的假设，例如“如果当时没有……，那么……”。我们将现实中实际发生的事称为“事实”，所以将设想的与现实完全相反的情况称为“反事实”。 这方面比较好操作的便是自然科学领域的各种实验了，比如这根试管加 xxx，另一根试管不加 xxx，观察两者之间的差异，验证假设之类的 但到了人文社科领域，要想像自然科学领域做实验，可谓是 “噫！吁嚱…” 所以作为因果科普文的《原因与结果的经济学》所介绍的方法也是有很多被challenge的地方。 后面的章节便是按照“证据等级”排序介绍了相应构建“反事实”的方法，与事实进行对比，进而判断因果关系 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:1:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"如何构建“反事实” 说到这个不禁想起硕士导师和我说过的话，只有实验才能得因果关系 至于构造“反事实”的方式方法，书中也简单罗列了几点 但不得不说，人文社科领域要想完全的像自然科学的实验那样严谨是比较难的，毕竟有时候个体之间也很难保持独立，即互相之间是会有影响的。 所以呢，CB（consumer behavior）以及心理学相关领域在评判学术文章时，除了idea 之外，还会看实验设计的是否巧妙。 此时针对此次实验研究（问卷形式）收集的数据便是“一手数据”，而非此次实验研究目的收集的那种便是“二手数据”。比如研究涉及电商评论，现有电商平台的评论数据对于我们的研究而言便是二手数据。 后续采取相应的分析方法常见有两种：一种是基于线性回归的 Conditional Process Analysis；另一种就是结构方程。可根据数据形式灵活采取相应的方法。 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:2:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"业界的增益 20世纪末美国陆陆续续将统计模型引入信贷领域风控业务1，贯彻“数据驱动”的理念，一直沿袭至今。从早些年基于 logistics回归的评分卡到现在的“GBDT”树类模型，但均未涉及因果。 在目前中国政府进一步压缩借贷利率上限的背景下，对风控业务而言便需要更加精细化的管理，在基于相关性搜寻风控策略的基础上进一步探究产生信用风险的因果关系，在总用户被压缩的情况下，在风险可承受范围内，进一步提高“进水口处”的“进水量”（/批核率/通过率）。 所以，在工具方面，是不是可以尝试或探究下“因果推断” 🤔 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:3:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"Reference http://www.sinotf.com/GB/consumerfinance/2018-01-16/0MMDAwMDI5OTc0Mw.html ↩︎ ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:4:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM"],"content":"一些观念被人们相信，是因为它们本身就是可被明证的事实；而另一些观念被人们相信，只是因为人们被这些观念反复的“洗脑”了而已。 最终，洗脑代替了证据，让人们普遍接受了这些“事实”。 当许多广为认知的观念接受事实与逻辑的检验，你会发现有的观念像纸牌屋一样不堪一击；有的观念看似真理，却只是一些思维谬误的产物。 经济政策中的谬误层出不穷，而且影响着社会的方方面面。小到住房，大到国际贸易，都是如此。 这些政策造成的灾难性后果往往要在好几年后才显现，但很少有人会对这些灾难的起因追根朔源。 即便一个政策出台后马上引发不良反应，很多人也不会去追究政策本身的缺陷，政策的倡导者还常常把这些不良后果嫁祸出去。他们甚至会辩解说如果没有他们推行的这套好政策，情况会更糟。（批注：因果反事实真的很难） 即使事实摆在眼前，谬误还是大行其道，其中的缘由各不相同。比如，政客为了避免影响自己的政治生涯，学者为了避免声望受损，扶贫活动的公益领袖为了避免内心的痛苦失落…… 没有人乐于承认自己的错误。 但在很多情况下，掩盖错误的代价是高昂的。这些代价让人们向现实低头，不管他们多么不愿意或多么痛苦。如果一个学生这次数学考试错了一道题，那下次考试之前他就必须把这道题解对；一家企业不会因为贯彻错误方针而任由企业不断亏损却不纠正。 简而言之，无论是出于实际需求还是理智，我们都有必要对谬误追根究底。 政府出台好的和错的经济政策都会影响无数人的生活，都会直接导致人们生活的更好或更差。这就是经济学研究为什么那么重要，而对谬误的研究不仅是单纯的学术行为。 经济谬误实在太多，无法一一列出。但我们可以概括出五大类常见的经济谬误进行分析： ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:0:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"1.零和式谬误 许多经济谬误都建立在一个错误的假设上，即交易是零和博弈，一方所得意味着另一方所失。 但是，如果不能让双方获利，交易就不会发生。这样浅显的道理，对于许多政策倡导者来说，不一定真的明白。 比如说，房租管制、最低工资，都是试图通过外部力量增加交易一方获得的收益。但是，当另一方无利可图或利益微薄时，交易就不再发生。 地产商不再建房，房东不再精心修缮房屋，导致更多的人租不到房子；老板不再雇佣新人，或者干脆将工厂搬到国外，导致本国人更多失业。 当你把交易误解成零和博弈，试图通过暴力强制交易一方减少收益，结果只会是交易量的减少。 零和博弈这种思维谬误造成的最坏的结果是：贫穷国家以为外来资本在剥削他们，为了避免被剥削，他们排斥和抵制外来资本。这种观念曾经被全世界广泛接受，造成的结果就是数千万人数代间都深陷贫困的泥沼。 许多国家后来纷纷抛弃了这种谬误，积极的拥抱资本和开展国际贸易。但在抛弃这种谬误之前的岁月里，无数人只能做无谓的牺牲，为一个没有事实根据的假设付出了巨大的代价。 可见，思维谬误造成的影响是极大的。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:1:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"2.组合式谬误 逻辑学家所说的“组合式”谬误是指部分正确即整体正确的观念。 一个棒球球迷在观众席上站起来看球场上的比赛可以看得清清楚楚，但如果所有球迷都站起来看，那最后的结果就是大家都看不好。许多经济政策都涉及组合式谬误。比如政治家们为某些特定群体、行业、利益集团代言，他们采取的政策看似造富全社会，其实不过是拆东墙补西墙的把戏。 例如，很多地方政府为了消灭贫民窟，“振兴社区”，大搞拆毁和重建，把贫民窟变成高档住宅和购物中心。人们总认为这些政府支出创造了就业、繁荣了经济，其实，不过是把纳税人的钱转移到了政府那里，导致纳税人无法用这些钱来消费和投资。政府宣称他们用繁荣社区取代了贫民窟，其实，那些贫民只不过是迁徙到了其他地方，去了其他的贫民窟。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:2:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"3.事后归因式谬误 事后归因式谬误不但最普遍，历史也最悠久。即对于两件相继发生的重大事件，人们会把第一件事当成第二件事的起因。 例如 1929 年股市崩盘之后，紧接着爆发了大萧条。所以一直以来人们都认为是股票市场的崩溃导致了整个经济的崩溃。然而，1987 年的股市崩盘，却迎来了持续 20 年的经济增长。 再例如世界上许多地方都开展过禁用 DDT 的运动。DDT 可以消灭传播疟疾的蚊虫，杀虫效果显著。但是使用 DDT 越多的地方，癌症发病率越高。以至于人们控诉 DDT 引发癌症，但这并非事实。禁止 DDT 让疟疾死灰复燃，夺走了全球上百万人的生命。 事后归因式谬误不仅仅是一个智力问题。人们总想着为好事邀功，把坏事归罪于人，这就是为什么会有那么多事后归因式谬误了。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:3:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"4.棋式谬误 在亚当·斯密的笔下，“教条理论家”是指自视甚高，认为“可以像摆弄棋盘上的棋子那样轻松驾驭一个国家”的一类人。这样的理论家仍然普遍存在，而且他们还影响着法律和政策的制定。 人类与棋子不同。他们有自己的喜好、价值观、计划和意愿。所有这些都可能会与“社会实验”的宏伟目标发生冲突。因为人不是棋子，不是任由摆布。 任何试图让人去机械的扮演某个宏伟计划的一部分的尝试都注定会失败，就像历史已经反复证明的那样。 很多教条理论家们并不死心，他们那种“如果一开始没有成功，那就多试几次”的想法是酿成更多灾祸的一味配方。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:4:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"5.开放式谬误 许多理想都忽略了一个最基本的经济事实：资源是稀缺的，而且有多种用途。 谁会反对安全、健康、保护环境呢？但是人们对这些的追求是无限的，政策的推动者也会促使人们进行无限制的追求。 大多人会乐意政府投资亿万美元用于癌症研究，但是同样的钱也可以用于许多其他疾病的研究。在倡导预防犯罪、更好的健康、更清洁的空气和水等理念时，人们确实遗漏了权衡取舍的概念，在政治正确下，我们的要求变得没有节制。 开放式没有上限的要求增加了财政预算，为权力扩张提供了便利，也助长了庞大的政府官僚机构。 法官在执行反垄断法时也经常陷入“开放式谬误”，即担忧某个企业正处于垄断“初期”。美国最高法院对布朗鞋业公司并购案的判决就是一个里程碑事件。法院禁止布朗鞋业收购肯尼连锁鞋店，因为后者占美国 1% 的鞋类市场，如果被收购，布朗鞋业有可能逐渐形成行业垄断，所以必须将这种垄断风险扼杀在摇篮中。 假设无限制的推断理论是正确的，那按照这种推理逻辑，如果黎明开始温度上升了 10 度，就意味着月底之前我们都会被高温烤成脆脆酥。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:5:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"总结：经济学的正确思考 许多信条其实经不起推敲，但是如果没有人真正去审视它们，它们就会一直延续下去，尤其是当精明的倡导者通过利用人们的情感诉求来规避理性检验的时候。 一些普遍的谬误已经有上百年的历史了，它们在几个世纪前就已经被驳斥过，但是到了今天却能新瓶装旧酒，重新粉饰以新的形式适应当下的时代潮流。 基于这些谬误制定的经济政策常常会危害世界各国千百万人民的福祉，甚至一再造成毁灭性的后果。洞察这些谬误远不止磨砺心智这么简单，更清晰的去理解经济学，还可以为提升世界人民的生活水平，带来许多意想不到的机会。 从这儿转载 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:6:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"常见的还款方式：等额本息、等额本金、等本等息 一个一个来盘 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:0:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等额本息 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"基本信息 等额本息还款法，也称定期付息。借款人每月按相等的金额偿还贷款本息，其中每月贷款利息按月初剩余贷款本金计算并逐月结清。（MBA智库百科） 每期还的钱是固定的，即每期 本金+利息 总额是固定的。但每期还的钱中 本金、利息 的占比是动态的 👇 上图中基本信息，假设 借款总额：300w 借款年限：30 年利率：5% ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:1","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"具体公式 假设借款总额 $p$ 元，分了 $n$ 期，每期的利率是 $r$，那么 每期应还款 $$ 每期应还款=\\frac{p \\times r \\times (1+r)^n}{(1+r)^n-1} $$ 第 i 期还款中本金部分 $$ 第i期还款中本金部分=p \\times r \\times \\frac{(1+r)^{(i-1)}}{(1+r)^n-1} $$ 第 i 期还款中利息部分 $$ 第i期还款中利息部分=\\frac{p \\times r}{(1+r)^n-1} \\times ((1+r)^n-(1+r)^{(i-1)}) $$ import pandas as pd class MRPI: ''' 等额本息 - 多种翻译 Matching the Repayment of Principal and interest / Fixed installment method / average capital plus interest method 计算 - 每月还款 - 本金、利息部分 - generate DataFrame ''' def __init__(self, p,R,N): ''' p: 贷款总额 R: 年化利率 N: 贷款年限 ''' self.p = p self.R = R self.N = N self.n = N * 12 self.r = R / 12 def pay_amt(self): ''' 每期总额（月供） ''' term_total = ( self.p * self.r * (1 + self.r)**self.n ) \\ / ( (1 + self.r)**self.n - 1 ) return round(term_total,2) def pay_part(self): ''' 月供中 利息，本金 部分 ''' interest,principal = [],[] for i in range(self.n): # 利息 term_interest = self.p * self.r * ( (1+self.r)**self.n - (1+self.r)**i ) \\ / ((1+self.r)**self.n-1) term_interest = round(term_interest,2) interest.append(term_interest) # 本金 term_principal = ( self.p * self.r * ( 1 + self.r )**i ) \\ / ( (1 + self.r)**self.n - 1 ) term_principal = round(term_principal,2) principal.append(term_principal) return interest,principal def get_detail(self): ''' convert to DataFrame ''' df_detail = pd.DataFrame({\"term\":[i for i in range(1,self.n + 1)] ,\"月还款\":[self.pay_amt()]*self.n ,\"本金部分\":self.pay_part()[1] ,\"利息部分\":self.pay_part()[0] }) df_total = pd.DataFrame({\"term\":[\"总计\"] ,\"月还款\":[self.pay_amt()*self.n] ,\"本金部分\":[self.p] ,\"利息部分\":[self.pay_amt()*self.n - self.p] }) df = df_detail.append(df_total) return df_detail,df.reset_index(drop=True) ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:2","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等额本金 等额本金还款法，也称利随本清、等本不等息还款法。借款人将本金分摊到每期，同时付清上一个交易日至本次还款日之间的利息。（MBA智库百科） 每期还的本金是固定的，随着剩余本金的减少，利息便也动态减少 👇 但前期还款总额较多 上图中基本信息，假设 借款总额：300w 借款年限：30 年利率：5% ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:2:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"具体公式 第 i 期应还款 $$ 第i期应还款=\\frac{p}{n} + (1- \\frac{i-1}{n}) \\times p \\times r $$ 第 i 期还款中本金部分 $$ 第i期还款中本金部分=\\frac{p}{n} $$ 第 i 期还款中利息部分 $$ 第i期还款中利息部分=(1- \\frac{i-1}{n}) \\times p \\times r $$ import pandas as pd class MPR: ''' 等额本金 - 多种翻译 Matching the Principal Repayment / Reducing installment method (Fixed Principal) / average capital method 计算 - 每月还款 - 本金、利息部分 - generate DataFrame ''' def __init__(self, p,R,N): ''' p: 贷款总额 R: 年化利率 N: 贷款年限 ''' self.p = p self.R = R self.N = N self.n = N * 12 self.r = R / 12 def pay_part(self): ''' 本金，每期总额（月供） 以及 利息部分 ''' # 本金 term_principal = self.p / self.n amt,interest = [],[] for i in range(self.n): # 每期利息 term_interest = ( 1 - i/self.n ) * self.p * self.r # 每期还款 term_amt = term_principal + term_interest term_amt = round(term_amt,2) amt.append(term_amt) term_interest = round(term_interest,2) interest.append(term_interest) return round(term_principal,2),amt,interest def get_detail(self): ''' convert to DataFrame ''' df_detail = pd.DataFrame({\"term\":[i for i in range(1,self.n + 1)] ,\"月还款\":self.pay_part()[1] ,\"本金部分\":[self.pay_part()[0]]* self.n ,\"利息部分\":self.pay_part()[2] }) df_total = pd.DataFrame({\"term\":[\"总计\"] ,\"月还款\":[self.p + (self.n+1)*self.p*(self.r/2)] ,\"本金部分\":[self.p] ,\"利息部分\":[self.p*(self.n+1)*(self.r/2)] }) df = df_detail.append(df_total) return df_detail,df.reset_index(drop=True) 俩还款方式每期还款金额如下图所示 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:2:1","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等本等息 关于这个“等本等息”，常见于互金的现金贷业务、信用卡分期等 上面的等额本息、等额本金还款方式在计算每一期的利息时，也有按照占用借款时间的概念来计算，即还了的部分就不计算利息了 但 “等本等息”还款方式一开始就计算的死死的，每期还款中本金、利息都是固定的 比如，小明借了12w元，年利率是12%，分12期还。那么每期应还$11200=(\\frac{120000*(1+0.12)}{12})$ 还是基于 12w，年利率12%，分12期的情况，不同还款方式的情况如下所示 等额本金 等额本息 等本等息 1 11200 10661.85 11200 2 11100 10661.85 11200 3 11000 10661.85 11200 4 10900 10661.85 11200 5 10800 10661.85 11200 6 10700 10661.85 11200 7 10600 10661.85 11200 8 10500 10661.85 11200 9 10400 10661.85 11200 10 10300 10661.85 11200 11 10200 10661.85 11200 12 10100 10661.85 11200 总计 127800 127942.2 134400 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:3:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"总结 本文罗列了等额本息、等额本金和等本等息三种还款方式的基本信息。但产品设计的角度思考较少 虽然表面上看起来等本等息还款方式很暴力，但我觉得每种还款方式背后都有金融产品设计者的考虑吧 银行不会雪中送炭，只会锦上添花。小额贷款等机构 、平台都是一样的，在风险可承受的情况下给予放款，毕竟这是门生意 但政府的监管，限制利率上限等政策，带来的结果便是，更多的人借不到钱 🤷‍♂️ 原本的毛细血管更加的“细”。 从奥派的角度来看“高利贷”等问题，可参考陈志武教授的这篇文章 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:4:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["生活"],"content":"在疫情的情况下毕业了🎓，时隔一年2个月，我们终于是和导师成功约饭了 这次“会师”似乎没有在学校时候的拘谨了，和导师之间可谓是亦师亦友了 总体而言，感觉多聊聊还是挺好的，聊的几个点也挺有意思，在此做个记录好了。 可能是之前受章慧南教授的影响 每个人都有自己的兴趣点或是说研究方向，可以通过大白话的形式分享自己关注的部分，同时也能加深自己对该领域知识的理解 从个人而言，就可以通过这种方式 “多一只眼看世界”。就像TED，在那儿有各种各样的角度，分享着各种各样有趣的东西 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:0:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"回归人性 话虽如此，但我并未实际领悟到这四个字的深层含义，但也提供了一个思考的方向，很多时候有啥困惑的试着从人性的角度先去思考“为什么” 或许就能豁然开朗，但可能也不排除会以小人之心度君子之腹？🤔 hhhhh，没有什么十全十美的事儿，自己开心最重要，解释通了、开行了就好 工作之后，直接、间接学习到的一点也是 先思考为什么，再想其他的 比较典型的是之前我接手某件事儿时，总会吐槽这儿、吐槽那儿，但后来想想，这个方案放在当时的情景可能是个局部最优的方案了 比如会受到产品着急上线的压力；比如没有现在的新工具等等 总之，多一份为什么，少一份自以为是。 因为我发现自己在和上下游一起处理问题时，也会先着手解决当前的问题，达到局部最优，算是较为经济的一个方案吧🌞 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:1:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"先有蛋 倒不是指传统的 先有鸡、后有蛋的问题。而是指先能生蛋，在此基础上，得到更好的环境，以便产出更多的鸡蛋 可能这个比喻不恰当 但表达的意思主要是在某个圈子里得先有实力能崭露头角，方能升级进入到高阶水平，不断的打怪升级 回想硕士期间，我也是先写了篇小论文出来，才找各位老师们提意见，看看能不能带我一起玩～ 所以，还是得在某个领域提升自己 吃饭时也提出了“加点”方向和圈子内对不上怎么办，就像是你待在一个研究机器学习的圈子，而你在研究数据库 导师的第一反应也是换到“加点”方向的那个圈子呀～ 这个也是让我豁然开朗，对啊，是这个道理啊，我为什么要鸡同鸭讲，需要的是“琴箫合奏”、沧海一声笑 所以，先有蛋的那个蛋得是在鸡圈里呀，在猪、羊等胎生动物的圈里，除非遇上“伯乐”，知道且需要蛋。 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:2:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"身心健康 这一点虽然之前也有感悟，但一直未贯彻落实 身心健康是两个维度：身体和心理 《中国文化的深层结构》一书中也曾提及，国内传统文化及教育很少涉及“心理”，典型的是长辈说的最多的是注意身体之类的；常年在外回到家后往往会准备好吃的好喝的。 不过这也是，安身方可安心～ 只是说需要注意的是，“身”、“心”应齐头并进、双管齐下 这一点虽是老生常谈，但我个人并未很好的落实呀～ 🤦‍♂️ 外界是很难甚至没法改变的，我们能改变的或容易改变的是自己，所以 “修身，齐家，治国，平天下”，修身乃第一☝️位 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:3:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"最后 最后，恭喜我导今年在营销领域顶刊 Journal of Marketing Research 发表相关学术论文～ 希望大家一切顺利～🌞 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:4:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"不知不觉，离开学校已经有1年2个月了。。。 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:0:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"目标 这一年以来真的是经历了许多乌龙事件。最大的干扰点有二：一是户口；二是工作岗位。 心态差的时候，只想说，或许我的运气在实习期间都用完了吧 😂 实习期间遇到的人太棒、太赞了，以至于现在一塌糊涂… 心态好的时候，时常告诉自己说，这些都是经历、是“入道”的素材。 有时候想想，确实也是这样啊。毕业论文的致谢中，我依然会由衷的感谢导师给予我的自由度，也曾感叹着自己“摸黑”的日子 但尽管如此，在学校的时候隐隐约约似乎能感受到有些大方向依然还是在那儿的，真的就像黑暗中的烛光，忽闪忽闪的 然而，现在的状态大方向处于飘忽不定的状态，甚至是太大、太遥远了，使得自己有时候丧失了一切动力，只想躺尸 就希望能什么都不用想、不用顾虑“面包”🍞，每天尽享时间的流逝，就像电视剧里的样子 小时候我爸打趣的和我说，你看电视剧里都不讲赚钱的问题呐 真的觉得“系统1” 能轻而易举的战胜“系统2”。 没有错，保持理性是很累的一件事儿，是会消耗大量能量以及脑细胞的。 所以推出要将某事成为一种习惯、形成chunk，让一切都显得那么的自然与丝滑 chunk 就像每天在站点等交通工具 或是倒车入库 至于怎样才能将某事成为一种习惯呐，暂时认为就需要和时间做朋友了吧～以及自身的兴趣或动力 这是不是又回到了“先有鸡，还是先有蛋”的问题呐。本想着让一切是那么的自然，而习惯又需要动力来促成 也有可能是我想错了。但似乎也不需要走极端吧。即一定程度上的习惯降低动力的依赖或损耗，两者相辅相成呀☯️ 基于我目前对“奥派”的理解，以上便是属于目标的问题。目前的小目标都是下一个目标的过程：既是终点也是起点。 目标有了又如何？不做不都是白瞎么 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:1:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"落地 首先，我认为做什么都是每个人的自由，每个人都有自己的偏好，基于偏好、机会成本做决策。 早之前在旭辉实习的时候遇到位大佬，在短暂的接触中，真的是由衷的钦佩他～也曾就类似的事儿向其请教过 背景 初窥地产研究岗，也关注了些公众号，看到相应的文章会收藏（比如现在的“浮窗”），但在收藏中躺了x天，都没打开过了 他说，因为你不认为这件事儿此刻对你而言是重要的，或者可能你压根就没这个需求。我会看是因为我的工作和这些相关，需要去了解这些讯息，了解…。 是的，我目前认为就是这样。这里面除了重要性之外，还有紧迫性的维度 这件事儿、这个目标当下对我是否重要？ 紧迫感、重要性的加持，我想大概率就能落地了吧。想想憋毕业论文的那段时间… 除非真的是无力回天…直接弃疗… 我们都知道身体的重要性，但并没有那种紧迫感。再极端点说，可能没和死神擦肩而过，对于重要性的感触也没那么的深刻。有些事儿真的很难感同身受… 所以对于这些，重要不紧急的目标，我该怎么办？或许这类事儿、目标真的不需要太多，让“系统2”和习惯共同加持吧，同时也需要一些“助推（Nudge）”的方式。真的需要踏出那一步o.o “助推”解释为那种轻轻用手肘碰下你，就能实现目标 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:2:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"WLB 感觉实际参加工作之后就没顺过，，，也是因为自己的决策问题吧，这也算是让自己直接学习了一波之前间接学习的经验：比较重大的事情要积极启用“系统2”来理性分析各种依赖关系！ 切片数据来看，我觉得可以将企业工作者分为能力、资源2个维度来看。 资源多 资源少 能力强 资源多-能力强 资源少-能力强 能力弱 资源多-能力弱 资源少-能力弱 某种程度上其实是不太合理，因为两个维度并非完全独立，也可能会存在互为因果的情况。比如直观理解，因为能力弱（强），所以资源少（多）。 但很多时候并非如此，资源是有限的 实际做决策的是个人，很多时候是由上而下的，分配资源的是各个节点上的人，各个节点上的人具体怎么分配的谁都不知道🤷‍♂️ 现实情况一样存在能力弱的人所拥有的资源多于能力强的。但这是不是也很难界定？因为溜须拍马、机遇也是种能力啊🤷‍♂️ 跟人扯上关系的都很复杂，所以我觉得人文社科领域是不存在类似物理世界的漂亮公式的 而且，实际工作中也并不需要多强的能力，企业越大，分工越明确、“通用件”越多、互换性越强。 互换性 工科应该都会提及“互换性技术”，带来的好处就是哪个零部件坏了，拆下来换个新的又能正常运转了，而且这个新的零部件也很容易获得，大大提高了生产效率。 最近刚好在欧神的文章中看到这么一段话👇 得想想办法怎么深度挖掘潜能🤔 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:3:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"愿望🙏 希望未来一切都好～ 目标能落地 找到深度挖掘潜能的路子 修身养性、由艺入道 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:4:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["机器学习"],"content":"RT，可视化决策树结果，直观感受决策流程 早之前可视化领域专家、学者提出之后要结合各种机器学习算法，制作相应可视化的图标以及工具。比如 TensorFlow 的可视化工具包—TensorBoard 针对结构化数据建模的算法中，树模型是较为常见的，之前 一直用sklearn自带的tree_plot()函数或Graphviz tree_plot 最近发现了一个可视化树模型结果的package1，画出来的图长这样👇 iris-TD-3-X 各个节点的分布情况也比较清楚，直观感受决策树的逻辑 更多代码示例可参考 这个 或 这个 图片格式 但目前图片格式只支持 svg 这里主要记录下安装说明 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装2 原文 Python 版本 \u003e= 3.6 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装 graphviz 确保 graphviz 是 通过 pip 的方式安装的。 如果有装 Anaconda，又不确定是不是通过 pip 安装的，就走遍流程👇 conda uninstall python-graphviz conda uninstall graphviz pip install graphviz 接下来以 Windows 为例， 下载 graphviz.msi 并更新Path环境变量 假设保存路径为 C:\\Program Files (x86)\\Graphviz2.38，则将 C:\\Program Files (x86)\\Graphviz2.38\\bin 添加至用户变量（Path） C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe 添加至系统变量（PATH） 可以通过 where dot 验证是否安装成功 (base) C:\\Users\\Terence Parr\u003ewhere dot C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:1","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装 dtreeviz 通过 pip 安装，Windows 可以直接打开 Anaconda Prompt pip install dtreeviz # install dtreeviz for sklearn pip install dtreeviz[xgboost] # install XGBoost related dependency pip install dtreeviz[pyspark] # install pyspark related dependency pip install dtreeviz[lightgbm] # install LightGBM related dependency 决策树 这里有酷炫的决策树介绍文档 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:2","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"Reference https://explained.ai/decision-tree-viz/index.html ↩︎ https://github.com/parrt/dtreeviz#install ↩︎ ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:2:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["数据分析"],"content":"【Mark】通过正则表达式选择相应的列 在涉及子查询时，平时习惯直接把需要的列全写出来，也没想太多。 最近有小伙伴谈起正则选择需要的列，一顿操作后做个记录。建模取特征时，常见这种操作 Tip Spark 和 Hive 都是支持这种操作的 只是相应的设置不同 spark/hive set hive1 set hive.support.quoted.identifiers=none spark2 set spark.sql.parser.quotedRegexColumnNames=true 其中，正则表达式的写法可参考 JAVA regex 语法 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:0:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"例1，查询去除某几列的所有列 select 去除 user_no, cust_no 的所有列 select`(user_no|cust_no)?+.+`fromtable1 select 去除以 no 结尾的所有列 select`(.*no)?+.+`fromtable1 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:1:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"例2，查询符合某特征的所有列 select 以 no 结尾的所有列 select`.+no`fromtable1 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:2:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"Reference https://community.cloudera.com/t5/Community-Articles/Spark-to-support-REGEX-column-specification-for-Hive-Queries/ta-p/316579 ↩︎ https://stackoverflow.com/questions/52526768/does-spark-sql-supports-hive-select-all-query-with-except-columns-using-regex-sp ↩︎ ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:3:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"类似Excel的数据透视表，分类聚合。也可以协助实现行转列，Pivoting “Wide” to “Long” Format 在统计分析时总会遇到分类汇总的场景，类似Excel的数据透视表。SQL中按照 case when 或 IF 的写法往往会显得臃肿，较为方便的便是通过 pivot 1实现，但 Hive 不支持😢 运行环境 以下基于 spark-2.4.5U3 及以上版本 ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:0:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"基本语法 PIVOT({aggregate_expression[ASaggregate_expression_alias]}[,...]FORcolumn_listIN(expression_list)) The PIVOT clause can be specified after the table name or subquery. ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:1:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"实际应用 假设有张存有各个地区、各个产品的月销量的表（sales_table），我们需要统计各个月份所有地区产品销量的加总，形如👇 selectmonth,'毛巾','肥皂'fromsales_tablepivot(sum(sales)forproductin('毛巾','肥皂')); ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:2:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"Reference https://spark.apache.org/docs/3.1.2/sql-ref-syntax-qry-select-pivot.html ↩︎ ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:3:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"已经2021年了，SQL条件函数已不局限于case when 了，针对常见场景已新增许多 conditional functions 汇总 Hive SQL 常用的条件函数，👇 ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:0:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"if(condition, value True, value FalseOrNull) This is the one of best Hive Conditional Function 类似Excel中的 if 函数 selectif(state='online','在线','离线')asstate; ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:1:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"case when then end case when 较为常见，可注意执行顺序问题：从上至下依次执行，直到满足条件则跳出 常见写法有两种，主要取决于是否单变量作判断 selectcasedayofweekwhen1then'星期一'when2then'星期二'endasweek;-- OR -- selectcasewhendayofweek=1then'星期一'whendayofweek=2then'星期二'endasweek; ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:2:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"isnull(a) 判断是否为null（数据库特有的一种数据类型），留意字符型 'null', 'NULL' selectisnull('');/*return false */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:3:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"isnotnull(a) 非null selectisnotnull('');/*return true */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:4:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"nullif(a,b) 如果 a=b 则返回null，否则返回a 等同于 case when a = b then null else a end selectnullif(1,1);/*return NULL */selectnullif(4,3);/*return 4 */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:5:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"nvl(arg1,arg2) 可以理解为用arg2的值替换 arg1 中值为 null 的部分 selectnvl(null,999);/*return 999 */selectnvl('null',999);/*return null */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:6:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"coalesce(value1,value2,…) 返回第一个非null的值 往往只想得到众多列中非 null 的值 selectcoalesce(null,7,8);/*return 7 */ null 涉及null的问题要留意数据库中的数据类型。譬如，某列数据格式为string，'null' 并不等同于 null ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:7:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["写作"],"content":"将博客源文件托管在某个地方，换台电脑继续编辑。再加上触发机制更好，即一上传⏫就同步更新博客～ 在之前快速搭建个人博客的文章中有提到 将 public 文件夹下的文件推送至GitHub仓库 每次这么操作还是有点繁琐的，所以就想如果更新完hugo源文件，博客自动更新就好了。此外，换台电脑💻，依然正常操作就更更更好了～ 终于，是找到了GitHub的 Actions 中 Workflows 功能 将Hugo源文件维护在GitHub上，只要源文件的仓库更新，自动更新存有 public 文件的仓库，那么博客也就随之更新了。 在此记录下实现过程 👇 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:0:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"1. 生成SSH key （Windows 环境） 参考这个 通过git bash cd 至 .ssh 文件夹 cd ~/.ssh/ 如果提示 No such file or directory，可以手动的创建一个 .ssh文件夹，BY mkdir ~/.ssh 配置全局 name 和 email git config --global user.name \"你的用户名\" git config --global user.email \"你的公司或个人邮箱\" 生成 key ssh-keygen -t rsa -C \"你的公司或个人邮箱\" 连续按 3 次回车 最后得到俩文件： id_rsa 和 id_rsa.pub ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:1:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2. 创建并配置仓库 参考这个 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.1 配置hugo源文件的仓库 仓库名称不限，在此以 unclehuzi.github.io.source 为例 进入unclehuzi.github.io.source仓库，添加Secrets，名称为ACTIONS_DEPLOY_KEY，将 id_rsa 文件的内容粘过去，得到内容如下所示 上传 hugo源文件 把 themes 主题文件夹中的 .git 文件删除 不然Github 会检测到是别的仓库，上传后文件夹是灰色的 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:1","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.2 配置 unclehuzi.github.io 仓库 仓库名称有讲究，得是这个 github_user_name.github.io 进入unclehuzi.github.io 仓库，添加Deploy keys ，名称不限制，将id_rsa.pub文件的内容粘过去。 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:2","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.3 配置工作流（Workflows） 进入unclehuzi.github.io.source仓库，创建 Actions 代码如下 👇 name:Deploy Hugo Site to Github Pages on Master Branchon:push:branches:- master # Attention 1jobs:build-deploy:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v2- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'0.83.1'extended:true# 使用扩展版# Attention 2- name:Buildrun:hugo#--minify- name:Deployuses:peaceiris/actions-gh-pages@v3with:deploy_key:${{ secrets.ACTIONS_DEPLOY_KEY }}# 这里的 ACTIONS_DEPLOY_KEY 则是上面设置 Private Key的变量名external_repository:unclehuzi/unclehuzi.github.io# Pages 远程仓库 publish_dir:./publickeep_files:false# remove existing filespublish_branch:master # deploying branch# Attention 3commit_message:${{ github.event.head_commit.message }} 备注 Attention 1 source 仓库的分支名称为 master Attention 2 hugo 的版本 Attention 3 unclehuzi.github.io 仓库的分支名称 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:3","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"3. Finished 以后维护好source这个仓库就能实现 触发机制以自动更新blog 换个电脑 💻 继续写blog ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:3:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"Reference https://jinnzy.github.io/shi-yong-hugolai-da-jian-ge-ren-blog/#%E5%88%A9%E7%94%A8github-pages%E9%83%A8%E7%BD%B2blog https://www.jianshu.com/p/95262f5eba7a ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:4:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["数据分析","机器学习"],"content":"RT，SQL计算多个变量的IV（Information Value） ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:0:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"背景 变量的预测能力往往可以通过IV值来判断，类似之前的 SQL计算PSI IV值也有经验区间供参考，以及可通过SQL完成指标的计算 Information Value Predictive Power \u003c 0.02 useless for prediction 0.02 - 0.1 weak predictor 0.1 - 0.3 medium predictor 0.3 - 0.5 strong predictor \u003e 0.5 suspicious or too good ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:1:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"计算公式 关于 IV 的详细介绍，可参考这篇文章 具体计算公式如下 $$IV=\\sum_{i=1}^{n}(\\frac{Bad_i}{Bad_T} - \\frac{Good_i}{Good_T}) \\times WOE_i$$ 其中， $$WOE_i=\\ln(\\frac{Bad_i}{Bad_T}) - \\ln(\\frac{Good_i}{Good_T})$$ 公式说明 Bad、Good即表示正负样本，风控场景有好、坏的称呼 $n$ 为分箱的个数 $Bad_i$, $Good_i$ 表示第i个箱子“坏”、“好”人数 $Bad_T$, $Good_T$ 表示“坏”、“好”总人数 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:2:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"计算样例 分箱方式：等频（缺失值单独划为一箱） score_group group_bad_i group_good_i woe_i iv_i r1 271 31882 0.218363 0.0054 r2 225 30572 0.074301 0.0006 r3 195 29107 -0.01969 0.0000 r4 188 28761 -0.04429 0.0002 r5 163 28400 -0.17435 0.0025 r6 182 27387 -0.02778 0.0001 r7 194 28058 0.01187 0.0000 r8 160 24564 -0.04782 0.0002 r9 158 29625 -0.24774 0.0052 r10 70 17302 -0.52404 0.0118 missing 327 36519 0.270413 0.0098 以上例子最终得到 $IV=\\sum_{i=1}^{11}(IV_i)=0.0358$ ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:3:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"SQL实现 准备好预测变量（$X$）和目标变量（$y$），score表形如 ym no x1 x2 x3 x4 y 202101 a1 617 481 773 671.68 1 202102 a2 585 585 522 600.56 0 202102 a3 617 548 677 635.68 1 202102 a4 647 null 765 655.63 0 202102 a5 596 478 656 635.3 0 202102 a6 636 618 595 630 0 202102 a7 714 572 842 644.28 0 202012 a8 null 495 720 628.79 0 202012 a9 636 618 595 426 0 202012 a10 557 562 null 589 1 基于此得到各个变量在不同月份的预测能力 这里依然涉及窗口函数的应用以及行列互转 窗口函数-聚合 窗口函数-排序 窗口函数的“窗口” 行转列、列转行 思路 类似PSI的计算思路，计算IV的整体思路依然参照公式，（等频）分箱后，基于数据的断点（Breakpoint Value）统计出每个箱子的好坏人数 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"1. 列转行 将score表进行列转行，变为key-value 键值对的形式 droptableifexistsscore_value;createtablescore_valueasselectno,ym,y,score,score_valuefrom(selectno,ym,x1,x2,x3,x4,yfromscore)lateralviewouterexplode(map('x1',x1,'x2',x2,'x3',x3,'x4',x4))tasscore,score_value; ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:1","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"2. 分箱统计好坏人数 这里采用的是 等频分箱 droptableifexistsscore_group_nums;createtablescore_group_numsasselectym,score,score_group,group_bad_i,group_good_i,sum(group_bad_i)over(partitionbyym,score)asgroup_bad_total,sum(group_good_i)over(partitionbyym,score)asgroup_good_totalfrom(selectym,score,score_group,count(casewheny=1thennoend)asgroup_bad_i,count(casewheny=0thennoend)asgroup_good_ifrom(selecta.*,casewhena.score_valueisnullora.score_valuein('','null','NULL')then'missing'whena.score_value\u003c=r.score_array[0]then'r1'whena.score_value\u003c=r.score_array[1]then'r2'whena.score_value\u003c=r.score_array[2]then'r3'whena.score_value\u003c=r.score_array[3]then'r4'whena.score_value\u003c=r.score_array[4]then'r5'whena.score_value\u003c=r.score_array[5]then'r6'whena.score_value\u003c=r.score_array[6]then'r7'whena.score_value\u003c=r.score_array[7]then'r8'whena.score_value\u003c=r.score_array[8]then'r9'whena.score_value\u003c=r.score_array[9]then'r10'endasscore_groupfromscore_valuealeftjoin(-- 等频分箱 10 bins selectym,score,percentile_approx(score_value,array(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),999999)asscore_arrayfromscore_valuegroupby1,2)ron(a.ym=r.ymanda.score=r.score))groupby1,2,3); ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:2","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"3. 计算IV 回顾下公式 $$IV=\\sum_{i=1}^{n}(\\frac{Bad_i}{Bad_T} - \\frac{Good_i}{Good_T}) \\times WOE_i$$ 其中， $$WOE_i=\\ln(\\frac{Bad_i}{Bad_T}) - \\ln(\\frac{Good_i}{Good_T})$$ selectym,score,sum(iv_i)asivfrom(selectym,score,score_group,(ln(group_bad_i/group_bad_total)-ln(group_good_i/group_good_total))*(group_bad_i/group_bad_total-group_good_i/group_good_total)asiv_ifromscore_group_nums)groupby1,2 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:3","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"总结 在非建模场景，只想大概看下（或监控）各变量的预测能力时，为省去导出数据用Python计算IV的麻烦，本文便以IV的计算公式出发详细记录SQL计算过程 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:5:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"RT，SQL批量计算各个模型分的PSI，更方便的搭建模型分稳定性的监控，满足模型应用的充分条件 — 样本分布一致性 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:0:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"背景 应用模型的一大前提便是建模样本尽量和实际生产样本在分布上保持一致性，保证跨期层面的准确性 当模型分偏移到一定程度时，也该考虑迭代一版了 偏移程度可以用 PSI 这个指标来评价，而对于这个程度业界有个经验值1 psi PSI \u003c 0.1: no significant population change PSI \u003c 0.2: moderate population change PSI \u003e= 0.2: significant population change ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:1:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"计算公式 关于PSI的详细介绍，可参考我司模型大佬的这篇文章 具体计算公式如下 $$PSI=\\sum_{i=1}^{n}((Actual_i\\% - Expected_i\\%)\\times \\ln(\\frac{Actual_i\\%}{Expected_i\\%}))$$ 公式说明 n 为分箱的个数 $Actual_i\\%$ 表示第i个箱子的实际占比 $Expected_i\\%$ 表示第i个箱子的预期占比，（称其为比较的基准） ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:2:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"计算样例 以某一个模型分为例，计算跨期的psi bucket excepted_num except_rate actual_num actual_rate psi 1 26780 0.1010 31779 0.1359 0.01036 2 26355 0.0994 27439 0.1173 0.00298 3 26532 0.1000 26008 0.1112 0.00118 4 27416 0.1034 25816 0.1104 0.00046 5 26495 0.0999 24113 0.1031 0.00010 6 26588 0.1002 23146 0.0990 0.00002 7 25957 0.0979 21176 0.0905 0.00057 8 27530 0.1038 21442 0.0917 0.00150 9 25898 0.0976 18310 0.0783 0.00428 10 25710 0.0969 14682 0.0628 0.01484 以上例子最终得到 $PSI=\\sum_{i=1}^{10}(psi)=0.0362$ ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:3:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"SQL实现 我们希望得到的是从存有各个模型分的表（命名为 score）得到上图👆，score表样例如下 no week scoreA scoreB scoreC scoreD scoreE a1 9 617 481 773 671.68 561 a2 2 585 585 522 600.56 588 a3 16 617 548 677 635.68 563 a4 7 647 564 765 655.63 586 a5 12 596 478 656 635.3 586 a6 7 636 618 595 630 572 a7 22 714 572 842 644.28 563 a8 23 638 495 720 628.79 563 a9 3 636 618 595 426 526 a10 3 557 562 526 589 535 备注 一行表示该用户对应的各种模型分数，scoreA…scoreE 其中， week 表示第几周，这里以2021年第一周（[2021-01-04,2021-01-10]）的数据作为基准，即 excepted 这里会涉及之前一些文章的知识点 窗口函数-聚合 窗口函数-排序 窗口函数的“窗口” 行转列、列转行 思路 整体思路还是跟着 PSI 的计算公式走，按照某种方式（等频 / 等距）将基准的数据分成 n 箱子，基于基准数据的断点（Breakpoint Value）统计实际占比（$Actual$） 为了方便计算，先进行 列转行 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"1. 列转行 将score表进行列转行，变为key-value 键值对的形式 droptableifexistsscore_value_weekly;createtablescore_value_weeklyasselectno,week,score,score_valuefrom(selectno,week,score,score_valuefromscorelateralviewexplode(map('scoreA',scoreA,'scoreB',scoreB,'scoreC',scoreC,'scoreD',scoreD,'scoreE',scoreE))tasscore,score_value)wherescore_valueisnotnullandscore_valuenotin('null','NULL'); ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:1","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"2. 基于基准数据统计各箱nums 这里就要分成两种情况： 等频分箱 等距分箱 具体使用哪种分箱方式还是要结合模型分的实际应用情况 但是，当模型分呈现较为严重的偏态分布时，等频分箱会出现好几个箱子重合的情况（如下图所示）。这种情况算出来的PSI会小于真实值，此时可以采用等距分箱 等频分箱 droptableifexistsscore_group_nums_weekly;createtablescore_group_nums_weeklyasselectscore,week,score_group,count(no)asnumsfrom(selecta.*,casewhena.score_value\u003c=r.score_array[0]then'r1'whena.score_value\u003c=r.score_array[1]then'r2'whena.score_value\u003c=r.score_array[2]then'r3'whena.score_value\u003c=r.score_array[3]then'r4'whena.score_value\u003c=r.score_array[4]then'r5'whena.score_value\u003c=r.score_array[5]then'r6'whena.score_value\u003c=r.score_array[6]then'r7'whena.score_value\u003c=r.score_array[7]then'r8'whena.score_value\u003c=r.score_array[8]then'r9'whena.score_value\u003c=r.score_array[9]then'r10'endasscore_groupfromscore_value_weeklyaleftjoin(-- 2. 等频分箱 selectscore,percentile_approx(score_value,array(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),99999)asscore_arrayfromscore_value_weeklywhereweek=1-- 以第一周为基准 groupby1)rona.score=r.score)groupby1,2,3; 等距分箱 等距分箱相比等频分箱而言稍微复杂点，我的思路是先算出基准数据各个模型分区间的上下限，再统计各箱nums -- 10个箱子 -- -- 统计各箱nums droptableifexistsscore_group_nums_weekly;createtablescore_group_nums_weeklyasselectscore,week,nasscore_group,count(no)asnumsfrom(selecta.*,r.n,casewhena.score_value\u003e=r.range_downanda.score_value\u003cr.range_upthen'Y'whena.score_value=r.range_upthen'Y'else'N'endasis_rangefromscore_value_weeklyaleftjoin(-- 基准月各分数间隔 selectscore,n,min_score,max_score,(min_score+(n-1)*step)asrange_down,(min_score+n*step)asrange_upfrom(selecta.*-- 10 箱 ,casewhena.score_value=b.min_scorethen1elseceil(10*(a.score_value-b.min_score)/(b.max_score-b.min_score))endasn,b.min_score,b.max_score,b.stepfrom(select*fromscore_value_weeklywhereweek=1)aleftjoin(selectscore,min(score_value)asmin_score,max(score_value)asmax_score-- 10 个箱子 ,((max(score_value)-min(score_value))/10)asstepfromscore_value_weeklywhereweek=1groupby1)bona.score=b.score)groupby1,2,3,4,5,6)rona.score=r.score)whereis_range='Y'groupby1,2,3; ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:2","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"3. 计算PSI 回顾下公式 $$PSI=\\sum_{i=1}^{n}((Actual_i\\% - Expected_i\\%)\\times \\ln(\\frac{Actual_i\\%}{Expected_i\\%}))$$ droptableifexistsscore_stability_result_weekly;createtablescore_stability_result_weeklyasselecta.*,b.psifrom(-- 各箱nums selectscore,week,score_group_value[\"r1\"]asr1,score_group_value[\"r2\"]asr2,score_group_value[\"r3\"]asr3,score_group_value[\"r4\"]asr4,score_group_value[\"r5\"]asr5,score_group_value[\"r6\"]asr6,score_group_value[\"r7\"]asr7,score_group_value[\"r8\"]asr8,score_group_value[\"r9\"]asr9,score_group_value[\"r10\"]asr10from(-- 行转列 selectscore,week,str_to_map(concat_ws(',',collect_set(concat_ws(':',score_group,nums))))asscore_group_valuefromscore_group_nums_weeklygroupby1,2))aleftjoin(-- psi selectf.score,f.week,sum((f.act_rate-b.exp_rate)*log(f.act_rate/b.exp_rate))aspsifrom(-- Actual selectscore,week,score_group,(nums/sum(nums)over(partitionbyscore,week))asact_ratefromscore_group_nums_weeklywhereweek\u003e1)fleftjoin(-- Excepted selectscore,week,score_group,(nums/sum(nums)over(partitionbyscore,week))asexp_ratefromscore_group_nums_weeklywhereweek=1)bon(f.score=b.scoreandf.score_group=b.score_group)wheref.score_groupisnotnullandf.score_groupnotin('null','NULL')groupby1,2)bon(a.score=b.scoreanda.week=b.week); ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:3","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"可视化 最后，可以用Excel或BI软件完成对应的可视化 可视化 本文选择的是面积图 Python代码示例2 👇 # libraries import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd # Make data data = pd.DataFrame({ 'group_A':[1,4,6,8,9], 'group_B':[2,24,7,10,12], 'group_C':[2,8,5,10,6], }, index=range(1,6)) # We need to transform the data from raw data to percentage (fraction) data_perc = data.divide(data.sum(axis=1), axis=0) # Make the plot plt.stackplot(range(1,6), data_perc[\"group_A\"], data_perc[\"group_B\"], data_perc[\"group_C\"], labels=['A','B','C']) plt.legend(loc='upper left') plt.margins(0,0) plt.title('100 % stacked area chart') plt.show() area chartPercentage Stacked Area Chart \" area chart ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:5:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"总结 本文主要是提供了通过SQL计算多个模型分PSI的方案，并采用了等频、等距分箱两种分箱方法，增加了适用性 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:6:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"Reference https://mwburke.github.io/data%20science/2018/04/29/population-stability-index.html ↩︎ https://www.python-graph-gallery.com/255-percentage-stacked-area-chart ↩︎ ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:7:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析"],"content":"SQL, Python 中解决行转列、列转行的问题 在日常工作中总会遇到类似下图中的问题 👇 备注 我把这种情况称为 行转列 《Python for Data Analysis》 书中将其称为 Pivoting “Wide” to “Long” Format 还有这种问题 👇 备注 我把这种情况称为 列转行 《Python for Data Analysis》 书中将其称为 Pivoting “Long” to “Wide” Format 那么，接下来将针对此类问题，汇总SQL、Python中的实现方式 ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:0:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"行转列 / “Wide” to “Long” ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"SQL 运行环境 基于 spark-2.4.5U3 及以上版本 selectscore,ym,score_group_value[\"r1\"]asr1,score_group_value[\"r2\"]asr2,score_group_value[\"r3\"]asr3from(selectscore,ym,str_to_map(concat_ws(',',collect_set(concat_ws(':',range_label,nums))))asscore_group_valuefromscore_group_numsgroupby1,2) 基本思路是将表中 range_label和nums转化为类似json的格式，之后通过 key 索引得到对应的value 这里用的是collect_set()，得到的是聚合、去重后无序的array，若需要有序则可用sort_array() ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:1","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Python 构造数据 df_wide = pd.DataFrame({'score_name': ['ScoreA']*3 ,'ym': ['202012']*3 ,'range_label': ['r1','r2','r3'] ,'nums': [1110,1105,1054]}) df_wide pivot() 函数 df_wide.pivot(index=['score_name','ym'] ,columns='range_label' ,values=['nums']) ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:2","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"列转行 / “Long” to “Wide” ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"SQL 运行环境 基于 spark-2.4.5U3 及以上版本 selectbiz_no,ym,range_label,numsfromscorelateralviewouterexplode(map('r1',r1,'r2',r2,'r3',r3))tasrange_label,nums map 之后，结合 lateral view explode1 实现列转行的问题 ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:1","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Python 构造数据 df_long = pd.DataFrame({'score_name': ['ScoreA'] ,'ym': ['202012'] ,'r1': [1110] ,'r2': [1105] ,'r3':[1054]}) df_long stack()函数 df_long.set_index(['score_name','ym']).stack(dropna=False).reset_index().rename(columns={\"level_2\": \"range_label\",0:\"nums\"}) ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:2","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Reference https://blog.csdn.net/oopsoom/article/details/26001307 ↩︎ ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:3:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"以官方例子：微博转发关系图为例，说明所需要的数据格式 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:0:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"数据样例 import json from pyecharts import options as opts from pyecharts.charts import Graph with open(\"weibo.json\", \"r\", encoding=\"utf-8\") as f: j = json.load(f) nodes, links, categories, cont, mid, userl = j c = ( Graph(init_opts=opts.InitOpts(width=\"1920px\" ,height=\"1080px\")) .add( \"\", nodes, links, categories, repulsion=900, # 节点之间的斥力因子。值越大则斥力越大 # 支持设置成数组表达斥力的范围，此时不同大小的值会线性映射到不同的斥力。 gravity=0.01, # 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。 linestyle_opts=opts.LineStyleOpts(curve=0.2), label_opts=opts.LabelOpts(is_show=False), ) .set_global_opts( legend_opts=opts.LegendOpts(is_show=False), title_opts=opts.TitleOpts(title=\"Graph-微博转发关系图\"), ) .render(\"graph_weibo.html\") ) weibo.json 文件，可从 这儿 获取 主要由5部分组成 nodes links categories cont mid userl ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:1:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"格式说明 说明 本文聚焦在前三个的数据格式说明。每一部分是个list，每个list 又由多个dict组成 以“转发微博”作为场景简单阐述关系图所展示的信息：某位具有影响力的微博用户A 发了条微博，被用户B、C、D看到并转发了；之后，用户E、F、G也转发了B所转发的这篇文章，以此类推 那么，这个过程中涉及到的每个用户便是一个node。为此，也以 nodes作为切入点展开说明 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"nodes 样例如下所示， 记录着节点的信息12， { \"name\": \"Camel3942\", # 节点名称，即博主昵称 \"symbolSize\": 5, # 图中标志大小 \"draggable\": \"False\", # 是否可拖动 \"value\": 1, # 被再次转发次数 \"category\": \"Camel3942\", # From Where \"label\": { # 此博主被再次转发后，含有此标签，否则不含 \"normal\": { \"show\": \"True\" } } } 警告 节点名称（name）不能重复 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:1","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"links 个人理解作用是将每个node 连接起来 { \"source\": \"新浪体育\", \"target\": \"Beijingold4\" }, { \"source\": \"Camel3942\", \"target\": \"xiaoA\" } ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:2","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"categories 而这部分则记录着有被他人转发的用户名称（name），即 links 中 source 所对应的内容 { \"name\": \"新浪体育\" }, { \"name\": \"Camel3942\" } ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:3","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"Reference https://blog.csdn.net/qq_35006861/article/details/116721589 ↩︎ https://blog.csdn.net/Kevin_HZH/article/details/91043392 ↩︎ ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:3:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["写作"],"content":"详述Hugo+Github搭建个人博客 受我司同期大佬的影响，我也便搭建个人博客折腾下，记录点什么~ 在这demo成型之际，以小白视角记录下基于 HuGo 和 Github 完成搭建的历程 所以，最初的准备工作便是（假设已安装Git） ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:0:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"安装 Hugo 可以参考官方文档完成在Windows, Mac等平台的安装 虽然HuGo是基于Go语言编写的，但并不是一定要安装Go才能使用HuGo。可跨平台 在此，以 Windows 系统为例记录安装过程 在这儿下载对应的压缩包 解压 hugo_0.83.1_Windows-64bit.zip解压后得到的文件如下图所示 添加至环境变量 将hugo.exe文件放入bin文件夹（若无可新建） 验证是否安装成功 hugo version # hugo v0.83.1-5AFE0A57 windows/amd64 BuildDate=2021-05-02T14:38:05Z VendorInfo=gohugoio 本地创建博客 终端 cd 至某个文件下 hugo new site unclehuzi # blog目录就是创建的博客目录 # Congratulations! Your new Hugo site is created in D:\\unclehuzi. # Just a few more steps and you're ready to go: # 1. Download a theme into the same-named folder. # Choose a theme from https://themes.gohugo.io/ or # create your own with the \"hugo new theme \u003cTHEMENAME\u003e\" command. # 2. Perhaps you want to add some content. You can add single files # with \"hugo new \u003cSECTIONNAME\u003e\\\u003cFILENAME\u003e.\u003cFORMAT\u003e\". # 3. Start the built-in live server via \"hugo server\". # Visit https://gohugo.io/ for quickstart guide and full documentation. tree unclehuzi/ ##### D:\\unclehuzi ├─archetypes ├─content # 博客文章存放目录（markdown文件） ├─data ├─layouts ├─static # 静态文件/图片/CSS/JS文件 ├─themes # 博客主题模板存放目录 └─config.toml # 博客的配置文件 选择主题 我选择的是 Maupassant，通过Git 的方式获取 # cd 至 unclehuzi 文件夹 git clone https://github.com/flysnow-org/maupassant-hugo themes/maupassant cp themes/maupassant/exampleSite/config.toml . # 使用模板自带的配置文件替换默认配置文件 mkdir content/post # 创建博客文章md文件存放路径（该主题模板要求放在content/post目录下） 根据需要调整 D:\\unclehuzi\\config.toml 文件 baseURL = \"your_github_name.github.io\" # 修改为博客的网址，此处使用github pages地址，后面具体介绍 languageCode = \"zh-CN\" title = \"胡子叔叔的小站\" # 博客的名字 theme = \"maupassant\" 具体可以参考 官方说明 进行配置 本地测试 # cd D:\\unclehuzi hugo new post/my-first-blog.md vim content/post/my-first-blog.md # 以下为md文件内容 +++ title=\"My First Blog\" tags=[\"blog\"] categories=[\"博客相关\"] date=2020-01-16T10:37:32+08:00 draft=false # 此处要改为false，否则在首页不会显示！ +++ #### Hello World! 通过 hugo server -D 命令启动，访问 http://localhost:1313/ 即可 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:1:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"建立博客专属的仓库 主要的事情说三遍： Repository name 填 github名字.github.io，如 unclehuzi.github.io Repository name 填 github名字.github.io，如 unclehuzi.github.io Repository name 填 github名字.github.io，如 unclehuzi.github.io 仓库创建完成之后记得修改 D:\\unclehuzi\\config.toml 文件的 baseURL ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"生成 public 文件 创建完成之后，通过 hugo 生成 public 文件 hugo --theme=maupassant --baseUrl=\"https://your_github_name.github.io\" # theme为主题模板名称，url为上一步创建的github仓库名称 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:1","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"通过 git 命令将 public 文件推送至仓库 cd D:\\unclehuzi\\public # 1. 初始化 git init # 2. 将文件夹下所有文件加入本地仓库 git add . # 3. 添加注释 git commit -m \"comments\" # git commit -m \"updates $(date)\" # 4. 建立连接 BY HTTPS git remote add origin https://github.com/unclehuzi/unclehuzi.github.io.git # 5. 远程仓库拉到本地（如果远程仓库内没有文件可跳过步骤5） git pull --rebase origin master # 之后更新本地： git pull origin master # 6. 上传 push git push -u origin master # git push -u origin master -f # 强制上传 # 备注 # 和远程建立连接之后，执行 2-3-6 步骤即可。 最后，访问 https://your_github_name.github.io 即可 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:2","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"总结 本文基于 Github Pages 服务，使用成熟的框架及主题快速搭建个人博客demo。 更新网站两步走： 在博客文件夹（D:\\unclehuzi）下生成 public 文件夹 将 public 文件夹内的所有文件推送至GitHub仓库 后续在网站设计方面还需要了解下Web开发相关知识，针对性的修改主题的源码以实现自己的需求~ ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:3:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"不打羽毛球、不写代码的营销人不是一名好分析师","date":"2021-05-28","objectID":"/about/","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"工作经历 ","date":"2021-05-28","objectID":"/about/:1:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"360数科 思维层面 🤔 💫 （直接/间接）抽象职场通用 skills 🤺 理论指导实践，如 客群划分 技术层面 👨‍💻 精进SQL（Spark、Hive），如 sql计算PSI sql计算IV 快、准、狠 🏃 🏃‍♂️ 数据建模 试图做一个从理论出发的的“调包侠” ","date":"2021-05-28","objectID":"/about/:1:1","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"实习经历 在 数据分析 这条路上的探索，“一顿操作”（实习）之后得到的主要结论（/经验）： 👇 数据分析依托于行业 / 场景，但更加需要基于某个行业抽象出复用的部分； 基于1，一定要把 数据分析 单拎出来的话，更像是中台赋能的味道（或者说“乙方”）； 公司内对数据的重视程度决定了数分的地位（或者说数分的负责人是否能“话事”），否则就是 “SQL-boy”。 资源的重要性往往远远大于能力。公司存活与发展靠的是制度，并非个人英雄主义～ 正是因为这一层特性，除了 “big boss”，没有人是不可替代的。真的到了一定程度，“big boss” 也是可以换的 业务、技术加点方向可以围绕价值链进行：从数据的产生 👉 存储 👉 离线分析与挖掘 👉 反哺。其实各个阶段都需要面向业务，如，存储阶段中数仓的主题域 行业 公司 岗位 主要干啥 收获了啥 互联网 哈啰出行 🚴‍♀️ 数据分析 对接UI、运营的需求 指标体系；用户转化（/迁移）；Presto、Hive 房地产 旭辉 市场研究 经济政策、微观行为 量化指标；逻辑至上、数据支撑 咨询 Teradata 数据分析与挖掘 骚扰电话特征挖掘 分析思路；框架先行 汽车 通用·GM China 市场研究 数据基建、数据管理 VBA、SQL；经济学思维启蒙 融资租赁 开心汽车 数据分析 盘借贷逾期的逻辑 要面向业务逻辑写SQL ","date":"2021-05-28","objectID":"/about/:2:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"研究经历 矛盾性追加评论对感知有用性及购买意愿的影响研究 最初是用京东的评论写了个小论文，水了个会议。 硕士毕业论文时，主要从 数据源（一手问卷和二手电商平台评论数据）和 模型（零膨胀计数模型）两个维度更近了一步。 虽然一顿操作猛如虎，一看结论真是虎～ 但是，整个过程就像自己 owner 一个项目，从0到1、一步一步推进 💃、精准打击 👊 各个困难点～ 无论是为了满足会议的时间截止时间还是毕业论文阶段的各个节点，都让我离“时间管理大师”更近了一步 😄 当然，除了时间管理、项目管理，还有学习能力。即，需要在短时间内掌握相关知识以解决相应的问题。 爬虫 情感二分类 研究方法论 中介、调节以及有调节的中介效应分析 R语言 。。。 ","date":"2021-05-28","objectID":"/about/:3:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"平时干些啥 ","date":"2021-05-28","objectID":"/about/:4:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"羽毛球 🏸️ 平时（主要是周末）也会在下面俩地方打羽毛球 🏸 虽然技术不咋地，但就是玩~约球~ 上海市浦东新区羽山路1200号(近崮山路) 上海市浦东新区峨山路91弄140号同学汇综合运动馆 ","date":"2021-05-28","objectID":"/about/:4:1","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"游戏 🎮 领略“海拉鲁大陆”的风光，看心情拯救公主 👸 ","date":"2021-05-28","objectID":"/about/:4:2","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"写文章 该个人博客便是个例子，以及我的个人公众号 ~欢迎关注一波~ ","date":"2021-05-28","objectID":"/about/:4:3","tags":null,"title":"CV","uri":"/about/"},{"categories":["数据分析"],"content":"分场景汇总日期函数 工作中总会遇到处理时间的问题，参考营销理论中基于利益细分的市场细分理论，我从使用场景的角度出发，将常用的日期函数分为四大类： 时间计算 时间提取 格式转换 当前时间 Tip 本文重点在于整合日期函数 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:0:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"时间计算 这部分主要是计算时间差（datediff(end_date,start_date), months_between(date1,date2)）、时间加减（date_add(),date_sub(),add_months()）等 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:1:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"时间提取 提取时间戳的年、季度、月、周、日、小时、分钟、秒 可以直接调用对应的函数，也可使用extract(field from column_name) 函数指定 field，其中field 支持day, dayofweek, hour, minute, month, quarter, second, week and year. ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:2:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"格式转换 有时为了适应不同时间格式的需求，需要做个转换，如yyyy-MM-dd 或 yyyy-MM-dd HH:mm:ss的形式转为yyyyMMdd 等 常用： to_date() 返回 date 形式的日期，即yyyy-MM-dd date_format() 转为指定格式的时间，如 date_format('2015-04-08','y') =\u003e '2015' ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:3:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"当前时间 若需要时间戳格式，则用current_timestamp 若只需要精确到天，即date格式，则用current_date ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:4:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"总结 本文重点在于从使用场景的角度出发，整合了hive, spark 环境下常用的日期函数。最后再以表格的形式简单汇总 场景 函数 返回值类型 描述 示例 当前时间 unix_timestamp() bigint 当前 Unix时间戳（e.g. 1622451519 ），但因查询优化问题推荐使用 current_timestamp 当前时间 current_date date 当前日期 2021-05-31 当前时间 current_timestamp timestamp 当前时间戳 2021-05-31 17:12:14.968 - - - - - 格式转换 from_unixtime(bigint unixtime[, string format]) string 数字转为格式形如 2021-05-31 17:12:14 的字符串 格式转换 to_date(string timestamp) date to_date(yyyy-MM-dd HH:mm:ss) =\u003e yyyy-MM-dd 格式转换 date_format(date/timestamp/string ts, string fmt) string 得到指定格式的时间 date_format(‘2015-04-08’, ‘y’) =\u003e ‘2015’ 格式转换 trunc(string date, string format) string 得到被指定format截断的日期，format支持MONTH/MON/MM, YEAR/YYYY/YY trunc(‘2015-03-17’, ‘MM’) =\u003e 2015-03-01 - - - - - 时间提取 year(string date) int 年 时间提取 quarter(date/timestamp/string) int 季度 时间提取 weekofyear(string date) int 该年的第几周 时间提取 month(string date) int 月 时间提取 day(string date) dayofmonth(date) int 日 时间提取 hour(string date) int 小时 时间提取 minute(string date) int 分钟 时间提取 second(string date) int 秒 时间提取 extract(field FROM source) int field 支持day, dayofweek, hour, minute, month, quarter, second, week and year. - - - - - 时间计算 datediff(string enddate, string startdate) int 天数差 datediff(‘2009-03-01’, ‘2009-02-27’) =\u003e 2 时间计算 date_add(date/timestamp/string startdate, tinyint/smallint/int days) date 加（减）x天后的日期 date_add(‘2008-12-31’, 1) =\u003e 2009-01-01, date_add(‘2008-12-31’, -1) =\u003e 2008-12-30 时间计算 date_sub(date/timestamp/string startdate, tinyint/smallint/int days) date 加（减）x天后的日期 date_sub(‘2008-12-31’, 1) =\u003e 2008-12-30, date_sub(‘2008-12-31’, -1) =\u003e 2009-01-01 时间计算 add_months(string start_date, int num_months, output_date_format) string x月后。如果start_date是该月的最后一天 或者 x月后的天数不是“大月”则结果为x月后该月的最后一天 add_months(‘2017-12-31 14:15:16’, 2, ‘YYYY-MM-dd HH:mm:ss’) =\u003e ‘2018-02-28 14:15:16’ 时间计算 last_day(string date) string 该月最后一天的日期 last_day(2021-05-11) =\u003e ‘2021-05-31’ 时间计算 next_day(string start_date, string day_of_week) string 返回大于 start_date 的日期中最近的一个周x next_day(‘2021-05-31’,‘Monday’) 时间计算 months_between(date1, date2) double date1-date2 月数差 months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) =\u003e 3.94959677 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:5:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"以 “计算当前和上一次事件的时间间隔” 引入 positional function 截止到目前，窗口函数整理了聚合、排序场景，解决了“组内占比”、“定位连续3天登录用户”等问题 在平时的分析工作中，还有个比较常见的问题：计算当前和上一次事件的时间间隔。比如，相邻两次外呼的时间间隔 这个时候，lead() 或 lag() 函数可较为方便的解决该类问题 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:0:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"实现的功能 lead(), lag() 实现的功能比较类似。 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"lead() 上移 selectseller_name,sale_value,lead(sale_value)over(orderbysale_value)asnext_sale_valuefromsale; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:1","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"lag() 下移 selectseller_name,sale_value,lag(sale_value)over(orderbysale_value)asprevious_sale_valuefromsale; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:2","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"基本语法 lag(expression[,offset[,default_value]])over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...)lead(expression[,offset[,default_value]])over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) lead(), lag() 中的3个参数： expression - string 被操作的列名 offset - int 移动的行数（/偏移量） default_value 定义为空的情况赋给的默认值 其中，参数 expression 是必须的。而 default_value（默认是 NULL） 是只有当 offset（默认是 1） 有值时才能使用 over() 语句中，order by 是必须要有的 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:2:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"应用 计算用户相邻两次登录的天数间隔 -- 如果只有1天有登录信息，则 diff_days 为 null select*,datediff(session_date,lag_session_date)asdiff_daysfrom(-- 下移 select*,lag(session_date)over(partitionbyuser_idorderbysession_dateasc)aslag_session_datefrom(-- 按天去重 selectuser_id,date_format(session_time,'yyyy-MM-dd')assession_datefromtable1groupby1,2)); 窗口函数还有俩常见的：first_value(), last_value()，在此就略过了。 有时候可以用 row_number() over() 结合 having 一起使用，如 确定用户最后一次登录时间 selectuser_id,row_number()over(orderbysession_datedesc)asrkfromtable1havingrk=1; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:3:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"详述“窗口”的概念，结合初中数学中区间的概念来理解\u003cwindow_frame\u003e，并以计算累计占比为例深化理解。此外，也分享了他人整理的窗口函数汇总表 基于之前整理的 排序 聚合 Positional functions: lead(), lag() 在窗口函数应用场景方面算是告一段落了。但是在 “窗口” 这个概念上陈述较少，在窗口函数部分的里程碑之际重新 “定义” “窗口” 另外，之前在浏览网页的时候发现了窗口函数的汇总图 而本文与图中对应的便是 WINDOW FAME 部分 ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:0:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"实现的功能 简单来说是定义窗口函数作用的范围（“FRAME”），通过下面这张图1可以更好的了解 FRAME 的概念 一般而言， 一张表（Table）基于WHERE条件的作用得到图中 Result Set 部分； 窗口函数 over() 语句中 partition（若有）得到图中 Partition 1…Partition m fram 语句（若有）在partition基础上得到图中 Frame 1…Frame n ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:1:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"语法、规则 \u003cwindow_frame\u003e:=[rows|range|groups]between[unboundedpreceding|\u003cn\u003epreceding|currentrow]and[unboundedfollowing|\u003cn\u003efollowing|currentrow] 批注 目前，只有 PostgreSQL 11 及以上版本支持 groups \u003cwindow_frame\u003e语句表明，相对于当前行（current row）对应的值而言，还有“区间”的概念，“区间”又受到 rows或range 控制：是行数范围还是值的范围。 rows 对应是行的条件 如rows between 1 preceding and unbounded following 表示最终的范围是排序后（若有），基于当前行的上 1 行和该partition本身的最后一行 range 对应是值的范围 如range between 1 preceding and 2 following 这里我们遵循小学数学中区间的性质：左区间的值小于等于右区间的值 因为涉及到值的范围，这里就要分两种情况讨论了，假设当前行对应的值为 x 顺序排序，即从小到大，order by column asc [x-1,x+2]，左区间为当前行的值减1（x-1）；右区间为当前行的值加2（x+2） 逆序排序，即从大到小，order by column desc [x-2,x+1]，左区间为当前行的值减2（x-2）；右区间为当前行的值加1（x+1） 最后再说明下没有 \u003cwindow_frame\u003e 语句时对应的Frame，此时将取决于是否有order by语句，即 无 \u003cwindow_frame\u003e 语句、有 order by 语句 Frame 为 range between unbounded preceding and current row 即Frame的第一行为该partition的上边界，当前行（current row）为下边界 无 \u003cwindow_frame\u003e 语句、无 order by 语句 Frame 为 rows between unbounded preceding and unbounded following 即Frame的边界就是partition的边界 关于，无\u003cwindow_frame\u003e语句的情况，总结如下 \\ 无 \u003cwindow_frame\u003e 有 order by rows between unbounded preceding and current row 无 order by rows between unbounded preceding and unbounded following ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:2:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"应用：一个例子 一堆枯涩的陈述，不如直接来个小例子：计算累计占比 实际业务中，在定义模型目标变量y的时候，往往也会结合数据的分布。如，风控场景中定义逾期 x 天以上为bad 假设samples 表中记录着一笔订单的逾期状态，over_due_days 表示逾期天数 希望得到的数据样式如下表所示 over_due_days c_sum c_sum_rate 0 1000 1 1 100 0.1 2 90 0.09 3 85 0.085 4 80 0.08 … … … over_due_days 为1的那一行表示 $逾期天数 \\geq 1$的订单数以及占总订单的比例，即 $$c\\_sum\\_rate=\\frac{over\\_due\\_days \\geq 1 的订单数}{总订单数}$$ selectover_due_days,sum(nums)over(orderbyover_due_daysnullslastrangebetweencurrentrowandunboundedfollowing)asc_sum,sum(nums)over(orderbyover_due_daysnullslastrangebetweencurrentrowandunboundedfollowing)/(selectcount(1)fromsamples)asc_sum_ratefrom(-- 分逾期天数统计订单数量 selectover_due_days,count(1)asnumsfromsamplesgroupby1); ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:3:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"总结 本文便是窗口函数部分的“收官之作”了。 主要是对“窗口”的概念展开了详细的陈述，结合初中数学中区间的概念来理解\u003cwindow_frame\u003e，并以计算累计占比为例深化理解。此外，也分享了他人整理的窗口函数 ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:4:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"Reference https://en.wikibooks.org/wiki/Structured_Query_Language/Window_functions ↩︎ ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:5:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"“抛” 计算组内占比 “引” 聚合窗口函数 窗口函数中 求和（sum）、均值（avg）、极值（max, min）、计数（count）等结合聚合函数使用的场景也较多。 数据分析过程中经常会遇到计算组内占比的情况。 Example 计算 多个模型分以及多个时间段 的 psi 时，（等频/等距）分箱之后计算各箱样本占总样本数的百分比 示例如下表所示， model ym bucket act_rate A 202103 1 0.1209 A 202103 2 0.1148 A 202103 3 0.1089 A 202103 4 0.1041 A 202103 5 0.1004 A 202103 6 0.0983 A 202103 7 0.0984 A 202103 8 0.0937 A 202103 9 0.0892 A 202103 10 0.0714 比较方便的操作方式就是结合 sum() over() 函数计算组内占比。 selectmodel,ym,bucket,(nums/sum(nums)over(partitionbymodel,ym))asact_ratefrommodel_bucket_nums 其他几个聚合函数只是实现的功能不同，最后还是要各取所需了。 ","date":"2021-04-19","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E8%81%9A%E5%90%88/:0:0","tags":["SQL"],"title":"窗口函数-聚合","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E8%81%9A%E5%90%88/"},{"categories":["数据分析"],"content":"整理排序场景常用函数，row_number() over(), rank() over(), dense_rank() over(), ntile(n) over()，并以连续登录问题为例深化理解 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:0:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"row_number() over() ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"实现的功能 从1开始依次排序，生成不会重复的编号 -- 按照nums 列，降序排序 selectid,nums,row_number()over(orderbynumsdesc)asrankfromtable id nums rank 1x 45 3 2x 78 2 3x 87 1 4x 32 4 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"语法 row_number()over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) partition by表示基于某（些）维度（/列）分组之后，再基于order by的规则实现组内排序。 select id ,nums ,row_number() over(partition by id order by nums desc) as rank from table ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:2","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"应用 问题 如何确定连续登录天数超过3天的用户 思路 找到连续3天登录用户所表现的数据特征。比如，按照登录日期排序得到编号，两者作差，若连续登录则作差后的值是一样的 基于这个现象，可用row_number实现 selectuser_id,(session_date-rk)asdiff,count(1)asnumsfrom(select*,row_number()over(partitionbyuser_idorderbysession_date)asrkfrom(-- 按天去重 selectuser_id,date_format(session_time,'yyyyMMdd')assession_datefromtable1groupby1,2))groupby1,2havingnums\u003e=3; ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:3","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"rank() over() 基本语法类似于row_number() rank()over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) 但不同的是，当值相等时 rank() 排序会出现重复序号的情况，且下个序号和当前序号之差为当前相同值的个数 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:2:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectdealer_id,emp_name,sales,rank()over(orderbysales)asrkfromq1_sales; dealer_id emp_name sales rank 1 Raphael Hull 8227 1 3 May Stout 9308 2 2 Haviva Montoya 9308 2 1 Jack Salazar 9710 4 3 Abel Kim 12369 5 3 Ursa George 15427 6 2 Beverly Lang 16233 7 2 Kameko French 16233 7 1 Ferris Brown 19745 9 1 Noel Meyer 19745 9 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:2:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"dense_rank() over() row_number() over() 、 rank() over() 和 dense_rank() over() 之间的差别主要在于对相同值的序号处理方式不同。 和rank() over()一样，遇到相同值时序号会重复，但是dense_rank() over() 的下一个序号和当前序号之差依然是1，不会出现空位的情况。 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:3:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectdealer_id,emp_name,sales,dense_rank()over(orderbysales)asdenserankfromq1_sales; dealer_id emp_name sales denserank 1 Raphael Hull 8227 1 3 May Stout 9308 2 2 Haviva Montoya 9308 2 1 Jack Salazar 9710 3 3 Abel Kim 12369 4 3 Ursa George 15427 5 2 Beverly Lang 16233 6 2 Kameko French 16233 6 1 Ferris Brown 19745 7 1 Noel Meyer 19745 7 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:3:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"ntile(n) over() ntile(n) over() 和之前那三个排序函数不太一样。形式来看，多了个参数n，是指按照顺序平均分成n份（/箱），返回当前所在的位置。且需要order by 语句。 但对于不能实现平均分的情况，会基于约定来操作： 约定 每箱记录数不能大于上一个箱子的记录数。即第1组的记录数大于等于第2组的记录数。 所有箱子的记录数要么相同。要么从某一记录数较少的箱子（命名为X）开始，后面所有箱子内的记录数都与该箱（X）的记录数相同。即如果前3箱的记录数都是9，而第4箱的记录数是8，那么第5、6箱及其之后箱子内的记录数也必须是8。 注意 最先分出来的箱子，采取向上取整（ceil()）的方式 比如，53条记录，基于ntile的约定分到5个箱子，则每个箱子的记录数如下所示 bucket nums 1 11 2 11 3 11 4 10 5 10 备注 ntile的方法能较好实现等频的效果，相比分位数作为分割点而言，不易受数据分布的影响。 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:4:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectemp_mgr,sales,ntile(5)over(orderbysales)asntilerankfromq1_sales; emp_mgr sales ntilerank Kari Phelps 8227 1 Rich Hernandez 9308 1 Kari Phelps 9710 2 Rich Hernandez 12369 2 Mike Palomino 13181 3 Rich Hernandez 15427 3 Kari Phelps 15547 4 Mike Palomino 16233 4 Dan Brodi 19745 5 Mike Palomino 23176 5 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:4:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"总结 窗口函数 返回类型 描述 row_number() int 从1开始依次排序，生成不会重复的序号 rank() int 从1开始依次排序。若值相等则得到同样的序号；且下一个序号将会出现空位，即若2个相等的值序号是1，则下一个序号是3 dense_rank() int 从1开始依次排序。若值相等则得到同样的序号；但下一个序号不会出现空位，即若2个相等的值序号是1，则下一个序号依然是2 ntile(n) int 将分组数据按照顺序平均分成n箱，返回当前值所在位置，n-th ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:5:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析","写作"],"content":"调节效应分析（ Moderation Analysis ），回答 WHEN 的问题 之前整理了中介效应分析，解决了怎么看中介效应是否显著的问题。 这篇继续整理调节效应分析（Moderation Analysis） 中介变量回答的是关于 HOW 的问题，而调节变量回答的是关于 WHEN 的问题 $X$ 什么时候影响 $Y$，或 $X$ 影响 $Y$ 的过程中是否取决于变量 $W$ ，而变量 $W$ 就是调节变量 典型且简单的调节效应模型图如下所示 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"理论先行 调节效应理论模型用 statistical diagram 表示为， 即， $$ Y=i_Y+b_1X+b_2W+b_3XW+e_Y $$ ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"系数解释 各系数的解释如下 $b_1$ $b_1$ 是当 $W=0$ 时，$X$ 改变一个单位，$Y$ 改变 $b_1$ $$ b_1=[\\widehat{Y}|(X=x+1,W=0)] - [\\widehat{Y}|(X=x,W=0)] $$ $b_2$ $b_2$ 是当 $X=0$ 时时，$W$ 和 $Y$ 之间的关系 $$ b_2=[\\widehat{Y}|(W=w+1,X=0)] - [\\widehat{Y}|(W=w,X=0)] $$ $b_3$ 这是个大头，他是比较两组之间的差异， 一组是 $W$ 不变，$X$ 改变一个单位 另一组是，$W$ 和 $X$ 都改变一个单位 即， $$ b_3=([\\widehat{Y}|(X=x+1,W=w)] - [\\widehat{Y}|(X=x,W=w)]) - ([\\widehat{Y}|(X=x+1,W=w+1)] - [\\widehat{Y}|(X=x,W=w)]) $$ 做调节效应分析的时候， 理论上是希望 $X$ 影响 $Y$ 的过程中取决于变量 $W$ 对应着，公式（1）也可以改写为， $$ Y=i_Y+ \\theta_{X \\rightarrow Y}X + b_2W + e_Y $$ 其中，$\\theta_{X \\rightarrow Y} = b_1+b_3W$ 这也就生动形象的表示了， 如果 $b_3$ 显著性不等于 0 ，那么 $W$ 的值不同，$X$ 对 $Y$ 的影响也不同 所以当回归结果中，系数 $b_3$ 显著时（$p\u003c0.05$），即变量 $W$ 的确起到调节作用时 我们将会进一步分析对应不同 $W$，$X$ 是如何影响 $Y$ 的 常用的方法便是 pick-a-point approach ，又称 spotlight analysis 我第一次看到 spotlight analysis 这个名词，是在一篇JM的文章上，当时查了半天也不知道啥意思。。。 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:1","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"pick-a-point approach 基本思路是根据 $W$ 的数据选三个值，进而表示低-中-高三种状态 一般就选数据16th, 50th, and 84th分位数分别表示低-中-高，进而分析 $X$ 对 $Y$ 的影响是否显著 如果 $W$ 是分类变量就直接看各自类别的情况了，不用取点了。 所以，综上所述，基于公式(1) 构建回归模型，根据交互项系数 $XW$（$b_3$）是否显著，进而确定调节效应是否存在。 以上这些都能通过PROCESS1很好的实现。 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:2","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"实际操作 步骤如下： SPSS加载 process的语法文件 使用过程中可以自主编写语法、也可以菜单式的操作，如 process y=justify/x=frame/w=skeptic/model=1/plot=1. 变量和数据字段一一对应 基于理论研究模型，选择PROCESS的模型（本例对应的是model 1） 选项中勾选 Generate code for visualizing interactions 如果是用spss语法的话，加上 plot=1 即可 这能方便用SPSS作斜率图，用于可视化调节效应 复制PROCESS分析结果中的可视化脚本即可，如 （可选）中心化 有些文章中会说做调节效应分析前，对变量 $X$ 和 $Y$ 进行中心化处理，可能会见到 scaling 这样的术语 但其实无所谓啦，而且就算对和进行中心化处理，也不会影响 $W$、$XW$ 的系数，只是会影响 $X$ 的系数（$b_1$）罢了 因为之前有提到，$b_1$ 是表示 $W$ 为0时，$X$ 对 $Y$ 的影响，如果对 $W$ 做中心化处理， 即 $W^,=W - \\overline{W}$ 那么这时候 $X$ 的系数对应的是 $W$ 取样本均值（$\\overline{W}$）时，$X$ 对 $Y$ 的影响 得到结果之后，直奔交互项系数（本例对应的是 $b_3$）即可 若显著（$p\u003c0.05$），PROCESS便会生成低-中-高状态下，$X$ 和 $Y$ 之间的关系如下所示， ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"参考资料 Hayes A F: Introduction_to_Mediation_Moderation_and_Conditional_Process_Analysis_A_Regression_Based[M].2ed.2018 ↩︎ ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析"],"content":"过去的一段时间里一直在琢磨客群划分的问题 segmentation 隐隐约约觉得所谓的 客群划分 和 市场细分 不谋而合，并且我发现风控领域很重视 “圈客群” 但因为部门不同，最后关注的目的也存在一定的差异。对于风控而言，在兼顾风险的同时，给出差异性的策略。在贷前，差异性策略方面可能更多是关注授信额度的问题。贷中，更多关注调额方面。 本文将整理市场细分/客群划分的一些方法 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:0:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"有监督学习—树模型 相关文章： 决策树简介 可视化决策树结果 样本不均衡的问题 decision tree 将客群划分（/市场细分）看成是一个分类问题，因为做客群划分肯定是有目的/目标的，比如是否违约、客户忠诚等等 针对我们的目标，定义好我们的问题，进一步得到模型需要的label（y值）。基于提取的变量，最后可采用决策树实现客群划分，甚至可以采用随机森林、XGBoost，可视化前几棵树，看看模型给出的变量规则。 除了对算法本身的了解外，我觉得 找变量（影响label的影响因素）也是难点，毕竟需要面向业务建模（最近新造的词）。除了关注模型区分度等，更多需要注意的是 解释性，是否make sense，能不能从业务的角度去解释树模型给出的规则。 所以，在特征选择以及构造新特征方面真的需要花很大的功夫。 得到树模型输出的规则后，我们还需要注意 客群之间的差异性 基于决策树得到的规则，看客群之间在目标方面（如，违约率）的差异性 客群的稳定性 基于规则，看未来的人数分布及目标（如，违约率） 这里还涉及到一个外部数据的问题。客群划分时，尽量避免使用外部数据、尤其是被加工过的一些指标。一是不知道计算逻辑；二是不稳定。所以一般直接选取用户本身属性变量。当然，这也取决于分群的场景。 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:1:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"聚类 clustering 聚类用于市场细分也是比较常见的。 但最近我觉得，聚类目的性不明确。换句话说，市场细分的目的性不明显。 聚类更多是在某些变量下将相似的归为一簇 但在车企，还是会采用聚类细分市场的，如基于价格、投影面积、离地高度等变量划分车型。这也符合最初的目的：根据某些车辆参数相似的归为一类。 到底是视为聚类还是分类问题，还是要具体问题具体分析 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:2:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"基于常规的细分变量 人口统计变量 性别、年龄、职业、收入等，但“收入”的数据一般比较难获取，大多数情况下可能也是通过各种方式去预测 地理位置 “地域决定论” + 一方水土养一方人 这个还是要考虑具体业务 如果从逾期等违约情况来看的话：经济基础决定上层建筑，而一个城市的文明程度和经济发展也是有很大关系的。而经济发展又会受到地域的限制，正是所谓的“地域决定论”，所以经济发展差距较大的城市往往违约率也存在一定差异 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:3:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"不成熟的想法 在现金贷背景下，基于入不敷出的逻辑来关注风险的问题，我通过构建“收支”模型（收入和支出之间的关系）实现客群的划分。 seg 收入 支出 1 高 高 2 高 低 3 低 高 4 低 低 当然，这里的高、低是针对产品的目标用户而言 显然，对于低收入、高支出的客群，更有可能发生逾期的情况，但同时这部分人群也是更有可能产生收益的人群，所以针对这部分人群就需要深挖，兼顾风险的同时给予一定的高额度。 但难点就是在于对“收入”、“支出”的测量，在无法准确获取收入、支出数据的情况下，无论是用于测量收入还是支出的变量都需要满足产品所服务对象的特征，以能较为准确的测量“收入”与“支出”的状态。 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:4:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"总结 本文分别阐述了 有监督、无监督以及 常规细分变量三大划分客群的方法，具体还是要结合实际业务场景采用相应的方法 基于相应的规则完成划分后，还要基于目标问题进行横纵向对比： 横向比较客群的稳定性 纵向比较客群的差异性 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:5:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["写作","数据分析"],"content":"基于R，总结零膨胀计数模型的应用流程 我之所以用计数模型，主要是受到清华水利专业某博士某篇论文1的启发。我也将他构造变量的思路也写进了我论文的展望部分。 在经济学及社会科学领域也会遇到对计数数据（Count Data）建模的任务 计数数据是一种数据类型，取值只能是非负整数{0,1,2,3,…}，并且数值并不是排名而是计数 泊松回归（Poisson Regression）是处理计数数据的常用方法 stats package - glm() 函数 但是泊松回归并不能很好的解决计数变量数据中“过度分散”（over-dispersion）的问题 针对 over-dispersion 的问题，往往会采用负二项回归（Negative Binomial Regression）的方法 MASS package - glm.nb() 函数 但，无论是泊松回归还是负二项回归都不能很好的解决计数数据零过多的问题 此时，零膨胀计数模型（zero-inflated models）就诞生了 zero-inflated models are mixture models that combine a count component and a point mass at zero. 关于零膨胀计数模型，各自的软件或语言都有自己的实现方法。从我查阅的资料来看，目前 stata 和 R 在这方面资料比较多，Python 相对较少。 我选择的是R，因为Anaconda装rstudio比较便捷，且学习成本比stata低。 所以本文基于R，在确定计数数据存在零膨胀的前提下（一般看计数数据的频率直方图，或者直接用vuong()完成模型选择），总结零膨胀计数模型的应用流程 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:0:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"启动rstudio 用 Anaconda 安装 Rstudio 的过程中，需要手动创建一个R运行的新环境（如，r_env） 打开 anaconda 比较费时儿，所以会在 conda命令端 启动Rstudio # 查看环境目录 conda info -e # 切换至文件目录下启动 cd \"工作目录\" # 激活环境 conda activate r_env # 启动 Rstudio rstudio ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:1:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"读取数据文件 R打开csv格式的文件比较省事儿 # 数据文件保存在rstudio启动的目录下 my.data1\u003c-read.csv(\"data.csv\") ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:2:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"Over-Dispersion？ 有两种方法检验计数数据是否存在over-dispersion的现象，但对象不同，用的模型不同 odTest() dispersiontest() ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"odTest() 需要 pscl package odTest(glmnb_obj, alpha=.05, digits = max(3, getOption(\"digits\") - 3)) 先用负二项模型跑一次回归 library(MASS) nb \u003c- glm.nb(y ~ x1+x2+x3, data=my.data1) library(pscl) odTest(nb) $p-value\u003c0.05$，说明计数数据存在 over-dispersion 现象 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:1","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"dispersiontest() glm_p \u003c- glm(y ~ x1+x2+x3, data = my.data1, family = poisson) library(AER) dispersiontest(glm_p,trafo=1) 计数数据存在零膨胀且over-dispersion的现象，那么就需要采用零膨胀负二项模型 若不存在over-dispersion的现象，则采用零膨胀泊松模型 当然，可以两个模型都跑一次，最后通过vuong()完成模型的选择 所以，也可以比较零膨胀泊松模型和泊松模型 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:2","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"run regressions 零膨胀计数模型都依赖 pscl package ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"零膨胀负二项回归模型 m3 \u003c- zeroinfl(y ~ x1+x2+x3|x4+x5+x6, data=my.data1,dist=\"negbin\") 零膨胀计数模型是分成了两部分建模：一部分是计数部分，另一部分处理为二分类 $(0,1)$ 的情况 $x_1,x_2,x_3$ 是选定的计数部分的影响因素；$x_4,x_5,x_6$ 是二分类情况的影响因素 如果两者的影响因素一样的，则公式形式可以简单写为 m3 \u003c- zeroinfl(y ~ x1+x2+x3, data=my.data1,dist=\"negbin\") # 若数据中除y之外均为影响因素 m3 \u003c- zeroinfl(y ~ ., data=my.data1,dist=\"negbin\") ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:1","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"零膨胀泊松回归模型 m4 \u003c- zeroinfl(y ~ x1+x2+x3|x4+x5+x6, data=my.data1,dist = 'poisson') $x_1,x_2,x_3$ 的表现方式同上 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:2","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"模型选择 vuong(m3,m4) 根据结果选择模型即可 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:5:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"参考资料 Hanchen Jiang et al.: 10.3390/su10051509 ↩︎ ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:6:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"这篇是文本情感分析的应用篇 运用情感分析技术，让我们的研究更丰富 本文所讨论的情感分析聚焦在 二分类 情况，即判断一句话（短文本）的情感倾向是正面还是负面 我们可以简单的理解为： 存在一个很牛逼的箱子，这个箱子有进口处和出口处，我们需要做的便是把某句话通过进口处放入这个箱子，之后这个箱子从出口处吐出结果 其中情感倾向的结果方面，大多数箱子是会告诉我们情感倾向为正的概率是多少（假设是 $a$ ），显然情感倾向为负的概率便是 $1-a$ 若 $a\u003e1-a$ ，即 $a\u003e0.5$ ，“箱子”认为这句话的情感倾向是正面的 总之，我们可以根据研究的需要，根据箱子吐出的结果构造变量用于下游的任务。 那么，本文就系统的整理了好用的“箱子” ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"ERNIE1 谷歌的BERT问世之后，预训练模型那是备受关注啊，算是自然语言处理领域的里程碑事件了。 我为了蹭热度，琢磨百度的这个ERNIE，也是废了点时间 在本地装好各种环境之后，在我这个小破电脑上训练了一天一夜吧 但最终的方案是蹭了百度的 AI Studo 注册账号，运行项目还送算力卡，这样就能用GPU训练了 小破电脑一天一夜的活，几分钟就搞定了 本地装包，没梯子的话，一定要用清华的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ Python代码 #需要更改训练集、验证集、测试集的文件位置 import paddlehub as hub !hub install ernie module = hub.Module(name=\"ernie\") class DemoDataset(BaseNLPDataset): \"\"\"DemoDataset\"\"\" def __init__(self): # 数据集存放位置 self.dataset_dir = \"work\" super(DemoDataset, self).__init__( base_path=self.dataset_dir, train_file=\"train.tsv\", dev_file=\"dev.tsv\", test_file=\"test.tsv\", # 如果还有待预测数据，可以放在predict.tsv predict_file=\"predict.tsv\", train_file_with_header=True, dev_file_with_header=True, test_file_with_header=True, predict_file_with_header=True, # 数据集类别 label_list=[\"0\", \"1\"]) dataset = DemoDataset() reader = hub.reader.ClassifyReader( dataset=dataset, vocab_path=module.get_vocab_path(), sp_model_path=module.get_spm_path(), word_dict_path=module.get_word_dict_path(), max_seq_len=128) strategy = hub.AdamWeightDecayStrategy( weight_decay=0.01, warmup_proportion=0.1, learning_rate=5e-5) config = hub.RunConfig( use_cuda=True, num_epoch=1, checkpoint_dir=\"ernie_txt_cls_turtorial_demo\", batch_size=128, #一般为2^n eval_interval=10, strategy=strategy) inputs, outputs, program = module.context( trainable=True, max_seq_len=128) # Use \"pooled_output\" for classification tasks on an entire sentence. pooled_output = outputs[\"pooled_output\"] feed_list = [ inputs[\"input_ids\"].name, inputs[\"position_ids\"].name, inputs[\"segment_ids\"].name, inputs[\"input_mask\"].name, ] cls_task = hub.TextClassifierTask( data_reader=reader, feature=pooled_output, feed_list=feed_list, num_classes=dataset.num_labels, config=config) run_states = cls_task.finetune_and_eval() # 预测 data = [[d.text_a] for d in dataset.get_predict_examples()] run_states = cls_task.predict(data=data) results = [run_state.run_results for run_state in run_states] ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"senta2 百度的senta模型 有种模型即软件的味道 对于我这种小白很友好，比ERNIE方便 加载好模型就能用 CNN, GRU, LSTM, BiLSTM import paddlehub as hub # 加载模型 cnn = hub.Module(name='senta_cnn') # 预测 data_dict = {\"text\":[\"你怎么那么好看\"]} results = cnn.sentiment_classify(data = data_dict) ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"snownlp3 snownlp比较老了，但他的训练集都是电商评论的数据 契合我的主题，所以我也用了 这个用起来也方便 from snownlp import SnowNLP s = SnowNLP(u'这个东西真心很赞') s.sentiments # 输出情感倾向为正面的概率 当然，snownlp还能干别的，如分词、繁体转简体、提取关键词… ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"参考资料 https://aistudio.baidu.com/aistudio/index ↩︎ https://www.paddlepaddle.org.cn/modelbasedetail/senta ↩︎ https://github.com/isnowfy/snownlp ↩︎ ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:4:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作"],"content":"毕业论文模板，样式先行 三年前的我搞毕业设计，第一次接触这玩意儿，一脸懵逼 好在我朱哥搞过大创（还是国家级的），当时给我各种科普单片机的知识 搞大创的好处就是当我们不知道是画机械图还是搞电路的时候，朱哥设计的成品以及论文都搞定了，，，跑马灯跑起来、酒精浓度测起来、小屏幕亮起来 不过朱哥也没继续搞汽车，后来投身了我国交通事业。依稀记得朱哥远程面试的时候，寝室几个人都在打游戏，，，在我们的影响下，朱哥最终去了广东某985高校 但那时候我们对Word排版都不怎么了解 以下情景历历在目： Example 这个参考文献的上标怎么弄？ 又要加篇参考文献？序号不得又重写？ 参考文献的引用格式是啥？ 这些字体都要改成三号、加粗？然后就用格式刷一顿更新 三线表是什么鬼？又要画三线表？ 。。。。。。 总之，我们都在Word上花了很长时间，不断的进行重复性的工作，以满足格式上要求 但当我们面对硕士毕业论文时，在格式方面花的时间就很少很少了，没有了各种重复性的操作 因为我们秉承着**样式先行。**在写论文之前，就根据学校的要求，把字体、表格的格式调整好，写作的过程中随时切换，而且就算后续要调整，统一调整对应的样式即可，避免了重复性操作。 本文并不是记录如何去调整/设计样式，因为我认为这个真的是太多了，重点是基于毕业论文格式记录word的正确打开方式，抽象出复用的部分。这也是我开发模板的原因。面对不同的需求，后续只需要微调即可。 可以在我的公众号内回复 word， 获取华东师范大学毕业论文模板 （用过的朋友都说好） ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:0:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"模块拆解 根据论文的总体框架，分别设计对应的样式。 换句话说便是先弄清楚整篇论文的字体、段落会涉及到哪些不同的格式要求，在正式写作之前，分别设计好对应的样式。以便写作过程调用。 毕业论文主体部分的格式要求，往往会涉及到各级标题、正文、脚注、参考文献（参考文献部分后续会单拎出来记录）字体的格式，理工科专业往往会要求表格是三线表。 确定好字体、段落的格式要求后，就可以针对性的修改/设计样式 为了方便后续的写作，可以设计相应的快捷键（样式-\u003e格式-\u003e快捷键） ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:1:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"参考文献格式 正文中往往会引用相关参考文献 这时候就要放出文献管理大杀器——NoteExpress 有了他，妈妈再也不用担心我修改参考文献的格式了 在软件中记录好被引用的内容，如作者、论文标题、年份、期刊或会议名称等等（大部分pdf、caj文献都能有效识别） 每当需要标注引用时，直接点击“引用”即可 通过NoteExpress引用至word后主要涉及两大部分： 正文部分的标注 上标序号 或 作者+年份 的形式 附在最后的参考文献 如 [1] LIU Y. Word of mouth for movies: Its dynamics and impact on box office revenue[J]. Journal of Marketing. 2006, 70(3): 74-89. 依然是样式先行的逻辑，简便的办法是先在NoteExpress样式库中选择与自己要求相似的样式，当然，完全满足需求就更好了。 接着，在所选样式的基础上修改相关格式以满足自己的需求 引文下的修改便是对应修改“正文部分的标注”的格式 题录下的修改便是对应修改“附在最后的参考文献”的格式 值得一提的是，题录中可设置排序的规则，因为有些学校要求英文文献在前面、之后再按照时间或作者排序。 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:2:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"Tips 最后再记录些杂七杂八的tips ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"表格、图片名称 选中表格（/图片），右键“插入题注”，标签选择“表”，编号选择“包含章节号” ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:1","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"交叉引用 论文中往往会看到如图xxx所示、如表xxx所示等表述方式，并且希望能定位到相关表格或图片时，就需要采用文内交叉引用的方法 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:2","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"公式 当我们用MathType编辑公式，粘贴至word时，往往会影响word的格式，主要体现在行与行之间的距离变大。 对应的解决方案便：在对应的段落内，右键选择“段落”，取消勾选下图中所示内容 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:3","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"代码高亮 涉及到代码的话，为了美观，往往希望word中代码也实现高亮 推荐 planetB 暂时统计了这几个tips，欢迎交流 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:4","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"关于爬虫的文章，之前写过两篇： 爬取拉勾网数据分析职位相关的数据 基于Scrapy框架爬取京东评论数据 京东那个其实也是为了写论文，虽说拿着那篇参加了个会议，但我觉得诟病比较多。 如今，为了毕业论文，又重操旧业了 这次选择的对象是苏宁易购手机评论数据 收购家乐福中国事件吸引了我的眼球 整体的逻辑也比较简单，没有很复杂的反爬技术，直接上流程图吧 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:0:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"提取url的共性 我觉得像我这种低端爬虫的基本思想就是所见即所得，代替人工的复制、粘贴 我们在网络上见到的东西，都会有个url与之对应 通过url给服务器发送请求 就像 https://www.baidu.com/ 服务器返回相应的数据，浏览器解析这些数据，最后就是大家看到的样子 爬虫也是这个逻辑 所以写代码之前需要分析苏宁易购评论数据的url 苏宁评论数据是动态加载的，需要抓包分析 寻找url的异同点，比如不同商品、不同页面之间url的差异，哪些是变的、哪些是不变的。 一顿操作之后，发现变动的主要是三部分： 商品ID 店铺ID Cluster_ID 商品、店铺ID还好理解，你这个东西是什么、在哪家卖的，不能把A家商品的评论数据放到B家商品那儿 但这个 Cluster_ID 是什么 问就是不知道 但还是要清楚这玩意儿可以从哪儿得到，不然评论数据的url也不知道呀 有幸结识了位美团的前端大佬,在大佬的帮助下，找到了Cluster_ID 结论是，在商品详情页的源代码中有这么个东西 所以在最终确定评论数据的URL之前，需要通过解析商品详情页的数据，获取 Cluster_ID def get_clusterId(product_id,ua,shop_id=\"0000000000\"): ''' 解析商品详情页url 得到clusterId ''' url = \"https://product.suning.com/{}/{}.html\".format(shop_id,product_id) header = {\"Referer\":\"https://search.suning.com/%E6%89%8B%E6%9C%BA/#second-filter\",\"User-Agent\": ua} response = requests.get(url, headers=header) html = response.text soup = bs(html,\"html.parser\") t = soup.select(\"head script\")[0] tstr = t.get_text() cluster_id = re.search(r\"clusterId\\\":\\\"(\\d*).*?\\\"\",tstr).group(1) # string return cluster_id shop_id=0000000000 表示店铺是苏宁自营 正如流程图中所示，通过商品、店铺ID，确定商品详情页的url，从服务器返回的数据中找到 Cluster_ID 其实到这，就已经结束了 后续的操作就是获取并解析json文件 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:1:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"json文件 根据url请求服务器，让其返回评论数据的json文件 def again_content(cluster_id,product_id,page,ua,shopid=\"0000000000\"): product_id_0 = \"0\"*(18-len(str(product_id))) + str(product_id) url = \"https://review.suning.com/ajax/cluster_review_lists/cluster-{}-{}-{}-again-{}-default-10-----reviewList.htm?callback=reviewList\".format(cluster_id,product_id_0,shopid,page) header = {\"User-Agent\": ua, \"Referer\":\"https://product.suning.com/{}/{}.html\".format(shopid,product_id), \"Host\": \"review.suning.com\", \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate, br\", \"Accept-Language\": \"zh-CN,zh;q=0.9,zh-TW;q=0.8,en;q=0.7\"} response = requests.get(url, headers=header) html = response.text text = json.loads(html.lstrip(\"reviewList(\").rstrip(\")\") ) reviews = text[\"commodityReviews\"] print(text[\"returnMsg\"],product_id,page) return reviews 根据json文件找到想要的数据，根据key-value的对应形式，慢慢获取就可以了,如 add_reviewID = reviews[\"againReview\"][\"againId\"] reviewId = reviews[\"commodityReviewId\"] # 之后要利用这个匹配 useful_vote first_review = reviews[\"content\"] first_review_pt = reviews[\"publishTime\"] add_review = reviews[\"againReview\"][\"againContent\"] add_review_pt = reviews[\"againReview\"][\"publishTime\"] diff_first_add_pt = reviews[\"againReview\"][\"publishTimeStr\"] qualityStar = reviews[\"qualityStar\"] first_pic = reviews[\"imgCnt\"] 最后保存相关数据即可 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:2:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"其他细节 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"循环次数 因为我是 FOR 循环的形式获取评论数据的，并且有些商品的追评数据比较少。为了避免无效的循环，所以我还额外通过追评总数计算了页面数，以决定循环次数。最多展示50页 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:1","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"反反爬 这里面学问比较多，对效率要求不高的话，可以设置停留时间，休息一会儿再请求服务器 模拟随机的UserAgent，就像是换个浏览器 这个时候就要安利下fake_useragent import time import numpy as np from fake_useragent import UserAgent # 随机休息3-10秒 time.sleep(np.random.randint(3,10)) # useragent ua = UserAgent() ua.random 其实，最好的方法是随机更换IP地址 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:2","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"增强稳健性 为了保证代码的正常运行，预判会报错的地方，采取相应的方式 爬虫主要是以放服务器拒绝请求 我的方式比较粗暴，歇息时间长一点继续工作 try: clusterId = get_clusterId(product_id,ua.random) except: time.sleep(np.random.randint(5,15)) continue ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:3","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["数据分析","写作"],"content":"Bootstrapping基本思想 之前几篇关于 中介效应 有调节的中介效应模型 文章中都有提到 Bootstrapping 这篇就简单的记录Bootstrapping 的基本思想 平时在做分析的时候，总体的数据往往是比较难获取的 所以，我们会基于抽样理论，从总体中抽取具有代表性的样本去估计总体的信息 比如用样本均值 $\\bar X$ 估计总体均值 $\\mu$ ；样本方差 $S^2$ 估计总体方差 $\\sigma^2$ 等等 或者用区间估计的方法给出总体的均值的置信区间 本文以样本均值估计总体均值为例，探究Bootstrap方法的实现方式 Bootstrapping1 是一种重采样的方法（resampling method） 之前有提到，从总体中进行抽样，得到样本数据， $x_1,x_2,…,x_n$ 而Bootstrapping呢，从得到的样本中再进行有放回的抽样 如果抽样个数等于样本数据个数（$n$），则称其为 Bootstrap Sample $$ S_i^*={x_{i1}^*,x_{i2}^*,…,x_{in}^*} $$ 以这种方式不断的对样本进行重采样，就会得到 bootstrap samples，$S^*$ $$ S^*={S_{1}^*,S_{2}^*,…,S_{R}^*} $$ 其中，$R$ 一般要大于等于1000 $R$ 便是对应着 number of bootstrap samples 的选择 Bootstrapping 估计样本均值就是先分别计算 $R$ 个Bootstrap Sample的均值，最后再计算 $R$ 个均值的均值 Python代码示例 import numpy as np # 假设总体为X X = np.random.normal(size=10000000) # 抽样 n = 1000 x_i = np.random.permutation(X)[:n] # Bootstrapping估计 R = 5000 boot_samples = [x_i[np.random.randint(0,n,n)].mean() for _ in range(R)] boot_mean = np.sum(boot_samples) / R 当然，也可以给出均值的 Bootstrap置信区间 需要先将每个Bootstrap Sample得到的均值从小到大排序 进而计算 $\\alpha /2$ 及 $(1- \\alpha/2)$ 分位数作为区间的上下限 s_sorted = np.sort(boot_samples) alpha = 0.05 s_sorted[[round(R*alpha/2), round(R*(1-alpha/2))]] ","date":"2020-03-30","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/:0:0","tags":["论文专题"],"title":"【论文专题】Bootstrapping","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/"},{"categories":["数据分析","写作"],"content":"参考资料 Brad Efron: http://www.jstor.org/discover/10.2307/2958830?uid=3739568\u0026uid=2\u0026uid=4\u0026uid=3739256\u0026sid=21102342537691 ↩︎ ","date":"2020-03-30","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/:1:0","tags":["论文专题"],"title":"【论文专题】Bootstrapping","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/"},{"categories":["数据分析","写作"],"content":"在营销、管理领域，发现 $X$ 会影响 $Y$ 后，还会进一步的琢磨是如何影响，回答一个关于 HOW 的问题。 我觉得是沿用了心理学的研究方法，这里就不谈流派了，毕业要紧 所以憋论文的时候除了找到变量 $X$、$Y$，往往还要憋个变量 $M$ 出来，这个变量 $M$ 就用来回答 $X$ 如何影响 $Y$，$M$ 被称为中介变量（Mediator Variable） 为了得到因果关系，之后往往会设计实验，以问卷的形式收集数据，最后用统计学的方法一顿操作验证 $X$ 是否会影响 $M$ 进而影响 $Y$ 这篇就是记录如何用 PROCESS1 完成中介效应的分析，以简单的中介效应模型为例。 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"1 理论部分 上图的理论模型用数学数学语言可写为 $$ M = i_M+aX+e_M $$ $$ Y = i_Y + c^,X + bM + e_Y $$ 正如下图所示 直接效应 $c^,$ 表示 $X$ 影响 $Y$ 的直接效应 $$ c^,=[\\widehat{Y}|(X=x+1,M=m)] - [\\widehat{Y}|(X=x,M=m)] $$ $c^,$ 可解释为：在变量 $M$ 保持不变的情况下，自变量 $X$ 增加一个单位，因变量 $Y$ 均值变化 $c^,$ 。 间接效应（indirect effect） 将公式$\\ref{M}$ 代入公式$\\ref{Y}$，可发现系数 $a \\times b$ 便是估计中介效应的，所以中介效应是否存在，就需要关注系数 $ab$ 的情况了 PROCESS也是采用最小二乘法得到系数的 但PROCESS特别的地方在于，他是采用 Bootstrap Confidence Interval 的方法对检验以下假设 $$ H0: ab=0 $$ 「MARK」之后再另起灶炉简单扯一点Bootstrapping 用PROCESS做中介效应分析，结果的最后会给出间接效应的 95% bootstrap confidence interval 如果区间内不包含0，则说明中介效应成立 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"2 实际操作 步骤如下： SPSS加载 process的语法文件 使用过程中可以自主编写语法、也可以菜单式的操作 变量和数据字段一一对应 基于理论研究模型，选择PROCESS的模型（本例对应的是model 4） 我感觉这也算是PROCESS的一个弊端吧，灵活性不高。 其他可选 number of bootstrap samples 一般是5000，数据量小的时候可往大了选 选项(options) 标准化、中心化、产生画图的数据 调节效应分析中，spotlight analysis 选择16th、84th分位数表示低高，还是选择 均值±标准差 表示低、高 多分类变量的处理 如果是二分类（dichotomous）就不管 因为多分类变量(假设4类)的1，2，3，4并没有意义，只是表示不同的类别而已，一般通过indicator coding（也叫dummy coding）的方式将变量转为dummy variable ，用 $4-1=3$ 个字段表示 一顿操作之后，便可得到如下结果 正如理论部分所说，是否存在中介效应， 就看结果中 Indirect effect(s) of X on Y 的部分 Bootstrap置信区间不包含0，则说明中介效应存在。 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"参考资料 Hayes A F: Introduction_to_Mediation_Moderation_and_Conditional_Process_Analysis_A_Regression_Based[M].2ed.2018 ↩︎ ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"}]