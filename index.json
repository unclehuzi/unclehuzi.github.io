[{"categories":["生活"],"content":"最近在撸 X战警系列的电影，从故事的时间线维度出发，整理现有影片的观影顺序 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:0:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: First Class(2011) 万磁王（Erik）小时候在奥斯维辛集中营因愤怒等情绪展现出了控制铁制品的超能力，被纳粹Klaus Schmidt盯上了，一顿蹂躏，甚至还当着Erik面枪杀他的母亲 时间到了1962年，也是故事的大背景——古巴导弹危机。之前的纳粹Klaus Schmidt，因为具备吸收能量的超能力，老的没那么快，之后叫 Shaw（Sebastian Shaw） Erik也各种展开复仇。 同时这部X战警也介绍了后续变种人中两大理念的领导者，Professor X（Charles） 和 Erik（“万磁王”）早期的爱恨情仇。以及“魔形女” Raven，Hank 四人之间的你侬我侬。 期间也闪现了Professor X（Charles） 和 Erik（“万磁王”）两人拉 James Howlett（即之后的“金刚狼”）入伙的镜头 Mark 这时候（1962年）越南战争还没结束。 第一战 即是变种人和普通人类之间的第一战，也是Professor X（Charles） 和 Erik（“万磁王”）之间的第一战 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:1:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men Origins: Wolverine (2009) / 金刚狼1 金刚狼（James Howlett）小时候一个不小心干掉了生父（Thomas Logan），之后和他的大兄弟（Victor Creed）狂奔，共同经历了美国内战，第一、二次世界大战，越南战争。 时间点 越南战争结束已是 1975 年，这期间已经过了121年，变种人的故事多多少少也会夹杂着同步发生 之后他俩加入了 Stryker 的变种人团队 X 时间来到了 6 年后，那么至少是1981年之后了 感觉后面的矛盾基本都由 Stryker 挑起，金刚狼那钢铁般的爪子也是他给整出来的 这部最后还有些体现时间点的东西： Professor X（Charles） 已经坐上轮椅且头发没了 眼睛喷火的大哥（Scott）还小 Deadpool 没完全GG ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:2:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: Days of Future Past ｜ 逆转未来（2014） ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:3:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men: Apocalypse | 天启（2016） Scott 的课堂上，老师讲授着，变种人大规模出现在公众视野是在1973年的巴黎和平协约上 balabala 这集主要是讲如何干掉 传说中第一个变种人：En Sabah Nur 在此期间出现的人物，包括但不限于 风暴女（Ororo Moonroe，还没好好跟着 X-Professor） Caliban，收集情报 夜行者 Kurt Wagner Jean Grey 这里最牛逼的当属 Jean（琴），最后干掉那个伪神（En Sabah Nur）时并没有太多笔墨，感觉完全就是两个世界的人，直接降 n 维打击，了结了所谓的Boss。可谓是 Jean 才是大大大Boss啊。 这里有两个点🉑️ 先 「Mark」 Jean 第一次与 金刚狼相遇，并帮他找到了名字 Logan。名字嘛，和“我是谁” 这种哲学性十足的问题简直不是个数量级的 X-Professor 的头发没了。挺难的：“第一站”之后难以直立行走，现在又头秃了，不是以前随意撩妹的帅教授了 最后关于时间点上来看还是有些瑕疵的，比如在“X-Men Origins: Wolverine” 的时候，Scott还是个孩子，那时至少是1981年之后，但这里已经是1983年了 不过本文并不是聚焦于此～ ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:4:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"X-Men（2000） 电影中似乎未提及具体时间，故事发生在1944年之后，not-too-distant future，但看着人物状态估计离X-Professor头秃的时候也有20年了，即2000年左右 X-Professor 和 万磁王（Erik）都老了，且之前X教授收的几个小弟（琴、暴风女、Scott）都具备一定的单兵作战能力 关于时间线方面，此时金刚狼还没找到当年实验基地的（An abandoned military installation in Canada）的记忆，即 X-Men Origins: Wolverine (2009) 讲述的故事。 ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:5:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["生活"],"content":"Reference https://rddiy.com/chuangyisheji/shijue/ygqje.html ↩︎ ","date":"2021-11-19","objectID":"/2021/11/life-movie-x-men/:6:0","tags":["电影"],"title":"X战警系列观影顺序","uri":"/2021/11/life-movie-x-men/"},{"categories":["数据分析"],"content":"Hi PySpark，初次见面，别来无恙 PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. 首先，我是这么来看PySpark的：有一波人会Python但不会Java，那就搞个接口让会Python的小伙伴享受Spark分布式环境带来的快感，更好的分析大数据。 那么对于“面向问题编程”的从业人员来说PySpark的作用就很明显了。当觉得现有的分析工具很慢时可以考虑下PySpark，当然这里是基于Spark环境。换句话说，“快”是分布式环境带来的快感之一。 引入 PySpark 后，分析工作大致流程就变成了这样 👇 其实，整体还是“箱子模型” 📦 ，“喂”数据 =\u003e 处理、计算模块 =\u003e 结果 所以，应用层角度来看 PySpark 也就简单了： 如何读取数据？ 如何处理、计算得到自己想要的结果，即“面向问题编程” 如何处理结果？要保存到哪儿？ ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:0:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"如何读取数据 这个往往取决于数据在哪儿，譬如有些数据是以csv格式保存，有些是在数据库… 总之都是为了 Loading data onto Spark RDDs，享受分布式的快感 实际操作可以基于具体情况在网上检索相应的解决方案，如 pyspark read hive table # A example from https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html from os.path import abspath from pyspark.sql import SparkSession from pyspark.sql import Row # warehouse_location points to the default location for managed databases and tables warehouse_location = abspath('spark-warehouse') spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL Hive integration example\") \\ .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() # spark is an existing SparkSession spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\") spark.sql(\"LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src\") # The results of SQL queries are themselves DataFrames and support all normal functions. sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key \u003c 10 ORDER BY key\") # # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the \"License\"); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # \"\"\" An interactive shell. This file is designed to be launched as a PYTHONSTARTUP script. \"\"\" import atexit import os import platform import warnings import py4j from pyspark import SparkConf from pyspark.context import SparkContext from pyspark.sql import SparkSession, SQLContext if os.environ.get(\"SPARK_EXECUTOR_URI\"): SparkContext.setSystemProperty(\"spark.executor.uri\", os.environ[\"SPARK_EXECUTOR_URI\"]) SparkContext._ensure_initialized() try: # Try to access HiveConf, it will raise exception if Hive is not added conf = SparkConf() if conf.get('spark.sql.catalogImplementation', 'hive').lower() == 'hive': SparkContext._jvm.org.apache.hadoop.hive.conf.HiveConf() spark = SparkSession.builder\\ .enableHiveSupport()\\ .getOrCreate() else: spark = SparkSession.builder.getOrCreate() except py4j.protocol.Py4JError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() except TypeError: if conf.get('spark.sql.catalogImplementation', '').lower() == 'hive': warnings.warn(\"Fall back to non-hive support because failing to access HiveConf, \" \"please make sure you build spark with hive\") spark = SparkSession.builder.getOrCreate() sc = spark.sparkContext sql = spark.sql atexit.register(lambda: sc.stop()) # for compatibility sqlContext = spark._wrapped sqlCtx = sqlContext print(\"\"\"Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version %s/_/ \"\"\" % sc.version) print(\"Using Python version %s(%s, %s)\" % ( platform.python_version(), platform.python_build()[0], platform.python_build()[1])) print(\"SparkSession available as 'spark'.\") # The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP, # which allows us to execute the user's PYTHONSTARTUP file: _pythonstartup = os.environ.get('OLD_PYTHONSTARTUP') if _pythonstartup and os.path.isfile(_pythonstartup): with open(_pythonstartup) as f: code = compile(f.read(), _pythonstartup, 'exec') exec(code) ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:1:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"数据处理、计算 读取数据得到 Spark DataFrame 后，可以直接对此进行操作，除了常见的业务分析还有机器学习模块（MLlib） raw_data = sc.textFile(\"./kddcup.data.gz\") ## Comma-Separated Value csv = raw_data.map(lambda x: x.split(\",\")) metrics = csv.map(lambda x: [x[0], x[4], x[5]]) from pyspark.mllib.stat import Statistics Statistics.corr(metrics, method=\"spearman\") Statistics.corr(metrics, method=\"pearson\") 值得一提的是，Spark DataFrame to pandas DataFrame 可以用 toPandas() 方法，同时参数方面设置 spark.sql.execution.arrow.enabled=true 能提高效率 # A example from https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas import numpy as np import pandas as pd # Enable Arrow-based columnar data transfers spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Generate a pandas DataFrame pdf = pd.DataFrame(np.random.rand(100, 3)) # Create a Spark DataFrame from a pandas DataFrame using Arrow df = spark.createDataFrame(pdf) # Convert the Spark DataFrame back to a pandas DataFrame using Arrow result_pdf = df.select(\"*\").toPandas() 这部分再Mark一个关于 collect() 的小点，总之数据量比较大的时候就不要用这个方法。 The collect() function returns a list that contains all the elements in this RDD, and should only be used if the resulting array is expected to be ==small==, as all the data is loaded in a driver’s memory, in which case we lose the benefits of distributing the data around a cluster of Spark instances. ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:2:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"如何处理结果 处理、计算后的结果往往会再一次的落库，这个时候同 “数据读取” 的部分，🉑️ 根据具体情况进行检索。 以落到 hive 表为例，截止到目前整理的，大致有两种方法。 首先确保数据为 Spark DataFrame 状态（可以通过 spark.createDataFrame(df) 的方法将 pandas DataFrame 转为 Spark DataFrame） spark_df.write.mode(\"overwrite\").format(\"hive\").saveAsTable(\"dbName.tableName\") # 注意是 overwrite 或者 spark_df.createOrReplaceTempView(\"myTempTableName\") spark.sql(\"drop table if exists dbName.tableName\") spark.sql(\"create table dbName.tableName as select * from myTempTableName\") ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:3:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"总结 最近工作中遇到了 PySpark 的使用，在此从应用层小白视角通过 📦 “箱子模型”（Input =\u003e Box =\u003e Output） 简单记录大致的使用流程，方便于新手～ ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:4:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"Reference 360数科深圳数据组 Rudy Lai and Bartłomiej Potaczek.《Hands On Big Data Analytics With PySpark》 https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/spark-pandas http://spark.apache.org/docs/latest/api/python/index.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://stackoverflow.com/questions/30664008/how-to-save-dataframe-directly-to-hive https://bitbucket.org/cli14020/spark-cache/src/master/python/pyspark/shell.py ","date":"2021-11-10","objectID":"/2021/11/da-pyspark-beginning/:5:0","tags":["PySpark"],"title":"PySpark之应用层小白视角","uri":"/2021/11/da-pyspark-beginning/"},{"categories":["数据分析"],"content":"汇总一些【因果推断】方面的学习资料 最近看了下《原因与结果的经济学》并且结合硕士期间关于相关性与因果的思考，感觉这个 因果推断（causal inference）似乎挺有意思的。 基于数据分析的特性，我把目前因果推断方面的研究分为四大类 -- 聚焦某领域 未聚焦 理论 基于某领域的理论研究（如心理学因果关系的研究方法） 纯理论研究（这种常见于“扛把子”引领一个方向） 应用 基于某领域的应用研究（如业界的一些策略评估） 类似咨询 常见（或高产）的应该是“基于某领域的理论研究” 和 “基于某领域的应用研究”，至于第四种（“未聚焦领域的应用型研究”）因为商业性的问题应该较难找到公开案例。 后续也打算站在小白/门外汉的角度深入了解下，记录下遇到的学习资料。（持续更新…） ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:0:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"网文 ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"博客 Emre Kiciman @Microsoft Amit Sharma @Microsoft 二位似乎一起搞了个网站：Getting Started with Causal Inference，里面有正在写的书：Causal Reasoning: Fundamentals and Machine Learning Applications 以及一些课程之类的 Yishi Li @Tencent 统计之都 Brady Neal，有篇从需求出发选择读物的文章 ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:1","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"知乎 因果推断会是下一个AI热潮吗？ ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:2","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"Github amit-sharma / causal-inference-tutorial Paper_CausalInference_abtest ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:1:3","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"书籍 《原因与结果的经济学》 《别拿相关当因果！因果关系简易入门》（Why: A Guide to Finding and Using Causes） JUDEA PEARL. 《为什么》（THE BOOK OF WHY: THE NEW SCIENCE OF CAUSE AND EFFECT） JUDEA PEARL. Causality: Models, Reasoning, and Inference JUDEA PEARL. Causal Inference in Statistics: A Primer Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction Brady Neal. Introduction to Causal Inference Hernán MA, Robins JM. Causal Inference: What If，以及书中对应的Python-code ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:2:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["数据分析"],"content":"工具包 Uber-Causal ML: A Python package that provides a suite of uplift modeling and causal inference methods using machine learning. Microsoft-EconML: A Python package for estimating heterogeneous treatment effects from observational data via machine learning. Microsoft-DoWhy: A Python library that aims to spark causal thinking and analysis. CausalDiscoveryToolbox: A package for causal inference in graphs and in the pairwise settings ","date":"2021-10-09","objectID":"/2021/10/causal-resources/:3:0","tags":["因果推断"],"title":"因果推断补给站","uri":"/2021/10/causal-resources/"},{"categories":["FEM","读书笔记"],"content":"为什么要讲究因果？因为只有因果才能决定未来 可能在相关性盛行的大数据时代，因果关系也成为了一种稀缺 但科学研究一直提倡的是因果🤔 可以从数据中的规律出发，找寻因果关系（相关性 $\\Rightarrow$ 因果） 与此同时，经济学中还有个流派——从公理出发，经过一顿操作（逻辑推演），得到相应的结论。 或许这是“研究范式”的不同吧。按照邓小平爷爷的“猫论”，研究成果能造福人类就好。 本文是《原因与结果的经济学》的读书笔记，全书主要分为两大部分：1是提出因果推理并强调“反事实”是其必经之路；2是例举一些构建“反事实”的方式方法 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:0:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"因果推理 两个变量的关系是否真的是因果关系？解答这个问题所需的思维方法便是“因果推理”。 而判断因果关系有三个要点： 是否“纯属巧合”； 是否存在“第三变量” 是否存在“逆向因果关系” 备注 两个变量之间为因果关系时才能画如上图中的实线指向箭头，原因指向结果 而推翻以上三点的方法便是 对现实和“反事实”进行对比 反事实是指对过去未曾发生的事实所做的假设，例如“如果当时没有……，那么……”。我们将现实中实际发生的事称为“事实”，所以将设想的与现实完全相反的情况称为“反事实”。 这方面比较好操作的便是自然科学领域的各种实验了，比如这根试管加 xxx，另一根试管不加 xxx，观察两者之间的差异，验证假设之类的 但到了人文社科领域，要想像自然科学领域做实验，可谓是 “噫！吁嚱…” 所以作为因果科普文的《原因与结果的经济学》所介绍的方法也是有很多被challenge的地方。 后面的章节便是按照“证据等级”排序介绍了相应构建“反事实”的方法，与事实进行对比，进而判断因果关系 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:1:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"如何构建“反事实” 说到这个不禁想起硕士导师和我说过的话，只有实验才能得因果关系 至于构造“反事实”的方式方法，书中也简单罗列了几点 但不得不说，人文社科领域要想完全的像自然科学的实验那样严谨是比较难的，毕竟有时候个体之间也很难保持独立，即互相之间是会有影响的。 所以呢，CB（consumer behavior）以及心理学相关领域在评判学术文章时，除了idea 之外，还会看实验设计的是否巧妙。 此时针对此次实验研究（问卷形式）收集的数据便是“一手数据”，而非此次实验研究目的收集的那种便是“二手数据”。比如研究涉及电商评论，现有电商平台的评论数据对于我们的研究而言便是二手数据。 后续采取相应的分析方法常见有两种：一种是基于线性回归的 Conditional Process Analysis；另一种就是结构方程。可根据数据形式灵活采取相应的方法。 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:2:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"业界的增益 20世纪末美国陆陆续续将统计模型引入信贷领域风控业务1，贯彻“数据驱动”的理念，一直沿袭至今。从早些年基于 logistics回归的评分卡到现在的“GBDT”树类模型，但均未涉及因果。 在目前中国政府进一步压缩借贷利率上限的背景下，对风控业务而言便需要更加精细化的管理，在基于相关性搜寻风控策略的基础上进一步探究产生信用风险的因果关系，在总用户被压缩的情况下，在风险可承受范围内，进一步提高“进水口处”的“进水量”（/批核率/通过率）。 所以，在工具方面，是不是可以尝试或探究下“因果推断” 🤔 ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:3:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM","读书笔记"],"content":"Reference http://www.sinotf.com/GB/consumerfinance/2018-01-16/0MMDAwMDI5OTc0Mw.html ↩︎ ","date":"2021-10-05","objectID":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/:4:0","tags":["因果推断"],"title":"相关性能预测未来，但只有因果能决定未来","uri":"/2021/10/notes-%E5%8E%9F%E5%9B%A0%E4%B8%8E%E7%BB%93%E6%9E%9C%E7%9A%84%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"categories":["FEM"],"content":"一些观念被人们相信，是因为它们本身就是可被明证的事实；而另一些观念被人们相信，只是因为人们被这些观念反复的“洗脑”了而已。 最终，洗脑代替了证据，让人们普遍接受了这些“事实”。 当许多广为认知的观念接受事实与逻辑的检验，你会发现有的观念像纸牌屋一样不堪一击；有的观念看似真理，却只是一些思维谬误的产物。 经济政策中的谬误层出不穷，而且影响着社会的方方面面。小到住房，大到国际贸易，都是如此。 这些政策造成的灾难性后果往往要在好几年后才显现，但很少有人会对这些灾难的起因追根朔源。 即便一个政策出台后马上引发不良反应，很多人也不会去追究政策本身的缺陷，政策的倡导者还常常把这些不良后果嫁祸出去。他们甚至会辩解说如果没有他们推行的这套好政策，情况会更糟。（批注：因果反事实真的很难） 即使事实摆在眼前，谬误还是大行其道，其中的缘由各不相同。比如，政客为了避免影响自己的政治生涯，学者为了避免声望受损，扶贫活动的公益领袖为了避免内心的痛苦失落…… 没有人乐于承认自己的错误。 但在很多情况下，掩盖错误的代价是高昂的。这些代价让人们向现实低头，不管他们多么不愿意或多么痛苦。如果一个学生这次数学考试错了一道题，那下次考试之前他就必须把这道题解对；一家企业不会因为贯彻错误方针而任由企业不断亏损却不纠正。 简而言之，无论是出于实际需求还是理智，我们都有必要对谬误追根究底。 政府出台好的和错的经济政策都会影响无数人的生活，都会直接导致人们生活的更好或更差。这就是经济学研究为什么那么重要，而对谬误的研究不仅是单纯的学术行为。 经济谬误实在太多，无法一一列出。但我们可以概括出五大类常见的经济谬误进行分析： ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:0:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"1.零和式谬误 许多经济谬误都建立在一个错误的假设上，即交易是零和博弈，一方所得意味着另一方所失。 但是，如果不能让双方获利，交易就不会发生。这样浅显的道理，对于许多政策倡导者来说，不一定真的明白。 比如说，房租管制、最低工资，都是试图通过外部力量增加交易一方获得的收益。但是，当另一方无利可图或利益微薄时，交易就不再发生。 地产商不再建房，房东不再精心修缮房屋，导致更多的人租不到房子；老板不再雇佣新人，或者干脆将工厂搬到国外，导致本国人更多失业。 当你把交易误解成零和博弈，试图通过暴力强制交易一方减少收益，结果只会是交易量的减少。 零和博弈这种思维谬误造成的最坏的结果是：贫穷国家以为外来资本在剥削他们，为了避免被剥削，他们排斥和抵制外来资本。这种观念曾经被全世界广泛接受，造成的结果就是数千万人数代间都深陷贫困的泥沼。 许多国家后来纷纷抛弃了这种谬误，积极的拥抱资本和开展国际贸易。但在抛弃这种谬误之前的岁月里，无数人只能做无谓的牺牲，为一个没有事实根据的假设付出了巨大的代价。 可见，思维谬误造成的影响是极大的。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:1:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"2.组合式谬误 逻辑学家所说的“组合式”谬误是指部分正确即整体正确的观念。 一个棒球球迷在观众席上站起来看球场上的比赛可以看得清清楚楚，但如果所有球迷都站起来看，那最后的结果就是大家都看不好。许多经济政策都涉及组合式谬误。比如政治家们为某些特定群体、行业、利益集团代言，他们采取的政策看似造富全社会，其实不过是拆东墙补西墙的把戏。 例如，很多地方政府为了消灭贫民窟，“振兴社区”，大搞拆毁和重建，把贫民窟变成高档住宅和购物中心。人们总认为这些政府支出创造了就业、繁荣了经济，其实，不过是把纳税人的钱转移到了政府那里，导致纳税人无法用这些钱来消费和投资。政府宣称他们用繁荣社区取代了贫民窟，其实，那些贫民只不过是迁徙到了其他地方，去了其他的贫民窟。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:2:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"3.事后归因式谬误 事后归因式谬误不但最普遍，历史也最悠久。即对于两件相继发生的重大事件，人们会把第一件事当成第二件事的起因。 例如 1929 年股市崩盘之后，紧接着爆发了大萧条。所以一直以来人们都认为是股票市场的崩溃导致了整个经济的崩溃。然而，1987 年的股市崩盘，却迎来了持续 20 年的经济增长。 再例如世界上许多地方都开展过禁用 DDT 的运动。DDT 可以消灭传播疟疾的蚊虫，杀虫效果显著。但是使用 DDT 越多的地方，癌症发病率越高。以至于人们控诉 DDT 引发癌症，但这并非事实。禁止 DDT 让疟疾死灰复燃，夺走了全球上百万人的生命。 事后归因式谬误不仅仅是一个智力问题。人们总想着为好事邀功，把坏事归罪于人，这就是为什么会有那么多事后归因式谬误了。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:3:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"4.棋式谬误 在亚当·斯密的笔下，“教条理论家”是指自视甚高，认为“可以像摆弄棋盘上的棋子那样轻松驾驭一个国家”的一类人。这样的理论家仍然普遍存在，而且他们还影响着法律和政策的制定。 人类与棋子不同。他们有自己的喜好、价值观、计划和意愿。所有这些都可能会与“社会实验”的宏伟目标发生冲突。因为人不是棋子，不是任由摆布。 任何试图让人去机械的扮演某个宏伟计划的一部分的尝试都注定会失败，就像历史已经反复证明的那样。 很多教条理论家们并不死心，他们那种“如果一开始没有成功，那就多试几次”的想法是酿成更多灾祸的一味配方。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:4:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"5.开放式谬误 许多理想都忽略了一个最基本的经济事实：资源是稀缺的，而且有多种用途。 谁会反对安全、健康、保护环境呢？但是人们对这些的追求是无限的，政策的推动者也会促使人们进行无限制的追求。 大多人会乐意政府投资亿万美元用于癌症研究，但是同样的钱也可以用于许多其他疾病的研究。在倡导预防犯罪、更好的健康、更清洁的空气和水等理念时，人们确实遗漏了权衡取舍的概念，在政治正确下，我们的要求变得没有节制。 开放式没有上限的要求增加了财政预算，为权力扩张提供了便利，也助长了庞大的政府官僚机构。 法官在执行反垄断法时也经常陷入“开放式谬误”，即担忧某个企业正处于垄断“初期”。美国最高法院对布朗鞋业公司并购案的判决就是一个里程碑事件。法院禁止布朗鞋业收购肯尼连锁鞋店，因为后者占美国 1% 的鞋类市场，如果被收购，布朗鞋业有可能逐渐形成行业垄断，所以必须将这种垄断风险扼杀在摇篮中。 假设无限制的推断理论是正确的，那按照这种推理逻辑，如果黎明开始温度上升了 10 度，就意味着月底之前我们都会被高温烤成脆脆酥。 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:5:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"总结：经济学的正确思考 许多信条其实经不起推敲，但是如果没有人真正去审视它们，它们就会一直延续下去，尤其是当精明的倡导者通过利用人们的情感诉求来规避理性检验的时候。 一些普遍的谬误已经有上百年的历史了，它们在几个世纪前就已经被驳斥过，但是到了今天却能新瓶装旧酒，重新粉饰以新的形式适应当下的时代潮流。 基于这些谬误制定的经济政策常常会危害世界各国千百万人民的福祉，甚至一再造成毁灭性的后果。洞察这些谬误远不止磨砺心智这么简单，更清晰的去理解经济学，还可以为提升世界人民的生活水平，带来许多意想不到的机会。 从这儿转载 ","date":"2021-09-30","objectID":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/:6:0","tags":["经济学","奥派"],"title":"索维尔：最常见的五大经济谬误","uri":"/2021/09/fem-%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%94%E5%A4%A7%E7%BB%8F%E6%B5%8E%E8%B0%AC%E8%AF%AF/"},{"categories":["FEM"],"content":"常见的还款方式：等额本息、等额本金、等本等息 一个一个来盘 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:0:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等额本息 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"基本信息 等额本息还款法，也称定期付息。借款人每月按相等的金额偿还贷款本息，其中每月贷款利息按月初剩余贷款本金计算并逐月结清。（MBA智库百科） 每期还的钱是固定的，即每期 本金+利息 总额是固定的。但每期还的钱中 本金、利息 的占比是动态的 👇 上图中基本信息，假设 借款总额：300w 借款年限：30 年利率：5% ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:1","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"具体公式 假设借款总额 $p$ 元，分了 $n$ 期，每期的利率是 $r$，那么 每期应还款 $$ 每期应还款=\\frac{p \\times r \\times (1+r)^n}{(1+r)^n-1} $$ 第 i 期还款中本金部分 $$ 第i期还款中本金部分=p \\times r \\times \\frac{(1+r)^{(i-1)}}{(1+r)^n-1} $$ 第 i 期还款中利息部分 $$ 第i期还款中利息部分=\\frac{p \\times r}{(1+r)^n-1} \\times ((1+r)^n-(1+r)^{(i-1)}) $$ import pandas as pd class MRPI: ''' 等额本息 - 多种翻译 Matching the Repayment of Principal and interest / Fixed installment method / average capital plus interest method 计算 - 每月还款 - 本金、利息部分 - generate DataFrame ''' def __init__(self, p,R,N): ''' p: 贷款总额 R: 年化利率 N: 贷款年限 ''' self.p = p self.R = R self.N = N self.n = N * 12 self.r = R / 12 def pay_amt(self): ''' 每期总额（月供） ''' term_total = ( self.p * self.r * (1 + self.r)**self.n ) \\ / ( (1 + self.r)**self.n - 1 ) return round(term_total,2) def pay_part(self): ''' 月供中 利息，本金 部分 ''' interest,principal = [],[] for i in range(self.n): # 利息 term_interest = self.p * self.r * ( (1+self.r)**self.n - (1+self.r)**i ) \\ / ((1+self.r)**self.n-1) term_interest = round(term_interest,2) interest.append(term_interest) # 本金 term_principal = ( self.p * self.r * ( 1 + self.r )**i ) \\ / ( (1 + self.r)**self.n - 1 ) term_principal = round(term_principal,2) principal.append(term_principal) return interest,principal def get_detail(self): ''' convert to DataFrame ''' df_detail = pd.DataFrame({\"term\":[i for i in range(1,self.n + 1)] ,\"月还款\":[self.pay_amt()]*self.n ,\"本金部分\":self.pay_part()[1] ,\"利息部分\":self.pay_part()[0] }) df_total = pd.DataFrame({\"term\":[\"总计\"] ,\"月还款\":[self.pay_amt()*self.n] ,\"本金部分\":[self.p] ,\"利息部分\":[self.pay_amt()*self.n - self.p] }) df = df_detail.append(df_total) return df_detail,df.reset_index(drop=True) ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:1:2","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等额本金 等额本金还款法，也称利随本清、等本不等息还款法。借款人将本金分摊到每期，同时付清上一个交易日至本次还款日之间的利息。（MBA智库百科） 每期还的本金是固定的，随着剩余本金的减少，利息便也动态减少 👇 但前期还款总额较多 上图中基本信息，假设 借款总额：300w 借款年限：30 年利率：5% ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:2:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"具体公式 第 i 期应还款 $$ 第i期应还款=\\frac{p}{n} + (1- \\frac{i-1}{n}) \\times p \\times r $$ 第 i 期还款中本金部分 $$ 第i期还款中本金部分=\\frac{p}{n} $$ 第 i 期还款中利息部分 $$ 第i期还款中利息部分=(1- \\frac{i-1}{n}) \\times p \\times r $$ import pandas as pd class MPR: ''' 等额本金 - 多种翻译 Matching the Principal Repayment / Reducing installment method (Fixed Principal) / average capital method 计算 - 每月还款 - 本金、利息部分 - generate DataFrame ''' def __init__(self, p,R,N): ''' p: 贷款总额 R: 年化利率 N: 贷款年限 ''' self.p = p self.R = R self.N = N self.n = N * 12 self.r = R / 12 def pay_part(self): ''' 本金，每期总额（月供） 以及 利息部分 ''' # 本金 term_principal = self.p / self.n amt,interest = [],[] for i in range(self.n): # 每期利息 term_interest = ( 1 - i/self.n ) * self.p * self.r # 每期还款 term_amt = term_principal + term_interest term_amt = round(term_amt,2) amt.append(term_amt) term_interest = round(term_interest,2) interest.append(term_interest) return round(term_principal,2),amt,interest def get_detail(self): ''' convert to DataFrame ''' df_detail = pd.DataFrame({\"term\":[i for i in range(1,self.n + 1)] ,\"月还款\":self.pay_part()[1] ,\"本金部分\":[self.pay_part()[0]]* self.n ,\"利息部分\":self.pay_part()[2] }) df_total = pd.DataFrame({\"term\":[\"总计\"] ,\"月还款\":[self.p + (self.n+1)*self.p*(self.r/2)] ,\"本金部分\":[self.p] ,\"利息部分\":[self.p*(self.n+1)*(self.r/2)] }) df = df_detail.append(df_total) return df_detail,df.reset_index(drop=True) 俩还款方式每期还款金额如下图所示 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:2:1","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"等本等息 关于这个“等本等息”，常见于互金的现金贷业务、信用卡分期等 上面的等额本息、等额本金还款方式在计算每一期的利息时，也有按照占用借款时间的概念来计算，即还了的部分就不计算利息了 但 “等本等息”还款方式一开始就计算的死死的，每期还款中本金、利息都是固定的 比如，小明借了12w元，年利率是12%，分12期还。那么每期应还$11200=(\\frac{120000*(1+0.12)}{12})$ 还是基于 12w，年利率12%，分12期的情况，不同还款方式的情况如下所示 等额本金 等额本息 等本等息 1 11200 10661.85 11200 2 11100 10661.85 11200 3 11000 10661.85 11200 4 10900 10661.85 11200 5 10800 10661.85 11200 6 10700 10661.85 11200 7 10600 10661.85 11200 8 10500 10661.85 11200 9 10400 10661.85 11200 10 10300 10661.85 11200 11 10200 10661.85 11200 12 10100 10661.85 11200 总计 127800 127942.2 134400 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:3:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["FEM"],"content":"总结 本文罗列了等额本息、等额本金和等本等息三种还款方式的基本信息。但产品设计的角度思考较少 虽然表面上看起来等本等息还款方式很暴力，但我觉得每种还款方式背后都有金融产品设计者的考虑吧 银行不会雪中送炭，只会锦上添花。小额贷款等机构 、平台都是一样的，在风险可承受的情况下给予放款，毕竟这是门生意 但政府的监管，限制利率上限等政策，带来的结果便是，更多的人借不到钱 🤷‍♂️ 原本的毛细血管更加的“细”。 从奥派的角度来看“高利贷”等问题，可参考陈志武教授的这篇文章 ","date":"2021-09-13","objectID":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/:4:0","tags":["金融"],"title":"还款方式：等额本息-等额本金-等本等息","uri":"/2021/09/fem-%E8%BF%98%E6%AC%BE%E6%96%B9%E5%BC%8F/"},{"categories":["生活"],"content":"在疫情的情况下毕业了🎓，时隔一年2个月，我们终于是和导师成功约饭了 这次“会师”似乎没有在学校时候的拘谨了，和导师之间可谓是亦师亦友了 总体而言，感觉多聊聊还是挺好的，聊的几个点也挺有意思，在此做个记录好了。 可能是之前受章慧南教授的影响 每个人都有自己的兴趣点或是说研究方向，可以通过大白话的形式分享自己关注的部分，同时也能加深自己对该领域知识的理解 从个人而言，就可以通过这种方式 “多一只眼看世界”。就像TED，在那儿有各种各样的角度，分享着各种各样有趣的东西 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:0:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"回归人性 话虽如此，但我并未实际领悟到这四个字的深层含义，但也提供了一个思考的方向，很多时候有啥困惑的试着从人性的角度先去思考“为什么” 或许就能豁然开朗，但可能也不排除会以小人之心度君子之腹？🤔 hhhhh，没有什么十全十美的事儿，自己开心最重要，解释通了、开行了就好 工作之后，直接、间接学习到的一点也是 先思考为什么，再想其他的 比较典型的是之前我接手某件事儿时，总会吐槽这儿、吐槽那儿，但后来想想，这个方案放在当时的情景可能是个局部最优的方案了 比如会受到产品着急上线的压力；比如没有现在的新工具等等 总之，多一份为什么，少一份自以为是。 因为我发现自己在和上下游一起处理问题时，也会先着手解决当前的问题，达到局部最优，算是较为经济的一个方案吧🌞 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:1:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"先有蛋 倒不是指传统的 先有鸡、后有蛋的问题。而是指先能生蛋，在此基础上，得到更好的环境，以便产出更多的鸡蛋 可能这个比喻不恰当 但表达的意思主要是在某个圈子里得先有实力能崭露头角，方能升级进入到高阶水平，不断的打怪升级 回想硕士期间，我也是先写了篇小论文出来，才找各位老师们提意见，看看能不能带我一起玩～ 所以，还是得在某个领域提升自己 吃饭时也提出了“加点”方向和圈子内对不上怎么办，就像是你待在一个研究机器学习的圈子，而你在研究数据库 导师的第一反应也是换到“加点”方向的那个圈子呀～ 这个也是让我豁然开朗，对啊，是这个道理啊，我为什么要鸡同鸭讲，需要的是“琴箫合奏”、沧海一声笑 所以，先有蛋的那个蛋得是在鸡圈里呀，在猪、羊等胎生动物的圈里，除非遇上“伯乐”，知道且需要蛋。 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:2:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"身心健康 这一点虽然之前也有感悟，但一直未贯彻落实 身心健康是两个维度：身体和心理 《中国文化的深层结构》一书中也曾提及，国内传统文化及教育很少涉及“心理”，典型的是长辈说的最多的是注意身体之类的；常年在外回到家后往往会准备好吃的好喝的。 不过这也是，安身方可安心～ 只是说需要注意的是，“身”、“心”应齐头并进、双管齐下 这一点虽是老生常谈，但我个人并未很好的落实呀～ 🤦‍♂️ 外界是很难甚至没法改变的，我们能改变的或容易改变的是自己，所以 “修身，齐家，治国，平天下”，修身乃第一☝️位 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:3:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"最后 最后，恭喜我导今年在营销领域顶刊 Journal of Marketing Research 发表相关学术论文～ 希望大家一切顺利～🌞 ","date":"2021-09-07","objectID":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/:4:0","tags":["碎碎念","学术"],"title":"记毕业后第一次与导师的晚餐","uri":"/2021/09/life-%E4%B8%8E%E5%AF%BC%E5%B8%88%E7%9A%84%E6%99%9A%E9%A4%90/"},{"categories":["生活"],"content":"不知不觉，离开学校已经有1年2个月了。。。 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:0:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"目标 这一年以来真的是经历了许多乌龙事件。最大的干扰点有二：一是户口；二是工作岗位。 心态差的时候，只想说，或许我的运气在实习期间都用完了吧 😂 实习期间遇到的人太棒、太赞了，以至于现在一塌糊涂… 心态好的时候，时常告诉自己说，这些都是经历、是“入道”的素材。 有时候想想，确实也是这样啊。毕业论文的致谢中，我依然会由衷的感谢导师给予我的自由度，也曾感叹着自己“摸黑”的日子 但尽管如此，在学校的时候隐隐约约似乎能感受到有些大方向依然还是在那儿的，真的就像黑暗中的烛光，忽闪忽闪的 然而，现在的状态大方向处于飘忽不定的状态，甚至是太大、太遥远了，使得自己有时候丧失了一切动力，只想躺尸 就希望能什么都不用想、不用顾虑“面包”🍞，每天尽享时间的流逝，就像电视剧里的样子 小时候我爸打趣的和我说，你看电视剧里都不讲赚钱的问题呐 真的觉得“系统1” 能轻而易举的战胜“系统2”。 没有错，保持理性是很累的一件事儿，是会消耗大量能量以及脑细胞的。 所以推出要将某事成为一种习惯、形成chunk，让一切都显得那么的自然与丝滑 chunk 就像每天在站点等交通工具 或是倒车入库 至于怎样才能将某事成为一种习惯呐，暂时认为就需要和时间做朋友了吧～以及自身的兴趣或动力 这是不是又回到了“先有鸡，还是先有蛋”的问题呐。本想着让一切是那么的自然，而习惯又需要动力来促成 也有可能是我想错了。但似乎也不需要走极端吧。即一定程度上的习惯降低动力的依赖或损耗，两者相辅相成呀☯️ 基于我目前对“奥派”的理解，以上便是属于目标的问题。目前的小目标都是下一个目标的过程：既是终点也是起点。 目标有了又如何？不做不都是白瞎么 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:1:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"落地 首先，我认为做什么都是每个人的自由，每个人都有自己的偏好，基于偏好、机会成本做决策。 早之前在旭辉实习的时候遇到位大佬，在短暂的接触中，真的是由衷的钦佩他～也曾就类似的事儿向其请教过 背景 初窥地产研究岗，也关注了些公众号，看到相应的文章会收藏（比如现在的“浮窗”），但在收藏中躺了x天，都没打开过了 他说，因为你不认为这件事儿此刻对你而言是重要的，或者可能你压根就没这个需求。我会看是因为我的工作和这些相关，需要去了解这些讯息，了解…。 是的，我目前认为就是这样。这里面除了重要性之外，还有紧迫性的维度 这件事儿、这个目标当下对我是否重要？ 紧迫感、重要性的加持，我想大概率就能落地了吧。想想憋毕业论文的那段时间… 除非真的是无力回天…直接弃疗… 我们都知道身体的重要性，但并没有那种紧迫感。再极端点说，可能没和死神擦肩而过，对于重要性的感触也没那么的深刻。有些事儿真的很难感同身受… 所以对于这些，重要不紧急的目标，我该怎么办？或许这类事儿、目标真的不需要太多，让“系统2”和习惯共同加持吧，同时也需要一些“助推（Nudge）”的方式。真的需要踏出那一步o.o “助推”解释为那种轻轻用手肘碰下你，就能实现目标 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:2:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"WLB 感觉实际参加工作之后就没顺过，，，也是因为自己的决策问题吧，这也算是让自己直接学习了一波之前间接学习的经验：比较重大的事情要积极启用“系统2”来理性分析各种依赖关系！ 切片数据来看，我觉得可以将企业工作者分为能力、资源2个维度来看。 资源多 资源少 能力强 资源多-能力强 资源少-能力强 能力弱 资源多-能力弱 资源少-能力弱 某种程度上其实是不太合理，因为两个维度并非完全独立，也可能会存在互为因果的情况。比如直观理解，因为能力弱（强），所以资源少（多）。 但很多时候并非如此，资源是有限的 实际做决策的是个人，很多时候是由上而下的，分配资源的是各个节点上的人，各个节点上的人具体怎么分配的谁都不知道🤷‍♂️ 现实情况一样存在能力弱的人所拥有的资源多于能力强的。但这是不是也很难界定？因为溜须拍马、机遇也是种能力啊🤷‍♂️ 跟人扯上关系的都很复杂，所以我觉得人文社科领域是不存在类似物理世界的漂亮公式的 而且，实际工作中也并不需要多强的能力，企业越大，分工越明确、“通用件”越多、互换性越强。 互换性 工科应该都会提及“互换性技术”，带来的好处就是哪个零部件坏了，拆下来换个新的又能正常运转了，而且这个新的零部件也很容易获得，大大提高了生产效率。 最近刚好在欧神的文章中看到这么一段话👇 得想想办法怎么深度挖掘潜能🤔 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:3:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["生活"],"content":"愿望🙏 希望未来一切都好～ 目标能落地 找到深度挖掘潜能的路子 修身养性、由艺入道 ","date":"2021-09-05","objectID":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/:4:0","tags":["碎碎念"],"title":"如果真的什么都不用想","uri":"/2021/09/life-%E8%83%A1%E8%A8%80%E4%B9%B1%E8%AF%AD/"},{"categories":["机器学习"],"content":"RT，可视化决策树结果，直观感受决策流程 早之前可视化领域专家、学者提出之后要结合各种机器学习算法，制作相应可视化的图标以及工具。比如 TensorFlow 的可视化工具包—TensorBoard 针对结构化数据建模的算法中，树模型是较为常见的，之前 一直用sklearn自带的tree_plot()函数或Graphviz tree_plot 最近发现了一个可视化树模型结果的package1，画出来的图长这样👇 iris-TD-3-X 各个节点的分布情况也比较清楚，直观感受决策树的逻辑 更多代码示例可参考 这个 或 这个 图片格式 但目前图片格式只支持 svg 这里主要记录下安装说明 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装2 原文 Python 版本 \u003e= 3.6 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装 graphviz 确保 graphviz 是 通过 pip 的方式安装的。 如果有装 Anaconda，又不确定是不是通过 pip 安装的，就走遍流程👇 conda uninstall python-graphviz conda uninstall graphviz pip install graphviz 接下来以 Windows 为例， 下载 graphviz.msi 并更新Path环境变量 假设保存路径为 C:\\Program Files (x86)\\Graphviz2.38，则将 C:\\Program Files (x86)\\Graphviz2.38\\bin 添加至用户变量（Path） C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe 添加至系统变量（PATH） 可以通过 where dot 验证是否安装成功 (base) C:\\Users\\Terence Parr\u003ewhere dot C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:1","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"安装 dtreeviz 通过 pip 安装，Windows 可以直接打开 Anaconda Prompt pip install dtreeviz # install dtreeviz for sklearn pip install dtreeviz[xgboost] # install XGBoost related dependency pip install dtreeviz[pyspark] # install pyspark related dependency pip install dtreeviz[lightgbm] # install LightGBM related dependency 决策树 这里有酷炫的决策树介绍文档 ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:1:2","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["机器学习"],"content":"Reference https://explained.ai/decision-tree-viz/index.html ↩︎ https://github.com/parrt/dtreeviz#install ↩︎ ","date":"2021-08-19","objectID":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/:2:0","tags":["可视化"],"title":"可视化决策树结果","uri":"/2021/08/ml-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%86%B3%E7%AD%96%E6%A0%91/"},{"categories":["数据分析"],"content":"【Mark】通过正则表达式选择相应的列 在涉及子查询时，平时习惯直接把需要的列全写出来，也没想太多。 最近有小伙伴谈起正则选择需要的列，一顿操作后做个记录。建模取特征时，常见这种操作 Tip Spark 和 Hive 都是支持这种操作的 只是相应的设置不同 spark/hive set hive1 set hive.support.quoted.identifiers=none spark2 set spark.sql.parser.quotedRegexColumnNames=true 其中，正则表达式的写法可参考 JAVA regex 语法 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:0:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"例1，查询去除某几列的所有列 select 去除 user_no, cust_no 的所有列 select`(user_no|cust_no)?+.+`fromtable1 select 去除以 no 结尾的所有列 select`(.*no)?+.+`fromtable1 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:1:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"例2，查询符合某特征的所有列 select 以 no 结尾的所有列 select`.+no`fromtable1 ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:2:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"Reference https://community.cloudera.com/t5/Community-Articles/Spark-to-support-REGEX-column-specification-for-Hive-Queries/ta-p/316579 ↩︎ https://stackoverflow.com/questions/52526768/does-spark-sql-supports-hive-select-all-query-with-except-columns-using-regex-sp ↩︎ ","date":"2021-08-16","objectID":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/:3:0","tags":["SQL"],"title":"Spark、Hive QL-通过正则表达式选取需要的列","uri":"/2021/08/sql-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E9%80%89%E6%8B%A9%E5%88%97%E5%90%8D/"},{"categories":["数据分析"],"content":"类似Excel的数据透视表，分类聚合。也可以协助实现行转列，Pivoting “Wide” to “Long” Format 在统计分析时总会遇到分类汇总的场景，类似Excel的数据透视表。SQL中按照 case when 或 IF 的写法往往会显得臃肿，较为方便的便是通过 pivot 1实现，但 Hive 不支持😢 运行环境 以下基于 spark-2.4.5U3 及以上版本 ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:0:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"基本语法 PIVOT({aggregate_expression[ASaggregate_expression_alias]}[,...]FORcolumn_listIN(expression_list)) The PIVOT clause can be specified after the table name or subquery. ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:1:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"实际应用 假设有张存有各个地区、各个产品的月销量的表（sales_table），我们需要统计各个月份所有地区产品销量的加总，形如👇 selectmonth,'毛巾','肥皂'fromsales_tablepivot(sum(sales)forproductin('毛巾','肥皂')); ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:2:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"Reference https://spark.apache.org/docs/3.1.2/sql-ref-syntax-qry-select-pivot.html ↩︎ ","date":"2021-08-09","objectID":"/2021/08/sql-pivot/:3:0","tags":["SQL"],"title":"Spark SQL-Pivot","uri":"/2021/08/sql-pivot/"},{"categories":["数据分析"],"content":"已经2021年了，SQL条件函数已不局限于case when 了，针对常见场景已新增许多 conditional functions 汇总 Hive SQL 常用的条件函数，👇 ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:0:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"if(condition, value True, value FalseOrNull) This is the one of best Hive Conditional Function 类似Excel中的 if 函数 selectif(state='online','在线','离线')asstate; ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:1:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"case when then end case when 较为常见，可注意执行顺序问题：从上至下依次执行，直到满足条件则跳出 常见写法有两种，主要取决于是否单变量作判断 selectcasedayofweekwhen1then'星期一'when2then'星期二'endasweek;-- OR -- selectcasewhendayofweek=1then'星期一'whendayofweek=2then'星期二'endasweek; ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:2:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"isnull(a) 判断是否为null（数据库特有的一种数据类型），留意字符型 'null', 'NULL' selectisnull('');/*return false */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:3:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"isnotnull(a) 非null selectisnotnull('');/*return true */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:4:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"nullif(a,b) 如果 a=b 则返回null，否则返回a 等同于 case when a = b then null else a end selectnullif(1,1);/*return NULL */selectnullif(4,3);/*return 4 */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:5:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"nvl(arg1,arg2) 可以理解为用arg2的值替换 arg1 中值为 null 的部分 selectnvl(null,999);/*return 999 */selectnvl('null',999);/*return null */ ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:6:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["数据分析"],"content":"coalesce(value1,value2,…) 返回第一个非null的值 往往只想得到众多列中非 null 的值 selectcoalesce(null,7,8);/*return 7 */ null 涉及null的问题要留意数据库中的数据类型。譬如，某列数据格式为string，'null' 并不等同于 null ","date":"2021-07-31","objectID":"/2021/07/hql-conditional-functions/:7:0","tags":["SQL"],"title":"Hive SQL-条件函数汇总","uri":"/2021/07/hql-conditional-functions/"},{"categories":["写作"],"content":"将博客源文件托管在某个地方，换台电脑继续编辑。再加上触发机制更好，即一上传⏫就同步更新博客～ 在之前快速搭建个人博客的文章中有提到 将 public 文件夹下的文件推送至GitHub仓库 每次这么操作还是有点繁琐的，所以就想如果更新完hugo源文件，博客自动更新就好了。此外，换台电脑💻，依然正常操作就更更更好了～ 终于，是找到了GitHub的 Actions 中 Workflows 功能 将Hugo源文件维护在GitHub上，只要源文件的仓库更新，自动更新存有 public 文件的仓库，那么博客也就随之更新了。 在此记录下实现过程 👇 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:0:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"1. 生成SSH key （Windows 环境） 参考这个 通过git bash cd 至 .ssh 文件夹 cd ~/.ssh/ 如果提示 No such file or directory，可以手动的创建一个 .ssh文件夹，BY mkdir ~/.ssh 配置全局 name 和 email git config --global user.name \"你的用户名\" git config --global user.email \"你的公司或个人邮箱\" 生成 key ssh-keygen -t rsa -C \"你的公司或个人邮箱\" 连续按 3 次回车 最后得到俩文件： id_rsa 和 id_rsa.pub ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:1:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2. 创建并配置仓库 参考这个 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.1 配置hugo源文件的仓库 仓库名称不限，在此以 unclehuzi.github.io.source 为例 进入unclehuzi.github.io.source仓库，添加Secrets，名称为ACTIONS_DEPLOY_KEY，将 id_rsa 文件的内容粘过去，得到内容如下所示 上传 hugo源文件 把 themes 主题文件夹中的 .git 文件删除 不然Github 会检测到是别的仓库，上传后文件夹是灰色的 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:1","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.2 配置 unclehuzi.github.io 仓库 仓库名称有讲究，得是这个 github_user_name.github.io 进入unclehuzi.github.io 仓库，添加Deploy keys ，名称不限制，将id_rsa.pub文件的内容粘过去。 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:2","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"2.3 配置工作流（Workflows） 进入unclehuzi.github.io.source仓库，创建 Actions 代码如下 👇 name:Deploy Hugo Site to Github Pages on Master Branchon:push:branches:- master # Attention 1jobs:build-deploy:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v2- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'0.83.1'extended:true# 使用扩展版# Attention 2- name:Buildrun:hugo#--minify- name:Deployuses:peaceiris/actions-gh-pages@v3with:deploy_key:${{ secrets.ACTIONS_DEPLOY_KEY }}# 这里的 ACTIONS_DEPLOY_KEY 则是上面设置 Private Key的变量名external_repository:unclehuzi/unclehuzi.github.io# Pages 远程仓库 publish_dir:./publickeep_files:false# remove existing filespublish_branch:master # deploying branch# Attention 3commit_message:${{ github.event.head_commit.message }} 备注 Attention 1 source 仓库的分支名称为 master Attention 2 hugo 的版本 Attention 3 unclehuzi.github.io 仓库的分支名称 ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:2:3","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"3. Finished 以后维护好source这个仓库就能实现 触发机制以自动更新blog 换个电脑 💻 继续写blog ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:3:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["写作"],"content":"Reference https://jinnzy.github.io/shi-yong-hugolai-da-jian-ge-ren-blog/#%E5%88%A9%E7%94%A8github-pages%E9%83%A8%E7%BD%B2blog https://www.jianshu.com/p/95262f5eba7a ","date":"2021-07-14","objectID":"/2021/07/20210714-workflows/:4:0","tags":["blog"],"title":"远程管理hugo源文件以自动更新blog","uri":"/2021/07/20210714-workflows/"},{"categories":["数据分析","机器学习"],"content":"RT，SQL计算多个变量的IV（Information Value） ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:0:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"背景 变量的预测能力往往可以通过IV值来判断，类似之前的 SQL计算PSI IV值也有经验区间供参考，以及可通过SQL完成指标的计算 Information Value Predictive Power \u003c 0.02 useless for prediction 0.02 - 0.1 weak predictor 0.1 - 0.3 medium predictor 0.3 - 0.5 strong predictor \u003e 0.5 suspicious or too good ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:1:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"计算公式 关于 IV 的详细介绍，可参考这篇文章 具体计算公式如下 $$IV=\\sum_{i=1}^{n}(\\frac{Bad_i}{Bad_T} - \\frac{Good_i}{Good_T}) \\times WOE_i$$ 其中， $$WOE_i=\\ln(\\frac{Bad_i}{Bad_T}) - \\ln(\\frac{Good_i}{Good_T})$$ 公式说明 Bad、Good即表示正负样本，风控场景有好、坏的称呼 $n$ 为分箱的个数 $Bad_i$, $Good_i$ 表示第i个箱子“坏”、“好”人数 $Bad_T$, $Good_T$ 表示“坏”、“好”总人数 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:2:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"计算样例 分箱方式：等频（缺失值单独划为一箱） score_group group_bad_i group_good_i woe_i iv_i r1 271 31882 0.218363 0.0054 r2 225 30572 0.074301 0.0006 r3 195 29107 -0.01969 0.0000 r4 188 28761 -0.04429 0.0002 r5 163 28400 -0.17435 0.0025 r6 182 27387 -0.02778 0.0001 r7 194 28058 0.01187 0.0000 r8 160 24564 -0.04782 0.0002 r9 158 29625 -0.24774 0.0052 r10 70 17302 -0.52404 0.0118 missing 327 36519 0.270413 0.0098 以上例子最终得到 $IV=\\sum_{i=1}^{11}(IV_i)=0.0358$ ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:3:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"SQL实现 准备好预测变量（$X$）和目标变量（$y$），score表形如 ym no x1 x2 x3 x4 y 202101 a1 617 481 773 671.68 1 202102 a2 585 585 522 600.56 0 202102 a3 617 548 677 635.68 1 202102 a4 647 null 765 655.63 0 202102 a5 596 478 656 635.3 0 202102 a6 636 618 595 630 0 202102 a7 714 572 842 644.28 0 202012 a8 null 495 720 628.79 0 202012 a9 636 618 595 426 0 202012 a10 557 562 null 589 1 基于此得到各个变量在不同月份的预测能力 这里依然涉及窗口函数的应用以及行列互转 窗口函数-聚合 窗口函数-排序 窗口函数的“窗口” 行转列、列转行 思路 类似PSI的计算思路，计算IV的整体思路依然参照公式，（等频）分箱后，基于数据的断点（Breakpoint Value）统计出每个箱子的好坏人数 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"1. 列转行 将score表进行列转行，变为key-value 键值对的形式 droptableifexistsscore_value;createtablescore_valueasselectno,ym,y,score,score_valuefrom(selectno,ym,x1,x2,x3,x4,yfromscore)lateralviewouterexplode(map('x1',x1,'x2',x2,'x3',x3,'x4',x4))tasscore,score_value; ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:1","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"2. 分箱统计好坏人数 这里采用的是 等频分箱 droptableifexistsscore_group_nums;createtablescore_group_numsasselectym,score,score_group,group_bad_i,group_good_i,sum(group_bad_i)over(partitionbyym,score)asgroup_bad_total,sum(group_good_i)over(partitionbyym,score)asgroup_good_totalfrom(selectym,score,score_group,count(casewheny=1thennoend)asgroup_bad_i,count(casewheny=0thennoend)asgroup_good_ifrom(selecta.*,casewhena.score_valueisnullora.score_valuein('','null','NULL')then'missing'whena.score_value\u003c=r.score_array[0]then'r1'whena.score_value\u003c=r.score_array[1]then'r2'whena.score_value\u003c=r.score_array[2]then'r3'whena.score_value\u003c=r.score_array[3]then'r4'whena.score_value\u003c=r.score_array[4]then'r5'whena.score_value\u003c=r.score_array[5]then'r6'whena.score_value\u003c=r.score_array[6]then'r7'whena.score_value\u003c=r.score_array[7]then'r8'whena.score_value\u003c=r.score_array[8]then'r9'whena.score_value\u003c=r.score_array[9]then'r10'endasscore_groupfromscore_valuealeftjoin(-- 等频分箱 10 bins selectym,score,percentile_approx(score_value,array(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),999999)asscore_arrayfromscore_valuegroupby1,2)ron(a.ym=r.ymanda.score=r.score))groupby1,2,3); ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:2","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"3. 计算IV 回顾下公式 $$IV=\\sum_{i=1}^{n}(\\frac{Bad_i}{Bad_T} - \\frac{Good_i}{Good_T}) \\times WOE_i$$ 其中， $$WOE_i=\\ln(\\frac{Bad_i}{Bad_T}) - \\ln(\\frac{Good_i}{Good_T})$$ selectym,score,sum(iv_i)asivfrom(selectym,score,score_group,(ln(group_bad_i/group_bad_total)-ln(group_good_i/group_good_total))*(group_bad_i/group_bad_total-group_good_i/group_good_total)asiv_ifromscore_group_nums)groupby1,2 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:4:3","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"总结 在非建模场景，只想大概看下（或监控）各变量的预测能力时，为省去导出数据用Python计算IV的麻烦，本文便以IV的计算公式出发详细记录SQL计算过程 ","date":"2021-07-08","objectID":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/:5:0","tags":["Model","SQL"],"title":"SQL计算多个变量的IV","uri":"/2021/07/20210708-sql%E8%AE%A1%E7%AE%97iv/"},{"categories":["数据分析","机器学习"],"content":"RT，SQL批量计算各个模型分的PSI，更方便的搭建模型分稳定性的监控，满足模型应用的充分条件 — 样本分布一致性 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:0:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"背景 应用模型的一大前提便是建模样本尽量和实际生产样本在分布上保持一致性，保证跨期层面的准确性 当模型分偏移到一定程度时，也该考虑迭代一版了 偏移程度可以用 PSI 这个指标来评价，而对于这个程度业界有个经验值1 psi PSI \u003c 0.1: no significant population change PSI \u003c 0.2: moderate population change PSI \u003e= 0.2: significant population change ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:1:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"计算公式 关于PSI的详细介绍，可参考我司模型大佬的这篇文章 具体计算公式如下 $$PSI=\\sum_{i=1}^{n}((Actual_i\\% - Expected_i\\%)\\times \\ln(\\frac{Actual_i\\%}{Expected_i\\%}))$$ 公式说明 n 为分箱的个数 $Actual_i\\%$ 表示第i个箱子的实际占比 $Expected_i\\%$ 表示第i个箱子的预期占比，（称其为比较的基准） ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:2:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"计算样例 以某一个模型分为例，计算跨期的psi bucket excepted_num except_rate actual_num actual_rate psi 1 26780 0.1010 31779 0.1359 0.01036 2 26355 0.0994 27439 0.1173 0.00298 3 26532 0.1000 26008 0.1112 0.00118 4 27416 0.1034 25816 0.1104 0.00046 5 26495 0.0999 24113 0.1031 0.00010 6 26588 0.1002 23146 0.0990 0.00002 7 25957 0.0979 21176 0.0905 0.00057 8 27530 0.1038 21442 0.0917 0.00150 9 25898 0.0976 18310 0.0783 0.00428 10 25710 0.0969 14682 0.0628 0.01484 以上例子最终得到 $PSI=\\sum_{i=1}^{10}(psi)=0.0362$ ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:3:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"SQL实现 我们希望得到的是从存有各个模型分的表（命名为 score）得到上图👆，score表样例如下 no week scoreA scoreB scoreC scoreD scoreE a1 9 617 481 773 671.68 561 a2 2 585 585 522 600.56 588 a3 16 617 548 677 635.68 563 a4 7 647 564 765 655.63 586 a5 12 596 478 656 635.3 586 a6 7 636 618 595 630 572 a7 22 714 572 842 644.28 563 a8 23 638 495 720 628.79 563 a9 3 636 618 595 426 526 a10 3 557 562 526 589 535 备注 一行表示该用户对应的各种模型分数，scoreA…scoreE 其中， week 表示第几周，这里以2021年第一周（[2021-01-04,2021-01-10]）的数据作为基准，即 excepted 这里会涉及之前一些文章的知识点 窗口函数-聚合 窗口函数-排序 窗口函数的“窗口” 行转列、列转行 思路 整体思路还是跟着 PSI 的计算公式走，按照某种方式（等频 / 等距）将基准的数据分成 n 箱子，基于基准数据的断点（Breakpoint Value）统计实际占比（$Actual$） 为了方便计算，先进行 列转行 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"1. 列转行 将score表进行列转行，变为key-value 键值对的形式 droptableifexistsscore_value_weekly;createtablescore_value_weeklyasselectno,week,score,score_valuefrom(selectno,week,score,score_valuefromscorelateralviewexplode(map('scoreA',scoreA,'scoreB',scoreB,'scoreC',scoreC,'scoreD',scoreD,'scoreE',scoreE))tasscore,score_value)wherescore_valueisnotnullandscore_valuenotin('null','NULL'); ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:1","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"2. 基于基准数据统计各箱nums 这里就要分成两种情况： 等频分箱 等距分箱 具体使用哪种分箱方式还是要结合模型分的实际应用情况 但是，当模型分呈现较为严重的偏态分布时，等频分箱会出现好几个箱子重合的情况（如下图所示）。这种情况算出来的PSI会小于真实值，此时可以采用等距分箱 等频分箱 droptableifexistsscore_group_nums_weekly;createtablescore_group_nums_weeklyasselectscore,week,score_group,count(no)asnumsfrom(selecta.*,casewhena.score_value\u003c=r.score_array[0]then'r1'whena.score_value\u003c=r.score_array[1]then'r2'whena.score_value\u003c=r.score_array[2]then'r3'whena.score_value\u003c=r.score_array[3]then'r4'whena.score_value\u003c=r.score_array[4]then'r5'whena.score_value\u003c=r.score_array[5]then'r6'whena.score_value\u003c=r.score_array[6]then'r7'whena.score_value\u003c=r.score_array[7]then'r8'whena.score_value\u003c=r.score_array[8]then'r9'whena.score_value\u003c=r.score_array[9]then'r10'endasscore_groupfromscore_value_weeklyaleftjoin(-- 2. 等频分箱 selectscore,percentile_approx(score_value,array(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),99999)asscore_arrayfromscore_value_weeklywhereweek=1-- 以第一周为基准 groupby1)rona.score=r.score)groupby1,2,3; 等距分箱 等距分箱相比等频分箱而言稍微复杂点，我的思路是先算出基准数据各个模型分区间的上下限，再统计各箱nums -- 10个箱子 -- -- 统计各箱nums droptableifexistsscore_group_nums_weekly;createtablescore_group_nums_weeklyasselectscore,week,nasscore_group,count(no)asnumsfrom(selecta.*,r.n,casewhena.score_value\u003e=r.range_downanda.score_value\u003cr.range_upthen'Y'whena.score_value=r.range_upthen'Y'else'N'endasis_rangefromscore_value_weeklyaleftjoin(-- 基准月各分数间隔 selectscore,n,min_score,max_score,(min_score+(n-1)*step)asrange_down,(min_score+n*step)asrange_upfrom(selecta.*-- 10 箱 ,casewhena.score_value=b.min_scorethen1elseceil(10*(a.score_value-b.min_score)/(b.max_score-b.min_score))endasn,b.min_score,b.max_score,b.stepfrom(select*fromscore_value_weeklywhereweek=1)aleftjoin(selectscore,min(score_value)asmin_score,max(score_value)asmax_score-- 10 个箱子 ,((max(score_value)-min(score_value))/10)asstepfromscore_value_weeklywhereweek=1groupby1)bona.score=b.score)groupby1,2,3,4,5,6)rona.score=r.score)whereis_range='Y'groupby1,2,3; ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:2","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"3. 计算PSI 回顾下公式 $$PSI=\\sum_{i=1}^{n}((Actual_i\\% - Expected_i\\%)\\times \\ln(\\frac{Actual_i\\%}{Expected_i\\%}))$$ droptableifexistsscore_stability_result_weekly;createtablescore_stability_result_weeklyasselecta.*,b.psifrom(-- 各箱nums selectscore,week,score_group_value[\"r1\"]asr1,score_group_value[\"r2\"]asr2,score_group_value[\"r3\"]asr3,score_group_value[\"r4\"]asr4,score_group_value[\"r5\"]asr5,score_group_value[\"r6\"]asr6,score_group_value[\"r7\"]asr7,score_group_value[\"r8\"]asr8,score_group_value[\"r9\"]asr9,score_group_value[\"r10\"]asr10from(-- 行转列 selectscore,week,str_to_map(concat_ws(',',collect_set(concat_ws(':',score_group,nums))))asscore_group_valuefromscore_group_nums_weeklygroupby1,2))aleftjoin(-- psi selectf.score,f.week,sum((f.act_rate-b.exp_rate)*log(f.act_rate/b.exp_rate))aspsifrom(-- Actual selectscore,week,score_group,(nums/sum(nums)over(partitionbyscore,week))asact_ratefromscore_group_nums_weeklywhereweek\u003e1)fleftjoin(-- Excepted selectscore,week,score_group,(nums/sum(nums)over(partitionbyscore,week))asexp_ratefromscore_group_nums_weeklywhereweek=1)bon(f.score=b.scoreandf.score_group=b.score_group)wheref.score_groupisnotnullandf.score_groupnotin('null','NULL')groupby1,2)bon(a.score=b.scoreanda.week=b.week); ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:4:3","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"可视化 最后，可以用Excel或BI软件完成对应的可视化 可视化 本文选择的是面积图 Python代码示例2 👇 # libraries import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd # Make data data = pd.DataFrame({ 'group_A':[1,4,6,8,9], 'group_B':[2,24,7,10,12], 'group_C':[2,8,5,10,6], }, index=range(1,6)) # We need to transform the data from raw data to percentage (fraction) data_perc = data.divide(data.sum(axis=1), axis=0) # Make the plot plt.stackplot(range(1,6), data_perc[\"group_A\"], data_perc[\"group_B\"], data_perc[\"group_C\"], labels=['A','B','C']) plt.legend(loc='upper left') plt.margins(0,0) plt.title('100 % stacked area chart') plt.show() area chartPercentage Stacked Area Chart \" area chart ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:5:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"总结 本文主要是提供了通过SQL计算多个模型分PSI的方案，并采用了等频、等距分箱两种分箱方法，增加了适用性 ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:6:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析","机器学习"],"content":"Reference https://mwburke.github.io/data%20science/2018/04/29/population-stability-index.html ↩︎ https://www.python-graph-gallery.com/255-percentage-stacked-area-chart ↩︎ ","date":"2021-06-18","objectID":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/:7:0","tags":["Model","SQL","Python"],"title":"SQL计算多个模型分的PSI","uri":"/2021/06/20210619-sql%E8%AE%A1%E7%AE%97psi/"},{"categories":["数据分析"],"content":"SQL, Python 中解决行转列、列转行的问题 在日常工作中总会遇到类似下图中的问题 👇 备注 我把这种情况称为 行转列 《Python for Data Analysis》 书中将其称为 Pivoting “Wide” to “Long” Format 还有这种问题 👇 备注 我把这种情况称为 列转行 《Python for Data Analysis》 书中将其称为 Pivoting “Long” to “Wide” Format 那么，接下来将针对此类问题，汇总SQL、Python中的实现方式 ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:0:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"行转列 / “Wide” to “Long” ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"SQL 运行环境 基于 spark-2.4.5U3 及以上版本 selectscore,ym,score_group_value[\"r1\"]asr1,score_group_value[\"r2\"]asr2,score_group_value[\"r3\"]asr3from(selectscore,ym,str_to_map(concat_ws(',',collect_set(concat_ws(':',range_label,nums))))asscore_group_valuefromscore_group_numsgroupby1,2) 基本思路是将表中 range_label和nums转化为类似json的格式，之后通过 key 索引得到对应的value 这里用的是collect_set()，得到的是聚合、去重后无序的array，若需要有序则可用sort_array() ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:1","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Python 构造数据 df_wide = pd.DataFrame({'score_name': ['ScoreA']*3 ,'ym': ['202012']*3 ,'range_label': ['r1','r2','r3'] ,'nums': [1110,1105,1054]}) df_wide pivot() 函数 df_wide.pivot(index=['score_name','ym'] ,columns='range_label' ,values=['nums']) ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:1:2","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"列转行 / “Long” to “Wide” ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"SQL 运行环境 基于 spark-2.4.5U3 及以上版本 selectbiz_no,ym,range_label,numsfromscorelateralviewouterexplode(map('r1',r1,'r2',r2,'r3',r3))tasrange_label,nums map 之后，结合 lateral view explode1 实现列转行的问题 ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:1","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Python 构造数据 df_long = pd.DataFrame({'score_name': ['ScoreA'] ,'ym': ['202012'] ,'r1': [1110] ,'r2': [1105] ,'r3':[1054]}) df_long stack()函数 df_long.set_index(['score_name','ym']).stack(dropna=False).reset_index().rename(columns={\"level_2\": \"range_label\",0:\"nums\"}) ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:2:2","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"Reference https://blog.csdn.net/oopsoom/article/details/26001307 ↩︎ ","date":"2021-06-15","objectID":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/:3:0","tags":["SQL","Python"],"title":"行转列、列转行","uri":"/2021/06/hql-%E8%A1%8C%E5%88%97%E8%BD%AC%E5%8C%96/"},{"categories":["数据分析"],"content":"以官方例子：微博转发关系图为例，说明所需要的数据格式 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:0:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"数据样例 import json from pyecharts import options as opts from pyecharts.charts import Graph with open(\"weibo.json\", \"r\", encoding=\"utf-8\") as f: j = json.load(f) nodes, links, categories, cont, mid, userl = j c = ( Graph(init_opts=opts.InitOpts(width=\"1920px\" ,height=\"1080px\")) .add( \"\", nodes, links, categories, repulsion=900, # 节点之间的斥力因子。值越大则斥力越大 # 支持设置成数组表达斥力的范围，此时不同大小的值会线性映射到不同的斥力。 gravity=0.01, # 节点受到的向中心的引力因子。该值越大节点越往中心点靠拢。 linestyle_opts=opts.LineStyleOpts(curve=0.2), label_opts=opts.LabelOpts(is_show=False), ) .set_global_opts( legend_opts=opts.LegendOpts(is_show=False), title_opts=opts.TitleOpts(title=\"Graph-微博转发关系图\"), ) .render(\"graph_weibo.html\") ) weibo.json 文件，可从 这儿 获取 主要由5部分组成 nodes links categories cont mid userl ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:1:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"格式说明 说明 本文聚焦在前三个的数据格式说明。每一部分是个list，每个list 又由多个dict组成 以“转发微博”作为场景简单阐述关系图所展示的信息：某位具有影响力的微博用户A 发了条微博，被用户B、C、D看到并转发了；之后，用户E、F、G也转发了B所转发的这篇文章，以此类推 那么，这个过程中涉及到的每个用户便是一个node。为此，也以 nodes作为切入点展开说明 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"nodes 样例如下所示， 记录着节点的信息12， { \"name\": \"Camel3942\", # 节点名称，即博主昵称 \"symbolSize\": 5, # 图中标志大小 \"draggable\": \"False\", # 是否可拖动 \"value\": 1, # 被再次转发次数 \"category\": \"Camel3942\", # From Where \"label\": { # 此博主被再次转发后，含有此标签，否则不含 \"normal\": { \"show\": \"True\" } } } 警告 节点名称（name）不能重复 ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:1","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"links 个人理解作用是将每个node 连接起来 { \"source\": \"新浪体育\", \"target\": \"Beijingold4\" }, { \"source\": \"Camel3942\", \"target\": \"xiaoA\" } ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:2","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"categories 而这部分则记录着有被他人转发的用户名称（name），即 links 中 source 所对应的内容 { \"name\": \"新浪体育\" }, { \"name\": \"Camel3942\" } ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:2:3","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["数据分析"],"content":"Reference https://blog.csdn.net/qq_35006861/article/details/116721589 ↩︎ https://blog.csdn.net/Kevin_HZH/article/details/91043392 ↩︎ ","date":"2021-06-07","objectID":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/:3:0","tags":["可视化","pyecharts","Python"],"title":"pyecharts-关系图","uri":"/2021/06/20210607-%E5%8F%AF%E8%A7%86%E5%8C%96-%E5%85%B3%E7%B3%BB%E5%9B%BE/"},{"categories":["写作"],"content":"详述Hugo+Github搭建个人博客 受我司同期大佬的影响，我也便搭建个人博客折腾下，记录点什么~ 在这demo成型之际，以小白视角记录下基于 HuGo 和 Github 完成搭建的历程 所以，最初的准备工作便是（假设已安装Git） ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:0:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"安装 Hugo 可以参考官方文档完成在Windows, Mac等平台的安装 虽然HuGo是基于Go语言编写的，但并不是一定要安装Go才能使用HuGo。可跨平台 在此，以 Windows 系统为例记录安装过程 在这儿下载对应的压缩包 解压 hugo_0.83.1_Windows-64bit.zip解压后得到的文件如下图所示 添加至环境变量 将hugo.exe文件放入bin文件夹（若无可新建） 验证是否安装成功 hugo version # hugo v0.83.1-5AFE0A57 windows/amd64 BuildDate=2021-05-02T14:38:05Z VendorInfo=gohugoio 本地创建博客 终端 cd 至某个文件下 hugo new site unclehuzi # blog目录就是创建的博客目录 # Congratulations! Your new Hugo site is created in D:\\unclehuzi. # Just a few more steps and you're ready to go: # 1. Download a theme into the same-named folder. # Choose a theme from https://themes.gohugo.io/ or # create your own with the \"hugo new theme \u003cTHEMENAME\u003e\" command. # 2. Perhaps you want to add some content. You can add single files # with \"hugo new \u003cSECTIONNAME\u003e\\\u003cFILENAME\u003e.\u003cFORMAT\u003e\". # 3. Start the built-in live server via \"hugo server\". # Visit https://gohugo.io/ for quickstart guide and full documentation. tree unclehuzi/ ##### D:\\unclehuzi ├─archetypes ├─content # 博客文章存放目录（markdown文件） ├─data ├─layouts ├─static # 静态文件/图片/CSS/JS文件 ├─themes # 博客主题模板存放目录 └─config.toml # 博客的配置文件 选择主题 我选择的是 Maupassant，通过Git 的方式获取 # cd 至 unclehuzi 文件夹 git clone https://github.com/flysnow-org/maupassant-hugo themes/maupassant cp themes/maupassant/exampleSite/config.toml . # 使用模板自带的配置文件替换默认配置文件 mkdir content/post # 创建博客文章md文件存放路径（该主题模板要求放在content/post目录下） 根据需要调整 D:\\unclehuzi\\config.toml 文件 baseURL = \"your_github_name.github.io\" # 修改为博客的网址，此处使用github pages地址，后面具体介绍 languageCode = \"zh-CN\" title = \"胡子叔叔的小站\" # 博客的名字 theme = \"maupassant\" 具体可以参考 官方说明 进行配置 本地测试 # cd D:\\unclehuzi hugo new post/my-first-blog.md vim content/post/my-first-blog.md # 以下为md文件内容 +++ title=\"My First Blog\" tags=[\"blog\"] categories=[\"博客相关\"] date=2020-01-16T10:37:32+08:00 draft=false # 此处要改为false，否则在首页不会显示！ +++ #### Hello World! 通过 hugo server -D 命令启动，访问 http://localhost:1313/ 即可 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:1:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"建立博客专属的仓库 主要的事情说三遍： Repository name 填 github名字.github.io，如 unclehuzi.github.io Repository name 填 github名字.github.io，如 unclehuzi.github.io Repository name 填 github名字.github.io，如 unclehuzi.github.io 仓库创建完成之后记得修改 D:\\unclehuzi\\config.toml 文件的 baseURL ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"生成 public 文件 创建完成之后，通过 hugo 生成 public 文件 hugo --theme=maupassant --baseUrl=\"https://your_github_name.github.io\" # theme为主题模板名称，url为上一步创建的github仓库名称 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:1","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"通过 git 命令将 public 文件推送至仓库 cd D:\\unclehuzi\\public # 1. 初始化 git init # 2. 将文件夹下所有文件加入本地仓库 git add . # 3. 添加注释 git commit -m \"comments\" # git commit -m \"updates $(date)\" # 4. 建立连接 BY HTTPS git remote add origin https://github.com/unclehuzi/unclehuzi.github.io.git # 5. 远程仓库拉到本地（如果远程仓库内没有文件可跳过步骤5） git pull --rebase origin master # 之后更新本地： git pull origin master # 6. 上传 push git push -u origin master # git push -u origin master -f # 强制上传 # 备注 # 和远程建立连接之后，执行 2-3-6 步骤即可。 最后，访问 https://your_github_name.github.io 即可 ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:2:2","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":["写作"],"content":"总结 本文基于 Github Pages 服务，使用成熟的框架及主题快速搭建个人博客demo。 更新网站两步走： 在博客文件夹（D:\\unclehuzi）下生成 public 文件夹 将 public 文件夹内的所有文件推送至GitHub仓库 后续在网站设计方面还需要了解下Web开发相关知识，针对性的修改主题的源码以实现自己的需求~ ","date":"2021-05-28","objectID":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/:3:0","tags":["blog"],"title":"GitHub+Hugo快速搭建个人博客","uri":"/2021/05/20210528-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"categories":null,"content":"不打羽毛球、不写代码的营销人不是一名好分析师","date":"2021-05-28","objectID":"/about/","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"工作经历 ","date":"2021-05-28","objectID":"/about/:1:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"360数科 思维层面 🤔 💫 （直接/间接）抽象职场通用 skills 🤺 理论指导实践，如 客群划分 技术层面 👨‍💻 精进SQL（Spark、Hive），如 sql计算PSI sql计算IV 快、准、狠 🏃 🏃‍♂️ 数据建模 试图做一个从理论出发的的“调包侠” ","date":"2021-05-28","objectID":"/about/:1:1","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"实习经历 在 数据分析 这条路上的探索，“一顿操作”（实习）之后得到的主要结论（/经验）： 👇 数据分析依托于行业 / 场景，但更加需要基于某个行业抽象出复用的部分； 基于1，一定要把 数据分析 单拎出来的话，更像是中台赋能的味道（或者说“乙方”）； 公司内对数据的重视程度决定了数分的地位（或者说数分的负责人是否能“话事”），否则就是 “SQL-boy”。 资源的重要性往往远远大于能力。公司存活与发展靠的是制度，并非个人英雄主义～ 正是因为这一层特性，除了 “big boss”，没有人是不可替代的。真的到了一定程度，“big boss” 也是可以换的 业务、技术加点方向可以围绕价值链进行：从数据的产生 👉 存储 👉 离线分析与挖掘 👉 反哺。其实各个阶段都需要面向业务，如，存储阶段中数仓的主题域 行业 公司 岗位 主要干啥 收获了啥 互联网 哈啰出行 🚴‍♀️ 数据分析 对接UI、运营的需求 指标体系；用户转化（/迁移）；Presto、Hive 房地产 旭辉 市场研究 经济政策、微观行为 量化指标；逻辑至上、数据支撑 咨询 Teradata 数据分析与挖掘 骚扰电话特征挖掘 分析思路；框架先行 汽车 通用·GM China 市场研究 数据基建、数据管理 VBA、SQL；经济学思维启蒙 融资租赁 开心汽车 数据分析 盘借贷逾期的逻辑 要面向业务逻辑写SQL ","date":"2021-05-28","objectID":"/about/:2:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"研究经历 矛盾性追加评论对感知有用性及购买意愿的影响研究 最初是用京东的评论写了个小论文，水了个会议。 硕士毕业论文时，主要从 数据源（一手问卷和二手电商平台评论数据）和 模型（零膨胀计数模型）两个维度更近了一步。 虽然一顿操作猛如虎，一看结论真是虎～ 但是，整个过程就像自己 owner 一个项目，从0到1、一步一步推进 💃、精准打击 👊 各个困难点～ 无论是为了满足会议的时间截止时间还是毕业论文阶段的各个节点，都让我离“时间管理大师”更近了一步 😄 当然，除了时间管理、项目管理，还有学习能力。即，需要在短时间内掌握相关知识以解决相应的问题。 爬虫 情感二分类 研究方法论 中介、调节以及有调节的中介效应分析 R语言 。。。 ","date":"2021-05-28","objectID":"/about/:3:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"平时干些啥 ","date":"2021-05-28","objectID":"/about/:4:0","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"羽毛球 🏸️ 平时（主要是周末）也会在下面俩地方打羽毛球 🏸 虽然技术不咋地，但就是玩~约球~ 上海市浦东新区羽山路1200号(近崮山路) 上海市浦东新区峨山路91弄140号同学汇综合运动馆 ","date":"2021-05-28","objectID":"/about/:4:1","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"游戏 🎮 领略“海拉鲁大陆”的风光，看心情拯救公主 👸 ","date":"2021-05-28","objectID":"/about/:4:2","tags":null,"title":"CV","uri":"/about/"},{"categories":null,"content":"写文章 该个人博客便是个例子，以及我的个人公众号 ~欢迎关注一波~ ","date":"2021-05-28","objectID":"/about/:4:3","tags":null,"title":"CV","uri":"/about/"},{"categories":["数据分析"],"content":"分场景汇总日期函数 工作中总会遇到处理时间的问题，参考营销理论中基于利益细分的市场细分理论，我从使用场景的角度出发，将常用的日期函数分为四大类： 时间计算 时间提取 格式转换 当前时间 Tip 本文重点在于整合日期函数 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:0:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"时间计算 这部分主要是计算时间差（datediff(end_date,start_date), months_between(date1,date2)）、时间加减（date_add(),date_sub(),add_months()）等 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:1:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"时间提取 提取时间戳的年、季度、月、周、日、小时、分钟、秒 可以直接调用对应的函数，也可使用extract(field from column_name) 函数指定 field，其中field 支持day, dayofweek, hour, minute, month, quarter, second, week and year. ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:2:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"格式转换 有时为了适应不同时间格式的需求，需要做个转换，如yyyy-MM-dd 或 yyyy-MM-dd HH:mm:ss的形式转为yyyyMMdd 等 常用： to_date() 返回 date 形式的日期，即yyyy-MM-dd date_format() 转为指定格式的时间，如 date_format('2015-04-08','y') =\u003e '2015' ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:3:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"当前时间 若需要时间戳格式，则用current_timestamp 若只需要精确到天，即date格式，则用current_date ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:4:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"总结 本文重点在于从使用场景的角度出发，整合了hive, spark 环境下常用的日期函数。最后再以表格的形式简单汇总 场景 函数 返回值类型 描述 示例 当前时间 unix_timestamp() bigint 当前 Unix时间戳（e.g. 1622451519 ），但因查询优化问题推荐使用 current_timestamp 当前时间 current_date date 当前日期 2021-05-31 当前时间 current_timestamp timestamp 当前时间戳 2021-05-31 17:12:14.968 - - - - - 格式转换 from_unixtime(bigint unixtime[, string format]) string 数字转为格式形如 2021-05-31 17:12:14 的字符串 格式转换 to_date(string timestamp) date to_date(yyyy-MM-dd HH:mm:ss) =\u003e yyyy-MM-dd 格式转换 date_format(date/timestamp/string ts, string fmt) string 得到指定格式的时间 date_format(‘2015-04-08’, ‘y’) =\u003e ‘2015’ 格式转换 trunc(string date, string format) string 得到被指定format截断的日期，format支持MONTH/MON/MM, YEAR/YYYY/YY trunc(‘2015-03-17’, ‘MM’) =\u003e 2015-03-01 - - - - - 时间提取 year(string date) int 年 时间提取 quarter(date/timestamp/string) int 季度 时间提取 weekofyear(string date) int 该年的第几周 时间提取 month(string date) int 月 时间提取 day(string date) dayofmonth(date) int 日 时间提取 hour(string date) int 小时 时间提取 minute(string date) int 分钟 时间提取 second(string date) int 秒 时间提取 extract(field FROM source) int field 支持day, dayofweek, hour, minute, month, quarter, second, week and year. - - - - - 时间计算 datediff(string enddate, string startdate) int 天数差 datediff(‘2009-03-01’, ‘2009-02-27’) =\u003e 2 时间计算 date_add(date/timestamp/string startdate, tinyint/smallint/int days) date 加（减）x天后的日期 date_add(‘2008-12-31’, 1) =\u003e 2009-01-01, date_add(‘2008-12-31’, -1) =\u003e 2008-12-30 时间计算 date_sub(date/timestamp/string startdate, tinyint/smallint/int days) date 加（减）x天后的日期 date_sub(‘2008-12-31’, 1) =\u003e 2008-12-30, date_sub(‘2008-12-31’, -1) =\u003e 2009-01-01 时间计算 add_months(string start_date, int num_months, output_date_format) string x月后。如果start_date是该月的最后一天 或者 x月后的天数不是“大月”则结果为x月后该月的最后一天 add_months(‘2017-12-31 14:15:16’, 2, ‘YYYY-MM-dd HH:mm:ss’) =\u003e ‘2018-02-28 14:15:16’ 时间计算 last_day(string date) string 该月最后一天的日期 last_day(2021-05-11) =\u003e ‘2021-05-31’ 时间计算 next_day(string start_date, string day_of_week) string 返回大于 start_date 的日期中最近的一个周x next_day(‘2021-05-31’,‘Monday’) 时间计算 months_between(date1, date2) double date1-date2 月数差 months_between(‘1997-02-28 10:30:00’, ‘1996-10-30’) =\u003e 3.94959677 ","date":"2021-05-20","objectID":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/:5:0","tags":["SQL"],"title":"Spark, Hive QL-日期函数汇总","uri":"/2021/05/hql-%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0/"},{"categories":["数据分析"],"content":"以 “计算当前和上一次事件的时间间隔” 引入 positional function 截止到目前，窗口函数整理了聚合、排序场景，解决了“组内占比”、“定位连续3天登录用户”等问题 在平时的分析工作中，还有个比较常见的问题：计算当前和上一次事件的时间间隔。比如，相邻两次外呼的时间间隔 这个时候，lead() 或 lag() 函数可较为方便的解决该类问题 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:0:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"实现的功能 lead(), lag() 实现的功能比较类似。 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"lead() 上移 selectseller_name,sale_value,lead(sale_value)over(orderbysale_value)asnext_sale_valuefromsale; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:1","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"lag() 下移 selectseller_name,sale_value,lag(sale_value)over(orderbysale_value)asprevious_sale_valuefromsale; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:1:2","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"基本语法 lag(expression[,offset[,default_value]])over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...)lead(expression[,offset[,default_value]])over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) lead(), lag() 中的3个参数： expression - string 被操作的列名 offset - int 移动的行数（/偏移量） default_value 定义为空的情况赋给的默认值 其中，参数 expression 是必须的。而 default_value（默认是 NULL） 是只有当 offset（默认是 1） 有值时才能使用 over() 语句中，order by 是必须要有的 ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:2:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"应用 计算用户相邻两次登录的天数间隔 -- 如果只有1天有登录信息，则 diff_days 为 null select*,datediff(session_date,lag_session_date)asdiff_daysfrom(-- 下移 select*,lag(session_date)over(partitionbyuser_idorderbysession_dateasc)aslag_session_datefrom(-- 按天去重 selectuser_id,date_format(session_time,'yyyyMMdd')assession_datefromtable1groupby1,2)); 窗口函数还有俩常见的：first_value(), last_value()，在此就略过了。 有时候可以用 row_number() over() 结合 having 一起使用，如 确定用户最后一次登录时间 selectuser_id,row_number()over(orderbysession_datedesc)asrkfromtable1havingrk=1; ","date":"2021-05-10","objectID":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/:3:0","tags":["SQL"],"title":"窗口函数-Positional Functions","uri":"/2021/05/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-positionalfunctions/"},{"categories":["数据分析"],"content":"详述“窗口”的概念，结合初中数学中区间的概念来理解\u003cwindow_frame\u003e，并以计算累计占比为例深化理解。此外，也分享了他人整理的窗口函数汇总表 基于之前整理的 排序 聚合 Positional functions: lead(), lag() 在窗口函数应用场景方面算是告一段落了。但是在 “窗口” 这个概念上陈述较少，在窗口函数部分的里程碑之际重新 “定义” “窗口” 另外，之前在浏览网页的时候发现了窗口函数的汇总图 而本文与图中对应的便是 WINDOW FAME 部分 ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:0:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"实现的功能 简单来说是定义窗口函数作用的范围（“FRAME”），通过下面这张图1可以更好的了解 FRAME 的概念 一般而言， 一张表（Table）基于WHERE条件的作用得到图中 Result Set 部分； 窗口函数 over() 语句中 partition（若有）得到图中 Partition 1…Partition m fram 语句（若有）在partition基础上得到图中 Frame 1…Frame n ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:1:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"语法、规则 \u003cwindow_frame\u003e:=[rows|range|groups]between[unboundedpreceding|\u003cn\u003epreceding|currentrow]and[unboundedfollowing|\u003cn\u003efollowing|currentrow] 批注 目前，只有 PostgreSQL 11 及以上版本支持 groups \u003cwindow_frame\u003e语句表明，相对于当前行（current row）对应的值而言，还有“区间”的概念，“区间”又受到 rows或range 控制：是行数范围还是值的范围。 rows 对应是行的条件 如rows between 1 preceding and unbounded following 表示最终的范围是排序后（若有），基于当前行的上 1 行和该partition本身的最后一行 range 对应是值的范围 如range between 1 preceding and 2 following 这里我们遵循小学数学中区间的性质：左区间的值小于等于右区间的值 因为涉及到值的范围，这里就要分两种情况讨论了，假设当前行对应的值为 x 顺序排序，即从小到大，order by column asc [x-1,x+2]，左区间为当前行的值减1（x-1）；右区间为当前行的值加2（x+2） 逆序排序，即从大到小，order by column desc [x-2,x+1]，左区间为当前行的值减2（x-2）；右区间为当前行的值加1（x+1） 最后再说明下没有 \u003cwindow_frame\u003e 语句时对应的Frame，此时将取决于是否有order by语句，即 无 \u003cwindow_frame\u003e 语句、有 order by 语句 Frame 为 range between unbounded preceding and current row 即Frame的第一行为该partition的上边界，当前行（current row）为下边界 无 \u003cwindow_frame\u003e 语句、无 order by 语句 Frame 为 rows between unbounded preceding and unbounded following 即Frame的边界就是partition的边界 关于，无\u003cwindow_frame\u003e语句的情况，总结如下 \\ 无 \u003cwindow_frame\u003e 有 order by range between unbounded preceding and current row 无 order by rows between unbounded preceding and unbounded following ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:2:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"应用：一个例子 一堆枯涩的陈述，不如直接来个小例子：计算累计占比 实际业务中，在定义模型目标变量y的时候，往往也会结合数据的分布。如，风控场景中定义逾期 x 天以上为bad 假设samples 表中记录着一笔订单的逾期状态，over_due_days 表示逾期天数 希望得到的数据样式如下表所示 over_due_days c_sum c_sum_rate 0 1000 1 1 100 0.1 2 90 0.09 3 85 0.085 4 80 0.08 … … … over_due_days 为1的那一行表示 $逾期天数 \\geq 1$的订单数以及占总订单的比例，即 $$c\\_sum\\_rate=\\frac{over\\_due\\_days \\geq 1 的订单数}{总订单数}$$ selectover_due_days,sum(nums)over(orderbyover_due_daysnullslastrangebetweencurrentrowandunboundedfollowing)asc_sum,sum(nums)over(orderbyover_due_daysnullslastrangebetweencurrentrowandunboundedfollowing)/(selectcount(1)fromsamples)asc_sum_ratefrom(-- 分逾期天数统计订单数量 selectover_due_days,count(1)asnumsfromsamplesgroupby1); ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:3:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"总结 本文便是窗口函数部分的“收官之作”了。 主要是对“窗口”的概念展开了详细的陈述，结合初中数学中区间的概念来理解\u003cwindow_frame\u003e，并以计算累计占比为例深化理解。此外，也分享了他人整理的窗口函数 ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:4:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"Reference https://en.wikibooks.org/wiki/Structured_Query_Language/Window_functions ↩︎ ","date":"2021-04-30","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/:5:0","tags":["SQL"],"title":"窗口函数的“窗口”","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E7%AA%97%E5%8F%A3/"},{"categories":["数据分析"],"content":"“抛” 计算组内占比 “引” 聚合窗口函数 窗口函数中 求和（sum）、均值（avg）、极值（max, min）、计数（count）等结合聚合函数使用的场景也较多。 数据分析过程中经常会遇到计算组内占比的情况。 Example 计算 多个模型分以及多个时间段 的 psi 时，（等频/等距）分箱之后计算各箱样本占总样本数的百分比 示例如下表所示， model ym bucket act_rate A 202103 1 0.1209 A 202103 2 0.1148 A 202103 3 0.1089 A 202103 4 0.1041 A 202103 5 0.1004 A 202103 6 0.0983 A 202103 7 0.0984 A 202103 8 0.0937 A 202103 9 0.0892 A 202103 10 0.0714 比较方便的操作方式就是结合 sum() over() 函数计算组内占比。 selectmodel,ym,bucket,(nums/sum(nums)over(partitionbymodel,ym))asact_ratefrommodel_bucket_nums 其他几个聚合函数只是实现的功能不同，最后还是要各取所需了。 ","date":"2021-04-19","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E8%81%9A%E5%90%88/:0:0","tags":["SQL"],"title":"窗口函数-聚合","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E8%81%9A%E5%90%88/"},{"categories":["数据分析"],"content":"整理排序场景常用函数，row_number() over(), rank() over(), dense_rank() over(), ntile(n) over()，并以连续登录问题为例深化理解 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:0:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"row_number() over() ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"实现的功能 从1开始依次排序，生成不会重复的编号 -- 按照nums 列，降序排序 selectid,nums,row_number()over(orderbynumsdesc)asrankfromtable id nums rank 1x 45 3 2x 78 2 3x 87 1 4x 32 4 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"语法 row_number()over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) partition by表示基于某（些）维度（/列）分组之后，再基于order by的规则实现组内排序。 select id ,nums ,row_number() over(partition by id order by nums desc) as rank from table ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:2","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"应用 问题 如何确定连续登录天数超过3天的用户 思路 找到连续3天登录用户所表现的数据特征。比如，按照登录日期排序得到编号，两者作差，若连续登录则作差后的值是一样的 基于这个现象，可用row_number实现 selectuser_id,(session_date-rk)asdiff,count(1)asnumsfrom(select*,row_number()over(partitionbyuser_idorderbysession_date)asrkfrom(-- 按天去重 selectuser_id,date_format(session_time,'yyyyMMdd')assession_datefromtable1groupby1,2))groupby1,2havingnums\u003e=3; ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:1:3","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"rank() over() 基本语法类似于row_number() rank()over([partitionbyexpr1,expr2,...]orderbyexpr1[asc|desc],expr2[asc|desc],...) 但不同的是，当值相等时 rank() 排序会出现重复序号的情况，且下个序号和当前序号之差为当前相同值的个数 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:2:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectdealer_id,emp_name,sales,rank()over(orderbysales)asrkfromq1_sales; dealer_id emp_name sales rank 1 Raphael Hull 8227 1 3 May Stout 9308 2 2 Haviva Montoya 9308 2 1 Jack Salazar 9710 4 3 Abel Kim 12369 5 3 Ursa George 15427 6 2 Beverly Lang 16233 7 2 Kameko French 16233 7 1 Ferris Brown 19745 9 1 Noel Meyer 19745 9 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:2:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"dense_rank() over() row_number() over() 、 rank() over() 和 dense_rank() over() 之间的差别主要在于对相同值的序号处理方式不同。 和rank() over()一样，遇到相同值时序号会重复，但是dense_rank() over() 的下一个序号和当前序号之差依然是1，不会出现空位的情况。 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:3:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectdealer_id,emp_name,sales,dense_rank()over(orderbysales)asdenserankfromq1_sales; dealer_id emp_name sales denserank 1 Raphael Hull 8227 1 3 May Stout 9308 2 2 Haviva Montoya 9308 2 1 Jack Salazar 9710 3 3 Abel Kim 12369 4 3 Ursa George 15427 5 2 Beverly Lang 16233 6 2 Kameko French 16233 6 1 Ferris Brown 19745 7 1 Noel Meyer 19745 7 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:3:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"ntile(n) over() ntile(n) over() 和之前那三个排序函数不太一样。形式来看，多了个参数n，是指按照顺序平均分成n份（/箱），返回当前所在的位置。且需要order by 语句。 但对于不能实现平均分的情况，会基于约定来操作： 约定 每箱记录数不能大于上一个箱子的记录数。即第1组的记录数大于等于第2组的记录数。 所有箱子的记录数要么相同。要么从某一记录数较少的箱子（命名为X）开始，后面所有箱子内的记录数都与该箱（X）的记录数相同。即如果前3箱的记录数都是9，而第4箱的记录数是8，那么第5、6箱及其之后箱子内的记录数也必须是8。 注意 最先分出来的箱子，采取向上取整（ceil()）的方式 比如，53条记录，基于ntile的约定分到5个箱子，则每个箱子的记录数如下所示 bucket nums 1 11 2 11 3 11 4 10 5 10 备注 ntile的方法能较好实现等频的效果，相比分位数作为分割点而言，不易受数据分布的影响。 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:4:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"示例 selectemp_mgr,sales,ntile(5)over(orderbysales)asntilerankfromq1_sales; emp_mgr sales ntilerank Kari Phelps 8227 1 Rich Hernandez 9308 1 Kari Phelps 9710 2 Rich Hernandez 12369 2 Mike Palomino 13181 3 Rich Hernandez 15427 3 Kari Phelps 15547 4 Mike Palomino 16233 4 Dan Brodi 19745 5 Mike Palomino 23176 5 ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:4:1","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析"],"content":"总结 窗口函数 返回类型 描述 row_number() int 从1开始依次排序，生成不会重复的序号 rank() int 从1开始依次排序。若值相等则得到同样的序号；且下一个序号将会出现空位，即若2个相等的值序号是1，则下一个序号是3 dense_rank() int 从1开始依次排序。若值相等则得到同样的序号；但下一个序号不会出现空位，即若2个相等的值序号是1，则下一个序号依然是2 ntile(n) int 将分组数据按照顺序平均分成n箱，返回当前值所在位置，n-th ","date":"2021-04-09","objectID":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/:5:0","tags":["SQL"],"title":"窗口函数-排序","uri":"/2021/04/hql-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-%E6%8E%92%E5%BA%8F/"},{"categories":["数据分析","写作"],"content":"调节效应分析（ Moderation Analysis ），回答 WHEN 的问题 之前整理了中介效应分析，解决了怎么看中介效应是否显著的问题。 这篇继续整理调节效应分析（Moderation Analysis） 中介变量回答的是关于 HOW 的问题，而调节变量回答的是关于 WHEN 的问题 $X$ 什么时候影响 $Y$，或 $X$ 影响 $Y$ 的过程中是否取决于变量 $W$ ，而变量 $W$ 就是调节变量 典型且简单的调节效应模型图如下所示 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"理论先行 调节效应理论模型用 statistical diagram 表示为， 即， $$ Y=i_Y+b_1X+b_2W+b_3XW+e_Y $$ ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"系数解释 各系数的解释如下 $b_1$ $b_1$ 是当 $W=0$ 时，$X$ 改变一个单位，$Y$ 改变 $b_1$ $$ b_1=[\\widehat{Y}|(X=x+1,W=0)] - [\\widehat{Y}|(X=x,W=0)] $$ $b_2$ $b_2$ 是当 $X=0$ 时时，$W$ 和 $Y$ 之间的关系 $$ b_2=[\\widehat{Y}|(W=w+1,X=0)] - [\\widehat{Y}|(W=w,X=0)] $$ $b_3$ 这是个大头，他是比较两组之间的差异， 一组是 $W$ 不变，$X$ 改变一个单位 另一组是，$W$ 和 $X$ 都改变一个单位 即， $$ b_3=([\\widehat{Y}|(X=x+1,W=w)] - [\\widehat{Y}|(X=x,W=w)]) - ([\\widehat{Y}|(X=x+1,W=w+1)] - [\\widehat{Y}|(X=x,W=w)]) $$ 做调节效应分析的时候， 理论上是希望 $X$ 影响 $Y$ 的过程中取决于变量 $W$ 对应着，公式（1）也可以改写为， $$ Y=i_Y+ \\theta_{X \\rightarrow Y}X + b_2W + e_Y $$ 其中，$\\theta_{X \\rightarrow Y} = b_1+b_3W$ 这也就生动形象的表示了， 如果 $b_3$ 显著性不等于 0 ，那么 $W$ 的值不同，$X$ 对 $Y$ 的影响也不同 所以当回归结果中，系数 $b_3$ 显著时（$p\u003c0.05$），即变量 $W$ 的确起到调节作用时 我们将会进一步分析对应不同 $W$，$X$ 是如何影响 $Y$ 的 常用的方法便是 pick-a-point approach ，又称 spotlight analysis 我第一次看到 spotlight analysis 这个名词，是在一篇JM的文章上，当时查了半天也不知道啥意思。。。 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:1","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"pick-a-point approach 基本思路是根据 $W$ 的数据选三个值，进而表示低-中-高三种状态 一般就选数据16th, 50th, and 84th分位数分别表示低-中-高，进而分析 $X$ 对 $Y$ 的影响是否显著 如果 $W$ 是分类变量就直接看各自类别的情况了，不用取点了。 所以，综上所述，基于公式(1) 构建回归模型，根据交互项系数 $XW$（$b_3$）是否显著，进而确定调节效应是否存在。 以上这些都能通过PROCESS1很好的实现。 ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:2","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"实际操作 步骤如下： SPSS加载 process的语法文件 使用过程中可以自主编写语法、也可以菜单式的操作，如 process y=justify/x=frame/w=skeptic/model=1/plot=1. 变量和数据字段一一对应 基于理论研究模型，选择PROCESS的模型（本例对应的是model 1） 选项中勾选 Generate code for visualizing interactions 如果是用spss语法的话，加上 plot=1 即可 这能方便用SPSS作斜率图，用于可视化调节效应 复制PROCESS分析结果中的可视化脚本即可，如 （可选）中心化 有些文章中会说做调节效应分析前，对变量 $X$ 和 $Y$ 进行中心化处理，可能会见到 scaling 这样的术语 但其实无所谓啦，而且就算对和进行中心化处理，也不会影响 $W$、$XW$ 的系数，只是会影响 $X$ 的系数（$b_1$）罢了 因为之前有提到，$b_1$ 是表示 $W$ 为0时，$X$ 对 $Y$ 的影响，如果对 $W$ 做中心化处理， 即 $W^,=W - \\overline{W}$ 那么这时候 $X$ 的系数对应的是 $W$ 取样本均值（$\\overline{W}$）时，$X$ 对 $Y$ 的影响 得到结果之后，直奔交互项系数（本例对应的是 $b_3$）即可 若显著（$p\u003c0.05$），PROCESS便会生成低-中-高状态下，$X$ 和 $Y$ 之间的关系如下所示， ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"参考资料 Hayes A F: Introduction_to_Mediation_Moderation_and_Conditional_Process_Analysis_A_Regression_Based[M].2ed.2018 ↩︎ ","date":"2021-03-25","objectID":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】-调节效应分析","uri":"/2021/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%B0%83%E8%8A%82%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析"],"content":"过去的一段时间里一直在琢磨客群划分的问题 segmentation 隐隐约约觉得所谓的 客群划分 和 市场细分 不谋而合，并且我发现风控领域很重视 “圈客群” 但因为部门不同，最后关注的目的也存在一定的差异。对于风控而言，在兼顾风险的同时，给出差异性的策略。在贷前，差异性策略方面可能更多是关注授信额度的问题。贷中，更多关注调额方面。 本文将整理市场细分/客群划分的一些方法 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:0:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"有监督学习—树模型 相关文章： 决策树简介 可视化决策树结果 样本不均衡的问题 decision tree 将客群划分（/市场细分）看成是一个分类问题，因为做客群划分肯定是有目的/目标的，比如是否违约、客户忠诚等等 针对我们的目标，定义好我们的问题，进一步得到模型需要的label（y值）。基于提取的变量，最后可采用决策树实现客群划分，甚至可以采用随机森林、XGBoost，可视化前几棵树，看看模型给出的变量规则。 除了对算法本身的了解外，我觉得 找变量（影响label的影响因素）也是难点，毕竟需要面向业务建模（最近新造的词）。除了关注模型区分度等，更多需要注意的是 解释性，是否make sense，能不能从业务的角度去解释树模型给出的规则。 所以，在特征选择以及构造新特征方面真的需要花很大的功夫。 得到树模型输出的规则后，我们还需要注意 客群之间的差异性 基于决策树得到的规则，看客群之间在目标方面（如，违约率）的差异性 客群的稳定性 基于规则，看未来的人数分布及目标（如，违约率） 这里还涉及到一个外部数据的问题。客群划分时，尽量避免使用外部数据、尤其是被加工过的一些指标。一是不知道计算逻辑；二是不稳定。所以一般直接选取用户本身属性变量。当然，这也取决于分群的场景。 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:1:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"聚类 clustering 聚类用于市场细分也是比较常见的。 但最近我觉得，聚类目的性不明确。换句话说，市场细分的目的性不明显。 聚类更多是在某些变量下将相似的归为一簇 但在车企，还是会采用聚类细分市场的，如基于价格、投影面积、离地高度等变量划分车型。这也符合最初的目的：根据某些车辆参数相似的归为一类。 到底是视为聚类还是分类问题，还是要具体问题具体分析 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:2:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"基于常规的细分变量 人口统计变量 性别、年龄、职业、收入等，但“收入”的数据一般比较难获取，大多数情况下可能也是通过各种方式去预测 地理位置 “地域决定论” + 一方水土养一方人 这个还是要考虑具体业务 如果从逾期等违约情况来看的话：经济基础决定上层建筑，而一个城市的文明程度和经济发展也是有很大关系的。而经济发展又会受到地域的限制，正是所谓的“地域决定论”，所以经济发展差距较大的城市往往违约率也存在一定差异 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:3:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"不成熟的想法 在现金贷背景下，基于入不敷出的逻辑来关注风险的问题，我通过构建“收支”模型（收入和支出之间的关系）实现客群的划分。 seg 收入 支出 1 高 高 2 高 低 3 低 高 4 低 低 当然，这里的高、低是针对产品的目标用户而言 显然，对于低收入、高支出的客群，更有可能发生逾期的情况，但同时这部分人群也是更有可能产生收益的人群，所以针对这部分人群就需要深挖，兼顾风险的同时给予一定的高额度。 但难点就是在于对“收入”、“支出”的测量，在无法准确获取收入、支出数据的情况下，无论是用于测量收入还是支出的变量都需要满足产品所服务对象的特征，以能较为准确的测量“收入”与“支出”的状态。 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:4:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["数据分析"],"content":"总结 本文分别阐述了 有监督、无监督以及 常规细分变量三大划分客群的方法，具体还是要结合实际业务场景采用相应的方法 基于相应的规则完成划分后，还要基于目标问题进行横纵向对比： 横向比较客群的稳定性 纵向比较客群的差异性 ","date":"2020-09-02","objectID":"/2020/09/da-segmentation/:5:0","tags":["用户画像"],"title":"客群划分思路大集合","uri":"/2020/09/da-segmentation/"},{"categories":["写作","数据分析"],"content":"基于R，总结零膨胀计数模型的应用流程 我之所以用计数模型，主要是受到清华水利专业某博士某篇论文1的启发。我也将他构造变量的思路也写进了我论文的展望部分。 在经济学及社会科学领域也会遇到对计数数据（Count Data）建模的任务 计数数据是一种数据类型，取值只能是非负整数{0,1,2,3,…}，并且数值并不是排名而是计数 泊松回归（Poisson Regression）是处理计数数据的常用方法 stats package - glm() 函数 但是泊松回归并不能很好的解决计数变量数据中“过度分散”（over-dispersion）的问题 针对 over-dispersion 的问题，往往会采用负二项回归（Negative Binomial Regression）的方法 MASS package - glm.nb() 函数 但，无论是泊松回归还是负二项回归都不能很好的解决计数数据零过多的问题 此时，零膨胀计数模型（zero-inflated models）就诞生了 zero-inflated models are mixture models that combine a count component and a point mass at zero. 关于零膨胀计数模型，各自的软件或语言都有自己的实现方法。从我查阅的资料来看，目前 stata 和 R 在这方面资料比较多，Python 相对较少。 我选择的是R，因为Anaconda装rstudio比较便捷，且学习成本比stata低。 所以本文基于R，在确定计数数据存在零膨胀的前提下（一般看计数数据的频率直方图，或者直接用vuong()完成模型选择），总结零膨胀计数模型的应用流程 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:0:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"启动rstudio 用 Anaconda 安装 Rstudio 的过程中，需要手动创建一个R运行的新环境（如，r_env） 打开 anaconda 比较费时儿，所以会在 conda命令端 启动Rstudio # 查看环境目录 conda info -e # 切换至文件目录下启动 cd \"工作目录\" # 激活环境 conda activate r_env # 启动 Rstudio rstudio ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:1:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"读取数据文件 R打开csv格式的文件比较省事儿 # 数据文件保存在rstudio启动的目录下 my.data1\u003c-read.csv(\"data.csv\") ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:2:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"Over-Dispersion？ 有两种方法检验计数数据是否存在over-dispersion的现象，但对象不同，用的模型不同 odTest() dispersiontest() ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"odTest() 需要 pscl package odTest(glmnb_obj, alpha=.05, digits = max(3, getOption(\"digits\") - 3)) 先用负二项模型跑一次回归 library(MASS) nb \u003c- glm.nb(y ~ x1+x2+x3, data=my.data1) library(pscl) odTest(nb) $p-value\u003c0.05$，说明计数数据存在 over-dispersion 现象 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:1","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"dispersiontest() glm_p \u003c- glm(y ~ x1+x2+x3, data = my.data1, family = poisson) library(AER) dispersiontest(glm_p,trafo=1) 计数数据存在零膨胀且over-dispersion的现象，那么就需要采用零膨胀负二项模型 若不存在over-dispersion的现象，则采用零膨胀泊松模型 当然，可以两个模型都跑一次，最后通过vuong()完成模型的选择 所以，也可以比较零膨胀泊松模型和泊松模型 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:3:2","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"run regressions 零膨胀计数模型都依赖 pscl package ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"零膨胀负二项回归模型 m3 \u003c- zeroinfl(y ~ x1+x2+x3|x4+x5+x6, data=my.data1,dist=\"negbin\") 零膨胀计数模型是分成了两部分建模：一部分是计数部分，另一部分处理为二分类 $(0,1)$ 的情况 $x_1,x_2,x_3$ 是选定的计数部分的影响因素；$x_4,x_5,x_6$ 是二分类情况的影响因素 如果两者的影响因素一样的，则公式形式可以简单写为 m3 \u003c- zeroinfl(y ~ x1+x2+x3, data=my.data1,dist=\"negbin\") # 若数据中除y之外均为影响因素 m3 \u003c- zeroinfl(y ~ ., data=my.data1,dist=\"negbin\") ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:1","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"零膨胀泊松回归模型 m4 \u003c- zeroinfl(y ~ x1+x2+x3|x4+x5+x6, data=my.data1,dist = 'poisson') $x_1,x_2,x_3$ 的表现方式同上 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:4:2","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"模型选择 vuong(m3,m4) 根据结果选择模型即可 ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:5:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"参考资料 Hanchen Jiang et al.: 10.3390/su10051509 ↩︎ ","date":"2020-05-06","objectID":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/:6:0","tags":["论文专题"],"title":"【论文专题】零膨胀计数模型","uri":"/2020/05/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E9%9B%B6%E8%86%A8%E8%83%80%E8%AE%A1%E6%95%B0%E6%A8%A1%E5%9E%8B/"},{"categories":["写作","数据分析"],"content":"这篇是文本情感分析的应用篇 运用情感分析技术，让我们的研究更丰富 本文所讨论的情感分析聚焦在 二分类 情况，即判断一句话（短文本）的情感倾向是正面还是负面 我们可以简单的理解为： 存在一个很牛逼的箱子，这个箱子有进口处和出口处，我们需要做的便是把某句话通过进口处放入这个箱子，之后这个箱子从出口处吐出结果 其中情感倾向的结果方面，大多数箱子是会告诉我们情感倾向为正的概率是多少（假设是 $a$ ），显然情感倾向为负的概率便是 $1-a$ 若 $a\u003e1-a$ ，即 $a\u003e0.5$ ，“箱子”认为这句话的情感倾向是正面的 总之，我们可以根据研究的需要，根据箱子吐出的结果构造变量用于下游的任务。 那么，本文就系统的整理了好用的“箱子” ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"ERNIE1 谷歌的BERT问世之后，预训练模型那是备受关注啊，算是自然语言处理领域的里程碑事件了。 我为了蹭热度，琢磨百度的这个ERNIE，也是废了点时间 在本地装好各种环境之后，在我这个小破电脑上训练了一天一夜吧 但最终的方案是蹭了百度的 AI Studo 注册账号，运行项目还送算力卡，这样就能用GPU训练了 小破电脑一天一夜的活，几分钟就搞定了 本地装包，没梯子的话，一定要用清华的镜像 conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/ Python代码 #需要更改训练集、验证集、测试集的文件位置 import paddlehub as hub !hub install ernie module = hub.Module(name=\"ernie\") class DemoDataset(BaseNLPDataset): \"\"\"DemoDataset\"\"\" def __init__(self): # 数据集存放位置 self.dataset_dir = \"work\" super(DemoDataset, self).__init__( base_path=self.dataset_dir, train_file=\"train.tsv\", dev_file=\"dev.tsv\", test_file=\"test.tsv\", # 如果还有待预测数据，可以放在predict.tsv predict_file=\"predict.tsv\", train_file_with_header=True, dev_file_with_header=True, test_file_with_header=True, predict_file_with_header=True, # 数据集类别 label_list=[\"0\", \"1\"]) dataset = DemoDataset() reader = hub.reader.ClassifyReader( dataset=dataset, vocab_path=module.get_vocab_path(), sp_model_path=module.get_spm_path(), word_dict_path=module.get_word_dict_path(), max_seq_len=128) strategy = hub.AdamWeightDecayStrategy( weight_decay=0.01, warmup_proportion=0.1, learning_rate=5e-5) config = hub.RunConfig( use_cuda=True, num_epoch=1, checkpoint_dir=\"ernie_txt_cls_turtorial_demo\", batch_size=128, #一般为2^n eval_interval=10, strategy=strategy) inputs, outputs, program = module.context( trainable=True, max_seq_len=128) # Use \"pooled_output\" for classification tasks on an entire sentence. pooled_output = outputs[\"pooled_output\"] feed_list = [ inputs[\"input_ids\"].name, inputs[\"position_ids\"].name, inputs[\"segment_ids\"].name, inputs[\"input_mask\"].name, ] cls_task = hub.TextClassifierTask( data_reader=reader, feature=pooled_output, feed_list=feed_list, num_classes=dataset.num_labels, config=config) run_states = cls_task.finetune_and_eval() # 预测 data = [[d.text_a] for d in dataset.get_predict_examples()] run_states = cls_task.predict(data=data) results = [run_state.run_results for run_state in run_states] ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"senta2 百度的senta模型 有种模型即软件的味道 对于我这种小白很友好，比ERNIE方便 加载好模型就能用 CNN, GRU, LSTM, BiLSTM import paddlehub as hub # 加载模型 cnn = hub.Module(name='senta_cnn') # 预测 data_dict = {\"text\":[\"你怎么那么好看\"]} results = cnn.sentiment_classify(data = data_dict) ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"snownlp3 snownlp比较老了，但他的训练集都是电商评论的数据 契合我的主题，所以我也用了 这个用起来也方便 from snownlp import SnowNLP s = SnowNLP(u'这个东西真心很赞') s.sentiments # 输出情感倾向为正面的概率 当然，snownlp还能干别的，如分词、繁体转简体、提取关键词… ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作","数据分析"],"content":"参考资料 https://aistudio.baidu.com/aistudio/index ↩︎ https://www.paddlepaddle.org.cn/modelbasedetail/senta ↩︎ https://github.com/isnowfy/snownlp ↩︎ ","date":"2020-04-29","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/:4:0","tags":["论文专题"],"title":"【论文专题】文本情感分析","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"},{"categories":["写作"],"content":"毕业论文模板，样式先行 三年前的我搞毕业设计，第一次接触这玩意儿，一脸懵逼 好在我朱哥搞过大创（还是国家级的），当时给我各种科普单片机的知识 搞大创的好处就是当我们不知道是画机械图还是搞电路的时候，朱哥设计的成品以及论文都搞定了，，，跑马灯跑起来、酒精浓度测起来、小屏幕亮起来 不过朱哥也没继续搞汽车，后来投身了我国交通事业。依稀记得朱哥远程面试的时候，寝室几个人都在打游戏，，，在我们的影响下，朱哥最终去了广东某985高校 但那时候我们对Word排版都不怎么了解 以下情景历历在目： Example 这个参考文献的上标怎么弄？ 又要加篇参考文献？序号不得又重写？ 参考文献的引用格式是啥？ 这些字体都要改成三号、加粗？然后就用格式刷一顿更新 三线表是什么鬼？又要画三线表？ 。。。。。。 总之，我们都在Word上花了很长时间，不断的进行重复性的工作，以满足格式上要求 但当我们面对硕士毕业论文时，在格式方面花的时间就很少很少了，没有了各种重复性的操作 因为我们秉承着**样式先行。**在写论文之前，就根据学校的要求，把字体、表格的格式调整好，写作的过程中随时切换，而且就算后续要调整，统一调整对应的样式即可，避免了重复性操作。 本文并不是记录如何去调整/设计样式，因为我认为这个真的是太多了，重点是基于毕业论文格式记录word的正确打开方式，抽象出复用的部分。这也是我开发模板的原因。面对不同的需求，后续只需要微调即可。 可以在我的公众号内回复 word， 获取华东师范大学毕业论文模板 （用过的朋友都说好） ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:0:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"模块拆解 根据论文的总体框架，分别设计对应的样式。 换句话说便是先弄清楚整篇论文的字体、段落会涉及到哪些不同的格式要求，在正式写作之前，分别设计好对应的样式。以便写作过程调用。 毕业论文主体部分的格式要求，往往会涉及到各级标题、正文、脚注、参考文献（参考文献部分后续会单拎出来记录）字体的格式，理工科专业往往会要求表格是三线表。 确定好字体、段落的格式要求后，就可以针对性的修改/设计样式 为了方便后续的写作，可以设计相应的快捷键（样式-\u003e格式-\u003e快捷键） ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:1:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"参考文献格式 正文中往往会引用相关参考文献 这时候就要放出文献管理大杀器——NoteExpress 有了他，妈妈再也不用担心我修改参考文献的格式了 在软件中记录好被引用的内容，如作者、论文标题、年份、期刊或会议名称等等（大部分pdf、caj文献都能有效识别） 每当需要标注引用时，直接点击“引用”即可 通过NoteExpress引用至word后主要涉及两大部分： 正文部分的标注 上标序号 或 作者+年份 的形式 附在最后的参考文献 如 [1] LIU Y. Word of mouth for movies: Its dynamics and impact on box office revenue[J]. Journal of Marketing. 2006, 70(3): 74-89. 依然是样式先行的逻辑，简便的办法是先在NoteExpress样式库中选择与自己要求相似的样式，当然，完全满足需求就更好了。 接着，在所选样式的基础上修改相关格式以满足自己的需求 引文下的修改便是对应修改“正文部分的标注”的格式 题录下的修改便是对应修改“附在最后的参考文献”的格式 值得一提的是，题录中可设置排序的规则，因为有些学校要求英文文献在前面、之后再按照时间或作者排序。 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:2:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"Tips 最后再记录些杂七杂八的tips ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:0","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"表格、图片名称 选中表格（/图片），右键“插入题注”，标签选择“表”，编号选择“包含章节号” ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:1","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"交叉引用 论文中往往会看到如图xxx所示、如表xxx所示等表述方式，并且希望能定位到相关表格或图片时，就需要采用文内交叉引用的方法 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:2","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"公式 当我们用MathType编辑公式，粘贴至word时，往往会影响word的格式，主要体现在行与行之间的距离变大。 对应的解决方案便：在对应的段落内，右键选择“段落”，取消勾选下图中所示内容 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:3","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"代码高亮 涉及到代码的话，为了美观，往往希望word中代码也实现高亮 推荐 planetB 暂时统计了这几个tips，欢迎交流 ","date":"2020-04-21","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/:3:4","tags":["论文专题"],"title":"【论文专题】毕业论文模板","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"},{"categories":["写作"],"content":"关于爬虫的文章，之前写过两篇： 爬取拉勾网数据分析职位相关的数据 基于Scrapy框架爬取京东评论数据 京东那个其实也是为了写论文，虽说拿着那篇参加了个会议，但我觉得诟病比较多。 如今，为了毕业论文，又重操旧业了 这次选择的对象是苏宁易购手机评论数据 收购家乐福中国事件吸引了我的眼球 整体的逻辑也比较简单，没有很复杂的反爬技术，直接上流程图吧 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:0:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"提取url的共性 我觉得像我这种低端爬虫的基本思想就是所见即所得，代替人工的复制、粘贴 我们在网络上见到的东西，都会有个url与之对应 通过url给服务器发送请求 就像 https://www.baidu.com/ 服务器返回相应的数据，浏览器解析这些数据，最后就是大家看到的样子 爬虫也是这个逻辑 所以写代码之前需要分析苏宁易购评论数据的url 苏宁评论数据是动态加载的，需要抓包分析 寻找url的异同点，比如不同商品、不同页面之间url的差异，哪些是变的、哪些是不变的。 一顿操作之后，发现变动的主要是三部分： 商品ID 店铺ID Cluster_ID 商品、店铺ID还好理解，你这个东西是什么、在哪家卖的，不能把A家商品的评论数据放到B家商品那儿 但这个 Cluster_ID 是什么 问就是不知道 但还是要清楚这玩意儿可以从哪儿得到，不然评论数据的url也不知道呀 有幸结识了位美团的前端大佬,在大佬的帮助下，找到了Cluster_ID 结论是，在商品详情页的源代码中有这么个东西 所以在最终确定评论数据的URL之前，需要通过解析商品详情页的数据，获取 Cluster_ID def get_clusterId(product_id,ua,shop_id=\"0000000000\"): ''' 解析商品详情页url 得到clusterId ''' url = \"https://product.suning.com/{}/{}.html\".format(shop_id,product_id) header = {\"Referer\":\"https://search.suning.com/%E6%89%8B%E6%9C%BA/#second-filter\",\"User-Agent\": ua} response = requests.get(url, headers=header) html = response.text soup = bs(html,\"html.parser\") t = soup.select(\"head script\")[0] tstr = t.get_text() cluster_id = re.search(r\"clusterId\\\":\\\"(\\d*).*?\\\"\",tstr).group(1) # string return cluster_id shop_id=0000000000 表示店铺是苏宁自营 正如流程图中所示，通过商品、店铺ID，确定商品详情页的url，从服务器返回的数据中找到 Cluster_ID 其实到这，就已经结束了 后续的操作就是获取并解析json文件 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:1:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"json文件 根据url请求服务器，让其返回评论数据的json文件 def again_content(cluster_id,product_id,page,ua,shopid=\"0000000000\"): product_id_0 = \"0\"*(18-len(str(product_id))) + str(product_id) url = \"https://review.suning.com/ajax/cluster_review_lists/cluster-{}-{}-{}-again-{}-default-10-----reviewList.htm?callback=reviewList\".format(cluster_id,product_id_0,shopid,page) header = {\"User-Agent\": ua, \"Referer\":\"https://product.suning.com/{}/{}.html\".format(shopid,product_id), \"Host\": \"review.suning.com\", \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate, br\", \"Accept-Language\": \"zh-CN,zh;q=0.9,zh-TW;q=0.8,en;q=0.7\"} response = requests.get(url, headers=header) html = response.text text = json.loads(html.lstrip(\"reviewList(\").rstrip(\")\") ) reviews = text[\"commodityReviews\"] print(text[\"returnMsg\"],product_id,page) return reviews 根据json文件找到想要的数据，根据key-value的对应形式，慢慢获取就可以了,如 add_reviewID = reviews[\"againReview\"][\"againId\"] reviewId = reviews[\"commodityReviewId\"] # 之后要利用这个匹配 useful_vote first_review = reviews[\"content\"] first_review_pt = reviews[\"publishTime\"] add_review = reviews[\"againReview\"][\"againContent\"] add_review_pt = reviews[\"againReview\"][\"publishTime\"] diff_first_add_pt = reviews[\"againReview\"][\"publishTimeStr\"] qualityStar = reviews[\"qualityStar\"] first_pic = reviews[\"imgCnt\"] 最后保存相关数据即可 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:2:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"其他细节 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:0","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"循环次数 因为我是 FOR 循环的形式获取评论数据的，并且有些商品的追评数据比较少。为了避免无效的循环，所以我还额外通过追评总数计算了页面数，以决定循环次数。最多展示50页 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:1","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"反反爬 这里面学问比较多，对效率要求不高的话，可以设置停留时间，休息一会儿再请求服务器 模拟随机的UserAgent，就像是换个浏览器 这个时候就要安利下fake_useragent import time import numpy as np from fake_useragent import UserAgent # 随机休息3-10秒 time.sleep(np.random.randint(3,10)) # useragent ua = UserAgent() ua.random 其实，最好的方法是随机更换IP地址 ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:2","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["写作"],"content":"增强稳健性 为了保证代码的正常运行，预判会报错的地方，采取相应的方式 爬虫主要是以放服务器拒绝请求 我的方式比较粗暴，歇息时间长一点继续工作 try: clusterId = get_clusterId(product_id,ua.random) except: time.sleep(np.random.randint(5,15)) continue ","date":"2020-04-05","objectID":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/:3:3","tags":["论文专题"],"title":"【论文专题】苏宁易购评论爬虫","uri":"/2020/04/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E8%8B%8F%E5%AE%81%E8%AF%84%E8%AE%BA%E6%95%B0%E6%8D%AE/"},{"categories":["数据分析","写作"],"content":"Bootstrapping基本思想 之前几篇关于 中介效应 有调节的中介效应模型 文章中都有提到 Bootstrapping 这篇就简单的记录Bootstrapping 的基本思想 平时在做分析的时候，总体的数据往往是比较难获取的 所以，我们会基于抽样理论，从总体中抽取具有代表性的样本去估计总体的信息 比如用样本均值 $\\bar X$ 估计总体均值 $\\mu$ ；样本方差 $S^2$ 估计总体方差 $\\sigma^2$ 等等 或者用区间估计的方法给出总体的均值的置信区间 本文以样本均值估计总体均值为例，探究Bootstrap方法的实现方式 Bootstrapping1 是一种重采样的方法（resampling method） 之前有提到，从总体中进行抽样，得到样本数据， $x_1,x_2,…,x_n$ 而Bootstrapping呢，从得到的样本中再进行有放回的抽样 如果抽样个数等于样本数据个数（$n$），则称其为 Bootstrap Sample $$ S_i^*={x_{i1}^*,x_{i2}^*,…,x_{in}^*} $$ 以这种方式不断的对样本进行重采样，就会得到 bootstrap samples，$S^*$ $$ S^*={S_{1}^*,S_{2}^*,…,S_{R}^*} $$ 其中，$R$ 一般要大于等于1000 $R$ 便是对应着 number of bootstrap samples 的选择 Bootstrapping 估计样本均值就是先分别计算 $R$ 个Bootstrap Sample的均值，最后再计算 $R$ 个均值的均值 Python代码示例 import numpy as np # 假设总体为X X = np.random.normal(size=10000000) # 抽样 n = 1000 x_i = np.random.permutation(X)[:n] # Bootstrapping估计 R = 5000 boot_samples = [x_i[np.random.randint(0,n,n)].mean() for _ in range(R)] boot_mean = np.sum(boot_samples) / R 当然，也可以给出均值的 Bootstrap置信区间 需要先将每个Bootstrap Sample得到的均值从小到大排序 进而计算 $\\alpha /2$ 及 $(1- \\alpha/2)$ 分位数作为区间的上下限 s_sorted = np.sort(boot_samples) alpha = 0.05 s_sorted[[round(R*alpha/2), round(R*(1-alpha/2))]] ","date":"2020-03-30","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/:0:0","tags":["论文专题"],"title":"【论文专题】Bootstrapping","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/"},{"categories":["数据分析","写作"],"content":"参考资料 Brad Efron: http://www.jstor.org/discover/10.2307/2958830?uid=3739568\u0026uid=2\u0026uid=4\u0026uid=3739256\u0026sid=21102342537691 ↩︎ ","date":"2020-03-30","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/:1:0","tags":["论文专题"],"title":"【论文专题】Bootstrapping","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-bootstrapping/"},{"categories":["数据分析","写作"],"content":"在营销、管理领域，发现 $X$ 会影响 $Y$ 后，还会进一步的琢磨是如何影响，回答一个关于 HOW 的问题。 我觉得是沿用了心理学的研究方法，这里就不谈流派了，毕业要紧 所以憋论文的时候除了找到变量 $X$、$Y$，往往还要憋个变量 $M$ 出来，这个变量 $M$ 就用来回答 $X$ 如何影响 $Y$，$M$ 被称为中介变量（Mediator Variable） 为了得到因果关系，之后往往会设计实验，以问卷的形式收集数据，最后用统计学的方法一顿操作验证 $X$ 是否会影响 $M$ 进而影响 $Y$ 这篇就是记录如何用 PROCESS1 完成中介效应的分析，以简单的中介效应模型为例。 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:0:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"1 理论部分 上图的理论模型用数学数学语言可写为 $$ M = i_M+aX+e_M $$ $$ Y = i_Y + c^,X + bM + e_Y $$ 正如下图所示 直接效应 $c^,$ 表示 $X$ 影响 $Y$ 的直接效应 $$ c^,=[\\widehat{Y}|(X=x+1,M=m)] - [\\widehat{Y}|(X=x,M=m)] $$ $c^,$ 可解释为：在变量 $M$ 保持不变的情况下，自变量 $X$ 增加一个单位，因变量 $Y$ 均值变化 $c^,$ 。 间接效应（indirect effect） 将公式$\\ref{M}$ 代入公式$\\ref{Y}$，可发现系数 $a \\times b$ 便是估计中介效应的，所以中介效应是否存在，就需要关注系数 $ab$ 的情况了 PROCESS也是采用最小二乘法得到系数的 但PROCESS特别的地方在于，他是采用 Bootstrap Confidence Interval 的方法对检验以下假设 $$ H0: ab=0 $$ 「MARK」之后再另起灶炉简单扯一点Bootstrapping 用PROCESS做中介效应分析，结果的最后会给出间接效应的 95% bootstrap confidence interval 如果区间内不包含0，则说明中介效应成立 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:1:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"2 实际操作 步骤如下： SPSS加载 process的语法文件 使用过程中可以自主编写语法、也可以菜单式的操作 变量和数据字段一一对应 基于理论研究模型，选择PROCESS的模型（本例对应的是model 4） 我感觉这也算是PROCESS的一个弊端吧，灵活性不高。 其他可选 number of bootstrap samples 一般是5000，数据量小的时候可往大了选 选项(options) 标准化、中心化、产生画图的数据 调节效应分析中，spotlight analysis 选择16th、84th分位数表示低高，还是选择 均值±标准差 表示低、高 多分类变量的处理 如果是二分类（dichotomous）就不管 因为多分类变量(假设4类)的1，2，3，4并没有意义，只是表示不同的类别而已，一般通过indicator coding（也叫dummy coding）的方式将变量转为dummy variable ，用 $4-1=3$ 个字段表示 一顿操作之后，便可得到如下结果 正如理论部分所说，是否存在中介效应， 就看结果中 Indirect effect(s) of X on Y 的部分 Bootstrap置信区间不包含0，则说明中介效应存在。 ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:2:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"},{"categories":["数据分析","写作"],"content":"参考资料 Hayes A F: Introduction_to_Mediation_Moderation_and_Conditional_Process_Analysis_A_Regression_Based[M].2ed.2018 ↩︎ ","date":"2020-03-24","objectID":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/:3:0","tags":["论文专题"],"title":"【论文专题】中介效应分析","uri":"/2020/03/%E8%AE%BA%E6%96%87%E4%B8%93%E9%A2%98-%E4%B8%AD%E4%BB%8B%E6%95%88%E5%BA%94%E5%88%86%E6%9E%90/"}]